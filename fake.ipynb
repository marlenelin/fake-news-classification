{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCc_g9nTbMwe"
      },
      "source": [
        "# Fake News Classification with Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6z2jXvBbMwh"
      },
      "source": [
        "## Intro: \n",
        "Following the previous blog post on image classification, we would be using tensorflow again to perform machine learning tasks —— on text data this time. \n",
        "\n",
        "While Plague Inc. went 'viral' again during the beginning of the COVID-19 outbreak due to public attention on the pandemics, my favourite game scenario in Plague In. has always been the fake news mode. Albeit some headlines are straight-out troll, the existence of such scenario still speaks volume about how the deliberate use of modern technology and psychological tricks could be used to infect the world with false information and cause extreme consequences to the democracy and health of the soceity.\n",
        "\n",
        "In this blog, we would be getting some hand-on combat experience against fake news through the creation of a ML & N-Gram based fake news classification model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ZwD_MXbMwi"
      },
      "source": [
        "## Data acquisition  \n",
        "\n",
        "The following data is a small segment of a Kaggle fakenews dataset. \n",
        "Each row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper：  \n",
        "\n",
        "*Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.io as pio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "pio.renderers.default = \"notebook_connected\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zOumAI-nbMwj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\" \n",
        "news = pd.read_csv(train_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aKxH71MFzWMK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "6Gc9suVzbMwk",
        "outputId": "9f26858a-cf4e-49ce-fcb1-840e6ab2287d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unnamed: 0     int64\n",
            "title         object\n",
            "text          object\n",
            "fake           int64\n",
            "dtype: object\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cd2f7173-ec7e-45bc-8799-102f2813ad0f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17366</td>\n",
              "      <td>Merkel: Strong result for Austria's FPO 'big c...</td>\n",
              "      <td>German Chancellor Angela Merkel said on Monday...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5634</td>\n",
              "      <td>Trump says Pence will lead voter fraud panel</td>\n",
              "      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17487</td>\n",
              "      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>\n",
              "      <td>On December 5, 2017, Circa s Sara Carter warne...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12217</td>\n",
              "      <td>Thyssenkrupp has offered help to Argentina ove...</td>\n",
              "      <td>Germany s Thyssenkrupp, has offered assistance...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5535</td>\n",
              "      <td>Trump say appeals court decision on travel ban...</td>\n",
              "      <td>President Donald Trump on Thursday called the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd2f7173-ec7e-45bc-8799-102f2813ad0f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd2f7173-ec7e-45bc-8799-102f2813ad0f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd2f7173-ec7e-45bc-8799-102f2813ad0f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0                                              title  \\\n",
              "0       17366  Merkel: Strong result for Austria's FPO 'big c...   \n",
              "1        5634       Trump says Pence will lead voter fraud panel   \n",
              "2       17487  JUST IN: SUSPECTED LEAKER and “Close Confidant...   \n",
              "3       12217  Thyssenkrupp has offered help to Argentina ove...   \n",
              "4        5535  Trump say appeals court decision on travel ban...   \n",
              "\n",
              "                                                text  fake  \n",
              "0  German Chancellor Angela Merkel said on Monday...     0  \n",
              "1  WEST PALM BEACH, Fla.President Donald Trump sa...     0  \n",
              "2  On December 5, 2017, Circa s Sara Carter warne...     1  \n",
              "3  Germany s Thyssenkrupp, has offered assistance...     0  \n",
              "4  President Donald Trump on Thursday called the ...     0  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(news.dtypes)\n",
        "news.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVh3lXttbMwk"
      },
      "source": [
        "## Create Dataset  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzwtSVLqbMwl"
      },
      "source": [
        "To parse the text and perform topic analysis, the first step is removing stopwords, i.e., uninformative words like 'and','the','a', etc. We make use of the nltk library to perform such task.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRV7aK-bbMwl",
        "outputId": "1b128035-e3be-4e29-b531-a62a3e8be4b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pr1DMOnlbMwl"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xJfWE43bMwl"
      },
      "source": [
        "Next, we will create a tensorflow dataset to host our news data. Tensorflow datasets are iterable and well-integrated with the machine learning pipeline. \n",
        "We would write a create_database function that does the following 2 things: \n",
        "\n",
        "1. Remove stopwords from the article text and title. \n",
        "\n",
        "2. Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column.\n",
        "\n",
        "For a tf dataset:\n",
        "- Elements refer to a single output from calling next() on a dataset iterator. Elements may be nested structures containing multiple components. For example, the element (1, (3, \"apple\")) has one tuple nested in another tuple. The components are 1, 3, and \"apple\".  \n",
        "- Components refers to the leaves in the nested structure of an element. \n",
        "- To set up a dataset for ML training, we need something in this format:   \n",
        "ds = tf.data.Dataset.from_tensor_slices((features_dict, labels)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dbj0fpL-gBzR"
      },
      "outputs": [],
      "source": [
        "def make_dataset(df):\n",
        "\n",
        "  stop = stopwords.words('english')\n",
        "  # retain the rest of words, separated by space\n",
        "  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() \n",
        "                                                        if word not in (stop)]))\n",
        "  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() \n",
        "                                                        if word not in (stop)]))\n",
        "  # Construct tf dataset\n",
        "  tfds = tf.data.Dataset.from_tensor_slices(({\"title\":df[[\"title\"]],\"text\":df[\"text\"]},#feature_dict\n",
        "            {'fake':df[[\"fake\"]]}))#labels\n",
        "  \n",
        "  # batch \n",
        "  tfds = tfds.batch(100)\n",
        "  return tfds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oUxaEzgpqn8S"
      },
      "outputs": [],
      "source": [
        "ds = make_dataset(news)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Geuh4t0qteYk",
        "outputId": "96a1ead6-380f-4a84-d426-cc9357b63ff4"
      },
      "outputs": [],
      "source": [
        "for idx,lbl in ds.take(1): # similar to data[:5]\n",
        "  print(idx['text'])\n",
        "  print(idx['title'])\n",
        "  print(lbl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au93W85ZqgCS"
      },
      "source": [
        "## Validation Data\n",
        "After constructing the primary dataset, we split off 20% for validation using skip and take.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mV6bzrilqncN"
      },
      "outputs": [],
      "source": [
        "val_size = int(0.2 * len(ds))\n",
        "\n",
        "ds = ds.shuffle(buffer_size = len(ds))\n",
        "val_ds = ds.take(val_size)\n",
        "train_ds = ds.skip(val_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3LC2KD1xgia",
        "outputId": "c8b944e7-462d-487c-f6b3-7b0adcfeab8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(45, 180)"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_ds),len(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcl24mbyqoWU"
      },
      "source": [
        "## Base Rate\n",
        "\n",
        "Base rate refers to the accuracy of a model that always makes the same guess. Determine the base rate for this data set by examining the labels on the training set —— the base rate would be the proportion of the label with the highest frequency in the label pool:\n",
        "- 1: fake\n",
        "- 0: non-fake\n",
        "\n",
        "In this case, the base rate is **0.5230** (52.30% of the entries are fake news in this dataset.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qd9dw_eq4yL",
        "outputId": "8d74eed5-7442-48b3-8a2c-534037173d89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    0.522963\n",
              "0    0.477037\n",
              "Name: fake, dtype: float64"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "news.fake.value_counts()/len(news)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBGyDcDqyvs2"
      },
      "source": [
        "## TextVectorization  \n",
        "\n",
        "Preprocess text and then map words to integers: we would create a frequency dictionary that encodes words with their total numbers of appearances in the dataset. And we set a limit of 2000 to only use the most frequent 2000 words to set up our word dictionary for training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oigOP8pOyyhL",
        "outputId": "50915af9-abd8-46a0-c0e8-df64a448b143"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "#preparing a title vectorization layer for tf model\n",
        "size_vocabulary = 2000\n",
        "\n",
        "#convert all strings to lower cases,\n",
        "#get rid of all puncuations\n",
        "def standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    no_punctuation = tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation),'')\n",
        "    return no_punctuation \n",
        "\n",
        "title_vectorize_layer = layers.TextVectorization(\n",
        "    standardize=standardization,\n",
        "    max_tokens=size_vocabulary, # only consider this many words\n",
        "    output_mode='int',\n",
        "    output_sequence_length=500) \n",
        "\n",
        "#this will make the layer 'learn' whatever words we've included from the titles\n",
        "title_vectorize_layer.adapt(train_ds.map(lambda x, y: x[\"title\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "acHAiCOP9B_N"
      },
      "outputs": [],
      "source": [
        "#preparing a text vectorization layer for tf model\n",
        "\n",
        "def standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    no_punctuation = tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation),'')\n",
        "    return no_punctuation \n",
        "\n",
        "text_vectorize_layer = layers.TextVectorization(\n",
        "    standardize=standardization,\n",
        "    max_tokens=size_vocabulary, # only consider this many words\n",
        "    output_mode='int',\n",
        "    output_sequence_length=500) \n",
        "\n",
        "text_vectorize_layer.adapt(train_ds.map(lambda x, y: x[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYunRVdx0SK_"
      },
      "source": [
        "## Create a Model   \n",
        "\n",
        "We would be building three models (using the functional API of keras) that train on only title, only text, both title and text respectively to answer the question:\n",
        "\n",
        "**When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amy3v2Tt0o8T"
      },
      "source": [
        "- In the first model, you should use only the article title as an input.\n",
        "- In the second model, you should use only the article text as an input.\n",
        "- In the third model, you should use both the article title and the article text as input.\n",
        "\n",
        "(Applied to text vectorization layer adaptation as well)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTrbSCx9pIqg"
      },
      "source": [
        "As suggested, we define an embedding layer that would be shared by all three models.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VaFUG4LOpUVO"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_tokens = 2000\n",
        "output_sequence_length = 25\n",
        "emb = layers.Embedding(max_tokens, output_dim = 3, name=\"embedding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x5ZmSUH8O-V"
      },
      "source": [
        "### Article title only "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0B7YtvOXeWKQ"
      },
      "outputs": [],
      "source": [
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXPHDkWlsqxg",
        "outputId": "237161b6-653b-4340-87ea-4b5c326efbc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"title_only\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " title (InputLayer)          [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVec  (None, 500)              0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 500, 3)            6000      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 500, 3)            0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 3)                0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 3)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                128       \n",
            "                                                                 \n",
            " fake (Dense)                (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,194\n",
            "Trainable params: 6,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "title_in = keras.Input(shape=(1,),name = \"title\", dtype = \"string\")\n",
        "title_layer = title_vectorize_layer(title_in) #vectorize title\n",
        "title_layer = emb(title_layer) #shared embedding\n",
        "title_layer = layers.Dropout(0.2)(title_layer) #randomaly drop 20% of the connections to reduce overfitting\n",
        "title_layer = layers.GlobalAveragePooling1D()(title_layer) #take the average of embedding vectors along the time axis\n",
        "title_layer = layers.Dropout(0.2)(title_layer)\n",
        "title_layer = layers.Dense(32, activation='relu')(title_layer)\n",
        "\n",
        "\n",
        "# output layer\n",
        "output = layers.Dense(2, name = \"fake\")(title_layer)\n",
        "model1 = keras.Model(inputs = title_in,outputs = output,name='title_only')\n",
        "model1.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "C3s9tgTK2Ada"
      },
      "outputs": [],
      "source": [
        "model1.compile(optimizer=\"adam\",\n",
        "              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipg-E1u2GA7h",
        "outputId": "c5b1a1cb-657d-409a-82d6-02d3da439299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "180/180 [==============================] - 5s 19ms/step - loss: 0.6918 - accuracy: 0.5204 - val_loss: 0.6895 - val_accuracy: 0.5248\n",
            "Epoch 2/20\n",
            "180/180 [==============================] - 3s 14ms/step - loss: 0.6809 - accuracy: 0.5791 - val_loss: 0.6627 - val_accuracy: 0.6393\n",
            "Epoch 3/20\n",
            "180/180 [==============================] - 3s 18ms/step - loss: 0.6163 - accuracy: 0.7934 - val_loss: 0.5440 - val_accuracy: 0.9353\n",
            "Epoch 4/20\n",
            "180/180 [==============================] - 6s 34ms/step - loss: 0.4662 - accuracy: 0.9038 - val_loss: 0.3705 - val_accuracy: 0.9411\n",
            "Epoch 5/20\n",
            "180/180 [==============================] - 2s 12ms/step - loss: 0.3244 - accuracy: 0.9332 - val_loss: 0.2537 - val_accuracy: 0.9504\n",
            "Epoch 6/20\n",
            "180/180 [==============================] - 2s 12ms/step - loss: 0.2380 - accuracy: 0.9452 - val_loss: 0.1815 - val_accuracy: 0.9618\n",
            "Epoch 7/20\n",
            "180/180 [==============================] - 3s 16ms/step - loss: 0.1870 - accuracy: 0.9529 - val_loss: 0.1344 - val_accuracy: 0.9711\n",
            "Epoch 8/20\n",
            "180/180 [==============================] - 2s 11ms/step - loss: 0.1579 - accuracy: 0.9562 - val_loss: 0.1206 - val_accuracy: 0.9674\n",
            "Epoch 9/20\n",
            "180/180 [==============================] - 2s 11ms/step - loss: 0.1364 - accuracy: 0.9608 - val_loss: 0.0928 - val_accuracy: 0.9762\n",
            "Epoch 10/20\n",
            "180/180 [==============================] - 2s 12ms/step - loss: 0.1171 - accuracy: 0.9671 - val_loss: 0.0967 - val_accuracy: 0.9667\n",
            "Epoch 11/20\n",
            "180/180 [==============================] - 2s 12ms/step - loss: 0.1102 - accuracy: 0.9659 - val_loss: 0.0836 - val_accuracy: 0.9718\n",
            "Epoch 12/20\n",
            "180/180 [==============================] - 3s 18ms/step - loss: 0.1007 - accuracy: 0.9684 - val_loss: 0.0698 - val_accuracy: 0.9813\n",
            "Epoch 13/20\n",
            "180/180 [==============================] - 2s 11ms/step - loss: 0.0932 - accuracy: 0.9702 - val_loss: 0.0697 - val_accuracy: 0.9818\n",
            "Epoch 14/20\n",
            "180/180 [==============================] - 2s 12ms/step - loss: 0.0884 - accuracy: 0.9715 - val_loss: 0.0608 - val_accuracy: 0.9831\n",
            "Epoch 15/20\n",
            "180/180 [==============================] - 2s 11ms/step - loss: 0.0850 - accuracy: 0.9706 - val_loss: 0.0641 - val_accuracy: 0.9827\n",
            "Epoch 16/20\n",
            "180/180 [==============================] - 3s 18ms/step - loss: 0.0792 - accuracy: 0.9737 - val_loss: 0.0552 - val_accuracy: 0.9827\n",
            "Epoch 17/20\n",
            "180/180 [==============================] - 2s 12ms/step - loss: 0.0746 - accuracy: 0.9747 - val_loss: 0.0609 - val_accuracy: 0.9782\n",
            "Epoch 18/20\n",
            "180/180 [==============================] - 2s 12ms/step - loss: 0.0756 - accuracy: 0.9738 - val_loss: 0.0476 - val_accuracy: 0.9824\n",
            "Epoch 19/20\n",
            "180/180 [==============================] - 2s 12ms/step - loss: 0.0690 - accuracy: 0.9770 - val_loss: 0.0466 - val_accuracy: 0.9842\n",
            "Epoch 20/20\n",
            "180/180 [==============================] - 2s 11ms/step - loss: 0.0682 - accuracy: 0.9753 - val_loss: 0.0424 - val_accuracy: 0.9860\n"
          ]
        }
      ],
      "source": [
        "history1 = model1.fit(train_ds, epochs=20, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqzZ521GlUVB"
      },
      "source": [
        "From the model fitting history, the title-input only fake news classification reaches a **98.60%** validation accuracy, which is slightly higher than the training accuracy **97.53%**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6sImT1ikfhm"
      },
      "source": [
        "## Article text only. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYT7Zl69kjaH",
        "outputId": "d0b76846-62ac-4e63-b947-f95fd40df8f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"text_only\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text (InputLayer)           [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 500)              0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 500, 3)            6000      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 500, 3)            0         \n",
            "                                                                 \n",
            " global_average_pooling1d_1   (None, 3)                0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 3)                 0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                128       \n",
            "                                                                 \n",
            " fake (Dense)                (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,194\n",
            "Trainable params: 6,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "text_in = keras.Input(shape=(1,),name = \"text\", dtype = \"string\")\n",
        "text_layer = text_vectorize_layer(text_in) #vectorize title\n",
        "text_layer = emb(text_layer) #shared embedding\n",
        "text_layer = layers.Dropout(0.2)(text_layer) #randomaly drop 20% of the connections to reduce overfitting\n",
        "text_layer = layers.GlobalAveragePooling1D()(text_layer) #take the average of embedding vectors along the time axis\n",
        "text_layer = layers.Dropout(0.2)(text_layer)\n",
        "text_layer = layers.Dense(32, activation='relu')(text_layer)\n",
        "\n",
        "\n",
        "# output layer\n",
        "output = layers.Dense(2, name = \"fake\")(text_layer)\n",
        "model2 = keras.Model(inputs = text_in,outputs = output,name='text_only')\n",
        "model2.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "kMSFz9Hckuxo"
      },
      "outputs": [],
      "source": [
        "model2.compile(optimizer=\"adam\",\n",
        "              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJYQhrXlkyX2",
        "outputId": "0f3e71a5-f1f4-4f3c-d398-adea71e54fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "180/180 [==============================] - 5s 20ms/step - loss: 0.6887 - accuracy: 0.5390 - val_loss: 0.6803 - val_accuracy: 0.5831\n",
            "Epoch 2/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.6538 - accuracy: 0.6232 - val_loss: 0.5856 - val_accuracy: 0.8478\n",
            "Epoch 3/20\n",
            "180/180 [==============================] - 4s 23ms/step - loss: 0.5290 - accuracy: 0.7648 - val_loss: 0.3800 - val_accuracy: 0.8930\n",
            "Epoch 4/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.3859 - accuracy: 0.8456 - val_loss: 0.2583 - val_accuracy: 0.9236\n",
            "Epoch 5/20\n",
            "180/180 [==============================] - 5s 27ms/step - loss: 0.3007 - accuracy: 0.8802 - val_loss: 0.2244 - val_accuracy: 0.9271\n",
            "Epoch 6/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.2516 - accuracy: 0.9063 - val_loss: 0.1965 - val_accuracy: 0.9413\n",
            "Epoch 7/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.2223 - accuracy: 0.9219 - val_loss: 0.1795 - val_accuracy: 0.9476\n",
            "Epoch 8/20\n",
            "180/180 [==============================] - 5s 26ms/step - loss: 0.1967 - accuracy: 0.9338 - val_loss: 0.1423 - val_accuracy: 0.9580\n",
            "Epoch 9/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.1759 - accuracy: 0.9415 - val_loss: 0.1314 - val_accuracy: 0.9611\n",
            "Epoch 10/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.1654 - accuracy: 0.9447 - val_loss: 0.1214 - val_accuracy: 0.9667\n",
            "Epoch 11/20\n",
            "180/180 [==============================] - 5s 27ms/step - loss: 0.1584 - accuracy: 0.9474 - val_loss: 0.1189 - val_accuracy: 0.9682\n",
            "Epoch 12/20\n",
            "180/180 [==============================] - 3s 19ms/step - loss: 0.1469 - accuracy: 0.9519 - val_loss: 0.0997 - val_accuracy: 0.9719\n",
            "Epoch 13/20\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1371 - accuracy: 0.9566 - val_loss: 0.1065 - val_accuracy: 0.9707\n",
            "Epoch 14/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.1400 - accuracy: 0.9560 - val_loss: 0.1080 - val_accuracy: 0.9709\n",
            "Epoch 15/20\n",
            "180/180 [==============================] - 3s 19ms/step - loss: 0.1325 - accuracy: 0.9571 - val_loss: 0.0907 - val_accuracy: 0.9747\n",
            "Epoch 16/20\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1227 - accuracy: 0.9605 - val_loss: 0.0901 - val_accuracy: 0.9731\n",
            "Epoch 17/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.1206 - accuracy: 0.9610 - val_loss: 0.0938 - val_accuracy: 0.9749\n",
            "Epoch 18/20\n",
            "180/180 [==============================] - 3s 19ms/step - loss: 0.1154 - accuracy: 0.9621 - val_loss: 0.0848 - val_accuracy: 0.9762\n",
            "Epoch 19/20\n",
            "180/180 [==============================] - 4s 23ms/step - loss: 0.1116 - accuracy: 0.9645 - val_loss: 0.0798 - val_accuracy: 0.9782\n",
            "Epoch 20/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.1072 - accuracy: 0.9649 - val_loss: 0.0790 - val_accuracy: 0.9800\n"
          ]
        }
      ],
      "source": [
        "history2 = model2.fit(train_ds, epochs=20, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBGJ00IemUnH"
      },
      "source": [
        "From the model fitting history, the title-input only fake news classification reaches a **98.00%** validation accuracy, which is slightly higher than the training accuracy **96.49%**, performing a little bit worse than the title-only model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpMu8TXRmYdd"
      },
      "source": [
        "## Combining text and title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD4RX4ZgxQnS"
      },
      "source": [
        "We use [concatenate](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate) to combine the above two model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KnR0-z3k0nm",
        "outputId": "8003cb2a-d134-468d-f0a6-645df282776a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"both\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " title (InputLayer)             [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " text (InputLayer)              [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " text_vectorization (TextVector  (None, 500)         0           ['title[0][0]']                  \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " text_vectorization_1 (TextVect  (None, 500)         0           ['text[0][0]']                   \n",
            " orization)                                                                                       \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 500, 3)       6000        ['text_vectorization[0][0]',     \n",
            "                                                                  'text_vectorization_1[0][0]']   \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 500, 3)       0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 500, 3)       0           ['embedding[1][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 3)           0           ['dropout[0][0]']                \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 3)           0           ['dropout_2[0][0]']              \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 3)            0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 3)            0           ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 32)           128         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 32)           128         ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 64)           0           ['dense[0][0]',                  \n",
            "                                                                  'dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 32)           2080        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " fake (Dense)                   (None, 2)            66          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,402\n",
            "Trainable params: 8,402\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "both = layers.concatenate([title_layer, text_layer], axis=1)\n",
        "both = layers.Dense(32, activation='relu')(both)\n",
        "output = layers.Dense(2, name = \"fake\")(both)\n",
        "model3 = keras.Model(inputs = [title_in,text_in],outputs = output,name='both')\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AitelMOJzgrI"
      },
      "outputs": [],
      "source": [
        "model3.compile(optimizer=\"adam\",\n",
        "              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eiie0jmWzkQy",
        "outputId": "b8175600-05ec-4292-a8d1-1f061091bc7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "180/180 [==============================] - 5s 21ms/step - loss: 0.6377 - accuracy: 0.6446 - val_loss: 0.4093 - val_accuracy: 0.9131\n",
            "Epoch 2/20\n",
            "180/180 [==============================] - 5s 25ms/step - loss: 0.2503 - accuracy: 0.9271 - val_loss: 0.1526 - val_accuracy: 0.9618\n",
            "Epoch 3/20\n",
            "180/180 [==============================] - 4s 19ms/step - loss: 0.1470 - accuracy: 0.9570 - val_loss: 0.1071 - val_accuracy: 0.9707\n",
            "Epoch 4/20\n",
            "180/180 [==============================] - 5s 26ms/step - loss: 0.1141 - accuracy: 0.9665 - val_loss: 0.0888 - val_accuracy: 0.9776\n",
            "Epoch 5/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.0986 - accuracy: 0.9694 - val_loss: 0.0654 - val_accuracy: 0.9834\n",
            "Epoch 6/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.0909 - accuracy: 0.9730 - val_loss: 0.0627 - val_accuracy: 0.9867\n",
            "Epoch 7/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.0773 - accuracy: 0.9765 - val_loss: 0.0599 - val_accuracy: 0.9864\n",
            "Epoch 8/20\n",
            "180/180 [==============================] - 4s 21ms/step - loss: 0.0756 - accuracy: 0.9770 - val_loss: 0.0550 - val_accuracy: 0.9858\n",
            "Epoch 9/20\n",
            "180/180 [==============================] - 4s 23ms/step - loss: 0.0661 - accuracy: 0.9814 - val_loss: 0.0408 - val_accuracy: 0.9904\n",
            "Epoch 10/20\n",
            "180/180 [==============================] - 4s 21ms/step - loss: 0.0573 - accuracy: 0.9818 - val_loss: 0.0403 - val_accuracy: 0.9904\n",
            "Epoch 11/20\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.0586 - accuracy: 0.9822 - val_loss: 0.0433 - val_accuracy: 0.9911\n",
            "Epoch 12/20\n",
            "180/180 [==============================] - 4s 21ms/step - loss: 0.0581 - accuracy: 0.9823 - val_loss: 0.0336 - val_accuracy: 0.9933\n",
            "Epoch 13/20\n",
            "180/180 [==============================] - 4s 21ms/step - loss: 0.0499 - accuracy: 0.9838 - val_loss: 0.0409 - val_accuracy: 0.9911\n",
            "Epoch 14/20\n",
            "180/180 [==============================] - 4s 23ms/step - loss: 0.0509 - accuracy: 0.9845 - val_loss: 0.0336 - val_accuracy: 0.9916\n",
            "Epoch 15/20\n",
            "180/180 [==============================] - 4s 21ms/step - loss: 0.0468 - accuracy: 0.9847 - val_loss: 0.0279 - val_accuracy: 0.9929\n",
            "Epoch 16/20\n",
            "180/180 [==============================] - 5s 27ms/step - loss: 0.0450 - accuracy: 0.9852 - val_loss: 0.0284 - val_accuracy: 0.9922\n",
            "Epoch 17/20\n",
            "180/180 [==============================] - 4s 20ms/step - loss: 0.0429 - accuracy: 0.9850 - val_loss: 0.0246 - val_accuracy: 0.9936\n",
            "Epoch 18/20\n",
            "180/180 [==============================] - 5s 26ms/step - loss: 0.0395 - accuracy: 0.9869 - val_loss: 0.0253 - val_accuracy: 0.9940\n",
            "Epoch 19/20\n",
            "180/180 [==============================] - 4s 21ms/step - loss: 0.0426 - accuracy: 0.9845 - val_loss: 0.0236 - val_accuracy: 0.9955\n",
            "Epoch 20/20\n",
            "180/180 [==============================] - 4s 21ms/step - loss: 0.0407 - accuracy: 0.9864 - val_loss: 0.0206 - val_accuracy: 0.9964\n"
          ]
        }
      ],
      "source": [
        "history3 = model3.fit(train_ds, epochs=20, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYTCdV5m8TDX"
      },
      "source": [
        "We reached the highest validation accuracy so far - **99.64%** with a training accuracy of **98.64%** using both text and title. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOwcdDYd8pP8"
      },
      "source": [
        "## Model Evaluation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bSyl2k280u1"
      },
      "source": [
        "Let's examine how well our classification model performs on unforseen data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6aokvhCq8SN_"
      },
      "outputs": [],
      "source": [
        "test_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "y7qq-1Ej-11I"
      },
      "outputs": [],
      "source": [
        "testdf = pd.read_csv(test_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "x8oeb_3B-yas"
      },
      "outputs": [],
      "source": [
        "test = make_dataset(testdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQa6yW4u_kHG",
        "outputId": "2a7e8777-7bbb-4b5f-9c10-caa032b91222"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model3.metrics_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FzNuQOu-lLw",
        "outputId": "16216696-afa7-4710-d5d9-72494bc7a20a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "225/225 [==============================] - 2s 8ms/step - loss: 0.0765 - accuracy: 0.9815\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.07650567591190338, 0.9815136790275574]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model3.evaluate(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwhVOtLN_mAv"
      },
      "source": [
        "We achieved a **98.15% accuracy** in fake news classification on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFAc3HFZ_sQe"
      },
      "source": [
        "## Embedding Visualization\n",
        "\n",
        "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. One of the ways to learn word embedding is through an embedding layer, a word embedding that is learned jointly with a neural network model on a specific natural language processing task, such as fake news classification.\n",
        "\n",
        "We will use PCA (principal component analysis) to distill the embedding down to two dimensions for ease of visualization while perserving the variations among words.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "gYyIA6jsAHwX"
      },
      "outputs": [],
      "source": [
        "text_vectorize_layer.adapt(train_ds.map(lambda x, y: x[\"title\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "-ivlCtbW_EG6"
      },
      "outputs": [],
      "source": [
        "vocab = text_vectorize_layer.get_vocabulary() # keeps track of mapping from word to integer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TCBV44FPANZD"
      },
      "outputs": [],
      "source": [
        "weights = model3.get_layer(\"embedding\").get_weights()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5drJRanAV6o",
        "outputId": "8c070271-d0fb-40c9-971d-ae6715cdd795"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 3)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights.shape # 2000 vocabs x 3 dimensional space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "MGsHdyGpAaVb"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA \n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
        "# principal components analysis - \n",
        "# project things to lower dimension such that the variance of the dataset is most preserved\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "weights = pca.fit_transform(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4Koow2KWAgHO"
      },
      "outputs": [],
      "source": [
        "embedding_df = pd.DataFrame({\n",
        "    'word': vocab,\n",
        "    'x0': weights[:, 0],\n",
        "    'x1': weights[:, 1]\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Z1x7aDZuni"
      },
      "source": [
        "We proceed to color the embedding [KMeans(https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyAOF1_YV_Rd",
        "outputId": "3f849148-e7ef-4d64-eeb0-1e32142e4dd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 2.0610277e-01,  1.6437012e-03],\n",
              "       [-2.8306237e-01, -3.1798152e-04],\n",
              "       [ 5.2419913e-01, -1.2679024e-04],\n",
              "       [-5.6891716e-01, -5.1641576e-03],\n",
              "       [ 1.1591365e+00, -1.5719092e-02],\n",
              "       [-3.8863741e-02,  2.0008855e-03],\n",
              "       [-1.1274347e+00, -2.8834003e-03]], dtype=float32)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "X = embedding_df[['x0','x1']]\n",
        "kmeans = KMeans(n_clusters=7, random_state=0, n_init=\"auto\").fit(X)\n",
        "embedding_df['color'] = kmeans.labels_\n",
        "kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "GTVdA6ZMAleL",
        "outputId": "79299485-c9ae-41e4-a2f7-1686d622ae2b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"f4af6e93-2687-41d8-81da-3b8622f7dcee\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f4af6e93-2687-41d8-81da-3b8622f7dcee\")) {                    Plotly.newPlot(                        \"f4af6e93-2687-41d8-81da-3b8622f7dcee\",                        [{\"hovertemplate\":\"<b>%{hovertext}</b><br><br>x0=%{x}<br>x1=%{y}<br>size=%{marker.size}<br>color=%{marker.color}<extra></extra>\",\"hovertext\":[\"\",\"[UNK]\",\"said\",\"trump\",\"the\",\"us\",\"i\",\"would\",\"president\",\"people\",\"it\",\"one\",\"state\",\"also\",\"new\",\"donald\",\"states\",\"house\",\"government\",\"clinton\",\"he\",\"obama\",\"republican\",\"could\",\"told\",\"united\",\"in\",\"like\",\"white\",\"campaign\",\"we\",\"two\",\"last\",\"time\",\"news\",\"election\",\"party\",\"first\",\"this\",\"a\",\"even\",\"former\",\"year\",\"country\",\"but\",\"years\",\"that\",\"many\",\"hillary\",\"security\",\"media\",\"say\",\"political\",\"may\",\"national\",\"get\",\"make\",\"made\",\"since\",\"court\",\"law\",\"police\",\"american\",\"going\",\"republicans\",\"presidential\",\"percent\",\"and\",\"back\",\"bill\",\"democratic\",\"support\",\"administration\",\"senate\",\"week\",\"know\",\"think\",\"including\",\"north\",\"vote\",\"way\",\"officials\",\"russia\",\"america\",\"public\",\"trumps\",\"take\",\"group\",\"according\",\"office\",\"federal\",\"they\",\"called\",\"statement\",\"right\",\"world\",\"military\",\"foreign\",\"million\",\"want\",\"department\",\"saying\",\"washington\",\"well\",\"you\",\"see\",\"tuesday\",\"much\",\"says\",\"still\",\"tax\",\"congress\",\"part\",\"there\",\"day\",\"another\",\"minister\",\"wednesday\",\"russian\",\"friday\",\"if\",\"women\",\"work\",\"thursday\",\"asked\",\"go\",\"policy\",\"monday\",\"2016\",\"democrats\",\"need\",\"city\",\"war\",\"next\",\"china\",\"secretary\",\"committee\",\"rights\",\"deal\",\"americans\",\"black\",\"official\",\"help\",\"three\",\"whether\",\"general\",\"case\",\"never\",\"york\",\"order\",\"around\",\"leader\",\"show\",\"man\",\"on\",\"korea\",\"candidate\",\"took\",\"members\",\"use\",\"come\",\"power\",\"senator\",\"good\",\"countries\",\"without\",\"left\",\"really\",\"put\",\"she\",\"meeting\",\"report\",\"end\",\"used\",\"every\",\"times\",\"attack\",\"fbi\",\"intelligence\",\"trade\",\"money\",\"top\",\"month\",\"justice\",\"investigation\",\"change\",\"already\",\"decision\",\"reported\",\"information\",\"family\",\"syria\",\"groups\",\"twitter\",\"fact\",\"business\",\"days\",\"plan\",\"leaders\",\"long\",\"iran\",\"story\",\"several\",\"far\",\"conservative\",\"nuclear\",\"international\",\"voters\",\"interview\",\"here\",\"now\",\"months\",\"children\",\"south\",\"speech\",\"so\",\"however\",\"place\",\"something\",\"director\",\"as\",\"likely\",\"fox\",\"call\",\"among\",\"clear\",\"must\",\"health\",\"press\",\"social\",\"came\",\"believe\",\"agency\",\"program\",\"things\",\"move\",\"recent\",\"major\",\"might\",\"chief\",\"issue\",\"barack\",\"border\",\"home\",\"got\",\"killed\",\"is\",\"immigration\",\"control\",\"number\",\"least\",\"john\",\"islamic\",\"though\",\"act\",\"reporters\",\"school\",\"sunday\",\"him\",\"matter\",\"seen\",\"billion\",\"trying\",\"went\",\"earlier\",\"supporters\",\"found\",\"actually\",\"executive\",\"today\",\"post\",\"sanders\",\"great\",\"yet\",\"thing\",\"system\",\"spokesman\",\"later\",\"point\",\"become\",\"look\",\"nation\",\"real\",\"march\",\"win\",\"making\",\"set\",\"at\",\"away\",\"give\",\"keep\",\"added\",\"little\",\"past\",\"economic\",\"free\",\"working\",\"legal\",\"senior\",\"more\",\"for\",\"all\",\"let\",\"them\",\"january\",\"defense\",\"violence\",\"what\",\"muslim\",\"stop\",\"democrat\",\"july\",\"big\",\"prime\",\"ever\",\"four\",\"companies\",\"nothing\",\"attacks\",\"member\",\"2015\",\"issues\",\"not\",\"comment\",\"forces\",\"european\",\"following\",\"company\",\"nations\",\"local\",\"lawmakers\",\"taking\",\"held\",\"expected\",\"cruz\",\"no\",\"human\",\"opposition\",\"eu\",\"head\",\"illegal\",\"care\",\"to\",\"talks\",\"enough\",\"person\",\"action\",\"june\",\"given\",\"across\",\"supreme\",\"process\",\"known\",\"governor\",\"force\",\"high\",\"better\",\"sanctions\",\"gun\",\"legislation\",\"nominee\",\"continue\",\"others\",\"when\",\"un\",\"possible\",\"taken\",\"released\",\"source\",\"community\",\"financial\",\"wall\",\"majority\",\"woman\",\"night\",\"done\",\"judge\",\"men\",\"evidence\",\"course\",\"lot\",\"job\",\"history\",\"pay\",\"important\",\"wrote\",\"response\",\"reports\",\"life\",\"union\",\"anyone\",\"team\",\"open\",\"second\",\"run\",\"close\",\"face\",\"attorney\",\"10\",\"private\",\"wants\",\"plans\",\"conference\",\"ago\",\"20\",\"special\",\"budget\",\"question\",\"using\",\"university\",\"fight\",\"mexico\",\"ban\",\"debate\",\"refugees\",\"gop\",\"anything\",\"1\",\"despite\",\"watch\",\"air\",\"able\",\"comments\",\"lives\",\"mr\",\"iraq\",\"november\",\"staff\",\"accused\",\"syrian\",\"saturday\",\"email\",\"ryan\",\"early\",\"find\",\"best\",\"less\",\"efforts\",\"while\",\"letter\",\"someone\",\"agreement\",\"along\",\"death\",\"after\",\"calling\",\"race\",\"role\",\"instead\",\"sure\",\"putin\",\"five\",\"crisis\",\"behind\",\"region\",\"full\",\"announced\",\"current\",\"name\",\"future\",\"within\",\"due\",\"council\",\"weeks\",\"students\",\"visit\",\"service\",\"hard\",\"economy\",\"lead\",\"his\",\"event\",\"comes\",\"civil\",\"israel\",\"coming\",\"jobs\",\"with\",\"sources\",\"getting\",\"december\",\"effort\",\"facebook\",\"live\",\"congressional\",\"running\",\"saudi\",\"sent\",\"global\",\"2014\",\"nearly\",\"votes\",\"elections\",\"authorities\",\"allow\",\"candidates\",\"october\",\"rules\",\"talk\",\"line\",\"coalition\",\"citizens\",\"september\",\"young\",\"thousands\",\"chairman\",\"out\",\"center\",\"britain\",\"8\",\"problem\",\"relations\",\"comey\",\"ruling\",\"capital\",\"emails\",\"texas\",\"ties\",\"representatives\",\"paul\",\"army\",\"wanted\",\"needs\",\"street\",\"muslims\",\"middle\",\"hold\",\"position\",\"late\",\"politics\",\"immediately\",\"some\",\"leave\",\"means\",\"together\",\"led\",\"weapons\",\"central\",\"15\",\"daily\",\"everyone\",\"claims\",\"peace\",\"officers\",\"climate\",\"rule\",\"services\",\"east\",\"gave\",\"criminal\",\"tell\",\"based\",\"august\",\"outside\",\"began\",\"april\",\"showed\",\"liberal\",\"thought\",\"florida\",\"2017\",\"policies\",\"lost\",\"failed\",\"words\",\"questions\",\"cannot\",\"healthcare\",\"different\",\"start\",\"obamacare\",\"bush\",\"latest\",\"access\",\"2\",\"whose\",\"february\",\"tried\",\"turkey\",\"county\",\"do\",\"workers\",\"rather\",\"up\",\"decided\",\"message\",\"elected\",\"bad\",\"reform\",\"immigrants\",\"try\",\"read\",\"speaking\",\"morning\",\"spending\",\"almost\",\"district\",\"threat\",\"recently\",\"reason\",\"ministry\",\"strong\",\"millions\",\"conservatives\",\"organization\",\"six\",\"allowed\",\"parliament\",\"voting\",\"received\",\"hope\",\"george\",\"again\",\"laws\",\"everything\",\"sexual\",\"release\",\"racist\",\"idea\",\"freedom\",\"rally\",\"cut\",\"meet\",\"makes\",\"ahead\",\"list\",\"concerns\",\"met\",\"agencies\",\"involved\",\"her\",\"calls\",\"stand\",\"of\",\"kind\",\"often\",\"charges\",\"germany\",\"bring\",\"always\",\"happened\",\"enforcement\",\"planned\",\"allegations\",\"allies\",\"denied\",\"protect\",\"poll\",\"parties\",\"needed\",\"30\",\"energy\",\"shot\",\"entire\",\"nomination\",\"fire\",\"situation\",\"century\",\"side\",\"voted\",\"talking\",\"key\",\"europe\",\"industry\",\"market\",\"hate\",\"chinese\",\"looking\",\"shooting\",\"provide\",\"especially\",\"bank\",\"james\",\"funding\",\"seems\",\"presidency\",\"movement\",\"host\",\"oil\",\"realdonaldtrump\",\"fake\",\"claim\",\"large\",\"include\",\"vice\",\"officer\",\"agreed\",\"3\",\"representative\",\"j\",\"either\",\"missile\",\"data\",\"spoke\",\"although\",\"personal\",\"s\",\"adding\",\"room\",\"near\",\"12\",\"old\",\"area\",\"small\",\"step\",\"hearing\",\"arrested\",\"west\",\"hours\",\"actions\",\"hit\",\"confirmed\",\"shows\",\"insurance\",\"terrorist\",\"me\",\"request\",\"water\",\"cases\",\"probably\",\"feel\",\"forward\",\"serious\",\"2012\",\"fighting\",\"return\",\"decades\",\"clearly\",\"address\",\"worked\",\"leading\",\"western\",\"polls\",\"term\",\"true\",\"wrong\",\"interest\",\"protesters\",\"british\",\"biggest\",\"front\",\"5\",\"documents\",\"california\",\"potential\",\"wife\",\"terrorism\",\"foundation\",\"appeared\",\"travel\",\"moscow\",\"main\",\"tweet\",\"crime\",\"board\",\"building\",\"alleged\",\"claimed\",\"families\",\"commission\",\"result\",\"korean\",\"love\",\"record\",\"david\",\"25\",\"paid\",\"declined\",\"fired\",\"relationship\",\"adviser\",\"soon\",\"simply\",\"review\",\"pressure\",\"passed\",\"turned\",\"nov\",\"issued\",\"food\",\"continued\",\"11\",\"mean\",\"started\",\"guy\",\"network\",\"leadership\",\"details\",\"posted\",\"dollars\",\"saw\",\"raised\",\"pretty\",\"included\",\"attempt\",\"previously\",\"tillerson\",\"spent\",\"taxes\",\"aid\",\"toward\",\"short\",\"mike\",\"truth\",\"mark\",\"signed\",\"myanmar\",\"college\",\"turn\",\"michael\",\"brought\",\"victory\",\"primary\",\"mccain\",\"influence\",\"bernie\",\"father\",\"forced\",\"points\",\"independence\",\"became\",\"seeking\",\"final\",\"hand\",\"total\",\"4\",\"sign\",\"deputy\",\"religious\",\"level\",\"half\",\"currently\",\"2013\",\"child\",\"incident\",\"popular\",\"friends\",\"longer\",\"account\",\"article\",\"proposed\",\"pass\",\"our\",\"rubio\",\"push\",\"protest\",\"view\",\"an\",\"whole\",\"sessions\",\"hundreds\",\"giving\",\"agenda\",\"21st\",\"protests\",\"obamas\",\"phone\",\"mayor\",\"town\",\"these\",\"conflict\",\"created\",\"arabia\",\"changes\",\"areas\",\"ambassador\",\"ted\",\"largest\",\"independent\",\"clintons\",\"armed\",\"remarks\",\"regional\",\"lawyer\",\"increase\",\"described\",\"san\",\"third\",\"else\",\"son\",\"repeatedly\",\"published\",\"criticized\",\"helped\",\"pence\",\"respond\",\"heard\",\"18\",\"education\",\"constitution\",\"flynn\",\"website\",\"speaker\",\"cia\",\"speak\",\"fund\",\"debt\",\"victims\",\"living\",\"hands\",\"ask\",\"firm\",\"reality\",\"example\",\"troops\",\"convention\",\"seven\",\"medical\",\"programs\",\"rate\",\"violent\",\"build\",\"apparently\",\"funds\",\"militants\",\"similar\",\"secret\",\"spokeswoman\",\"iraqi\",\"cost\",\"inside\",\"robert\",\"goes\",\"by\",\"telling\",\"carolina\",\"24\",\"remain\",\"warned\",\"criticism\",\"absolutely\",\"single\",\"mass\",\"crowd\",\"opinion\",\"100\",\"understand\",\"quickly\",\"appears\",\"refugee\",\"german\",\"employees\",\"then\",\"lower\",\"photo\",\"fear\",\"television\",\"risk\",\"completely\",\"asking\",\"how\",\"base\",\"radio\",\"experts\",\"land\",\"proposal\",\"joe\",\"form\",\"focus\",\"businesses\",\"merkel\",\"tv\",\"politicians\",\"individuals\",\"research\",\"previous\",\"urged\",\"are\",\"interests\",\"stay\",\"happen\",\"isis\",\"discuss\",\"13\",\"served\",\"online\",\"events\",\"voter\",\"nato\",\"france\",\"concern\",\"problems\",\"its\",\"safe\",\"mainstream\",\"leaving\",\"presidentelect\",\"northern\",\"japan\",\"dangerous\",\"fellow\",\"7\",\"numbers\",\"consider\",\"king\",\"ground\",\"results\",\"6\",\"tweeted\",\"southern\",\"page\",\"senators\",\"seek\",\"rep\",\"died\",\"parents\",\"coverage\",\"christian\",\"share\",\"project\",\"16\",\"measures\",\"prevent\",\"johnson\",\"island\",\"student\",\"prison\",\"why\",\"flag\",\"corruption\",\"safety\",\"down\",\"respect\",\"presidents\",\"create\",\"charged\",\"french\",\"development\",\"exactly\",\"certainly\",\"cause\",\"governments\",\"church\",\"transition\",\"powerful\",\"committed\",\"society\",\"ready\",\"internet\",\"drug\",\"choice\",\"responsible\",\"concerned\",\"attention\",\"who\",\"50\",\"poor\",\"were\",\"provided\",\"certain\",\"gets\",\"al\",\"moment\",\"answer\",\"trip\",\"threats\",\"reporter\",\"fraud\",\"critical\",\"considered\",\"book\",\"14\",\"expressed\",\"residents\",\"moore\",\"kurdish\",\"knew\",\"assault\",\"responded\",\"diplomatic\",\"series\",\"named\",\"ensure\",\"yes\",\"mother\",\"eight\",\"holding\",\"false\",\"measure\",\"target\",\"knows\",\"kelly\",\"sides\",\"22\",\"mcconnell\",\"filed\",\"chance\",\"terrorists\",\"refused\",\"organizations\",\"because\",\"terms\",\"husband\",\"threatened\",\"brexit\",\"approved\",\"amendment\",\"9\",\"takes\",\"hear\",\"panel\",\"expect\",\"charge\",\"schools\",\"believed\",\"backed\",\"worst\",\"beyond\",\"activists\",\"impact\",\"difficult\",\"cuts\",\"standing\",\"views\",\"affairs\",\"referendum\",\"ordered\",\"serve\",\"radical\",\"paris\",\"favor\",\"cities\",\"growing\",\"reached\",\"dead\",\"xi\",\"just\",\"class\",\"democracy\",\"complete\",\"population\",\"o\",\"london\",\"play\",\"negotiations\",\"abortion\",\"rhetoric\",\"low\",\"terror\",\"records\",\"direct\",\"agents\",\"parts\",\"god\",\"courts\",\"repeal\",\"offered\",\"massive\",\"send\",\"agree\",\"operations\",\"virginia\",\"critics\",\"afghanistan\",\"newspaper\",\"suggested\",\"rest\",\"protection\",\"individual\",\"believes\",\"before\",\"about\",\"progress\",\"sought\",\"investment\",\"statements\",\"guns\",\"ability\",\"maybe\",\"28\",\"gay\",\"car\",\"cabinet\",\"ways\",\"huge\",\"status\",\"offer\",\"corporate\",\"17\",\"strategy\",\"behavior\",\"exchange\",\"screen\",\"common\",\"sean\",\"27\",\"finally\",\"authority\",\"2018\",\"red\",\"opportunity\",\"my\",\"defend\",\"attacked\",\"additional\",\"21\",\"lack\",\"supporting\",\"per\",\"willing\",\"higher\",\"supported\",\"joint\",\"remains\",\"killing\",\"counsel\",\"rohingya\",\"dnc\",\"continues\",\"jerusalem\",\"beijing\",\"weekend\",\"w\",\"related\",\"inc\",\"body\",\"arrest\",\"puerto\",\"caused\",\"word\",\"includes\",\"raise\",\"publicly\",\"test\",\"gas\",\"regarding\",\"quite\",\"domestic\",\"2011\",\"michigan\",\"credit\",\"costs\",\"chicago\",\"gone\",\"regulations\",\"promised\",\"crimes\",\"during\",\"star\",\"mexican\",\"cover\",\"scandal\",\"perhaps\",\"significant\",\"lose\",\"environmental\",\"spicer\",\"labor\",\"announcement\",\"rich\",\"income\",\"effect\",\"sense\",\"avoid\",\"period\",\"replace\",\"referring\",\"reach\",\"operation\",\"finance\",\"towards\",\"block\",\"iranian\",\"fiscal\",\"considering\",\"arms\",\"jan\",\"directly\",\"canada\",\"supporter\",\"accept\",\"looks\",\"2010\",\"2008\",\"worth\",\"becoming\",\"appear\",\"particularly\",\"multiple\",\"challenge\",\"worse\",\"chris\",\"remember\",\"opposed\",\"mostly\",\"declared\",\"arab\",\"fair\",\"turkish\",\"capture\",\"lawsuit\",\"putting\",\"israeli\",\"decide\",\"noted\",\"kim\",\"socalled\",\"showing\",\"cyber\",\"summit\",\"dc\",\"buy\",\"growth\",\"sen\",\"seem\",\"approval\",\"19\",\"upon\",\"thinks\",\"join\",\"followed\",\"from\",\"electoral\",\"citing\",\"rise\",\"facts\",\"daughter\",\"oct\",\"deep\",\"necessary\",\"macron\",\"lies\",\"kids\",\"association\",\"battle\",\"lawyers\",\"launched\",\"shut\",\"sept\",\"cooperation\",\"mind\",\"responsibility\",\"legislative\",\"friend\",\"language\",\"stage\",\"soldiers\",\"subject\",\"minority\",\"journalists\",\"was\",\"vladimir\",\"trust\",\"reporting\",\"green\",\"steve\",\"jeff\",\"communications\",\"facing\",\"cuba\",\"tough\",\"probe\",\"establishment\",\"required\",\"over\",\"available\",\"aimed\",\"rival\",\"26\",\"sea\",\"largely\",\"begin\",\"other\",\"emergency\",\"yemen\",\"delegates\",\"eastern\",\"conspiracy\",\"discussed\",\"deals\",\"banks\",\"accusations\",\"ended\",\"regime\",\"price\",\"blame\",\"meetings\",\"ally\",\"pyongyang\",\"positions\",\"23\",\"infrastructure\",\"ruled\",\"ohio\",\"manager\",\"homeland\",\"trial\",\"various\",\"seriously\",\"hollywood\",\"communities\",\"transgender\",\"light\",\"did\",\"classified\",\"both\",\"ran\",\"meant\",\"kill\",\"revealed\",\"professor\",\"jr\",\"decisions\",\"winning\",\"briefing\",\"break\",\"sex\",\"approach\",\"alabama\",\"migrants\",\"helping\",\"bannon\",\"reasons\",\"goal\",\"follow\",\"caught\",\"most\",\"embassy\",\"de\",\"site\",\"separate\",\"powers\",\"investigating\",\"hill\",\"couple\",\"amid\",\"alliance\",\"tensions\",\"scheduled\",\"be\",\"accounts\",\"played\",\"moving\",\"joined\",\"conversation\",\"wikileaks\",\"property\",\"libya\",\"appeal\",\"bit\",\"will\",\"hurt\",\"ukraine\",\"those\",\"technology\",\"jones\",\"estate\",\"murder\",\"judges\",\"version\",\"pick\",\"tweets\",\"throughout\",\"prosecutors\",\"moved\",\"experience\",\"acting\",\"controversial\",\"constitutional\",\"racism\",\"nbc\",\"propaganda\",\"too\",\"paying\",\"cast\",\"appeals\",\"islam\",\"fully\",\"guilty\",\"condition\",\"carry\",\"stated\",\"lie\",\"african\",\"resolution\",\"values\",\"supposed\",\"mission\",\"investigations\",\"pm\",\"damage\",\"bureau\",\"amount\",\"allowing\",\"removed\",\"rejected\",\"benefits\",\"positive\",\"shared\",\"demand\",\"arizona\",\"average\",\"pushed\",\"game\",\"changed\",\"alone\",\"coal\",\"29\",\"hospital\",\"rightwing\",\"pointed\",\"planning\",\"literally\",\"sentence\",\"scott\",\"doubt\",\"treasury\",\"felt\",\"argued\",\"addition\",\"40\",\"figure\",\"stopped\",\"broke\",\"targeted\",\"cited\",\"written\",\"lady\",\"jail\",\"dropped\",\"billionaire\",\"present\",\"heart\",\"claiming\",\"seat\",\"bid\",\"60\",\"fall\",\"hopes\",\"borders\",\"2009\",\"familiar\",\"carried\",\"career\",\"piece\",\"msnbc\",\"focused\",\"annual\",\"voice\",\"rico\",\"partner\",\"identified\",\"age\",\"warning\",\"im\",\"angry\",\"africa\",\"beginning\",\"steps\",\"promise\",\"born\",\"taiwan\",\"below\",\"activities\",\"resources\",\"female\",\"please\",\"conditions\",\"brown\",\"romney\",\"faced\",\"closed\",\"aides\",\"under\",\"reduce\",\"picture\",\"featured\",\"stories\",\"russians\",\"mueller\",\"highly\",\"sarah\",\"matters\",\"treatment\",\"religion\",\"none\",\"document\",\"confirmation\",\"zone\",\"territory\",\"particular\",\"lying\",\"defeat\",\"decade\",\"internal\",\"campus\",\"abc\",\"whatever\",\"possibly\",\"pentagon\",\"analysis\",\"victim\",\"uk\",\"solution\",\"investors\",\"explain\",\"designed\",\"audience\",\"lebanon\",\"judicial\",\"conduct\",\"compared\",\"ceo\",\"biden\",\"works\",\"opponents\",\"surprise\",\"increased\",\"disaster\",\"carson\",\"girl\",\"province\",\"t\",\"sort\",\"dozens\",\"strike\",\"require\",\"kept\",\"institute\",\"happy\",\"entering\",\"warren\",\"or\",\"bringing\",\"basis\",\"veterans\",\"restrictions\",\"requests\",\"possibility\",\"unless\",\"server\",\"management\",\"losing\",\"judiciary\",\"frontrunner\",\"iowa\",\"thats\",\"minutes\",\"spain\",\"playing\",\"islamist\",\"hannity\",\"bills\",\"drew\",\"affordable\",\"prosecutor\",\"highest\",\"hacking\",\"australia\",\"inauguration\",\"bloc\",\"systems\",\"starting\",\"receive\",\"dec\",\"partners\",\"save\",\"militant\",\"faces\",\"nine\",\"racial\",\"negative\",\"totally\",\"specific\",\"suspected\",\"discussion\",\"sales\",\"numerous\",\"illegally\",\"humanitarian\",\"very\",\"scene\",\"flint\",\"built\",\"airport\",\"pushing\",\"clean\",\"behalf\",\"arrived\",\"detained\",\"collusion\",\"prior\",\"ongoing\",\"martin\",\"allegedly\",\"search\",\"involvement\",\"intended\",\"hotel\",\"markets\",\"happening\",\"vowed\",\"campaigns\",\"kushner\",\"field\",\"eventually\",\"opened\",\"hell\",\"bangladesh\",\"ad\",\"watching\",\"remove\",\"reforms\",\"prepared\",\"loss\",\"asia\",\"offensive\",\"note\",\"jim\",\"erdogan\",\"sometimes\",\"secure\",\"dr\",\"associated\",\"assembly\",\"admitted\",\"ones\",\"housing\",\"wisconsin\",\"visa\",\"ultimately\",\"thank\",\"testimony\",\"mattis\",\"wait\",\"miles\",\"leftist\",\"ben\",\"progressive\",\"keeping\",\"brussels\",\"activist\",\"parenthood\",\"destroy\",\"conducted\",\"correct\",\"listen\",\"bomb\",\"thinking\",\"increasingly\",\"31\",\"aide\",\"reportedly\",\"benefit\",\"agent\",\"backing\",\"seemed\",\"polling\",\"palestinian\",\"medicaid\",\"happens\",\"streets\",\"greater\",\"getty\",\"fighters\",\"assistance\",\"administrations\",\"immediate\",\"explained\",\"truly\",\"prices\",\"attempted\",\"worried\",\"resignation\",\"looked\",\"levels\",\"jersey\",\"tom\",\"stance\",\"innocent\",\"estimated\",\"epa\",\"contact\",\"cash\",\"study\",\"sitting\",\"republic\",\"raising\",\"shown\",\"richard\",\"relief\",\"production\",\"paper\",\"mention\",\"fuel\",\"communist\",\"appointed\",\"names\",\"limited\",\"evening\",\"rates\",\"rape\",\"marriage\",\"lines\",\"door\",\"choose\",\"treated\",\"pakistan\",\"liberals\",\"institutions\",\"wounded\",\"eric\",\"diplomats\",\"aware\",\"training\",\"spend\",\"prove\",\"path\",\"2017realdonaldtrump\",\"otherwise\",\"orders\",\"guard\",\"extremely\",\"alternative\",\"detroit\",\"check\",\"capitol\",\"21wire\",\"drop\",\"conway\",\"citizen\",\"christmas\",\"blocked\",\"abuse\",\"vehicle\",\"politically\",\"payments\",\"oh\",\"nobody\",\"drive\",\"don\",\"vietnam\",\"tells\",\"kremlin\",\"gives\",\"actual\",\"stood\",\"easy\",\"boost\",\"strongly\",\"girls\",\"ethnic\",\"draft\",\"standards\",\"serving\",\"seats\",\"investigators\",\"heads\",\"christie\",\"boy\",\"broadcast\",\"gender\",\"attend\",\"google\",\"failing\",\"confidence\",\"park\",\"failure\",\"dismissed\",\"activity\",\"platform\",\"neither\",\"natural\",\"hurricane\",\"advance\",\"road\",\"potentially\",\"off\",\"thanks\",\"strikes\",\"signs\",\"retired\",\"blamed\",\"attempts\",\"andrew\",\"yesterday\",\"strategic\",\"screenshot\",\"pledged\",\"floor\",\"fine\",\"providing\",\"does\",\"date\",\"simple\",\"products\",\"civilians\",\"camp\",\"suspect\",\"35\",\"sweden\",\"pennsylvania\",\"catalan\",\"asylum\",\"wonder\",\"involving\",\"interior\",\"culture\",\"investigate\",\"attended\",\"assad\",\"posts\",\"missiles\",\"links\",\"lee\",\"injured\",\"dialogue\",\"prominent\",\"projects\",\"opening\",\"places\",\"learned\",\"have\",\"grant\",\"appearance\",\"carrying\",\"widely\",\"seeing\",\"drugs\",\"denies\",\"code\",\"chair\",\"wage\",\"venezuela\",\"surveillance\",\"sheriff\",\"session\",\"ridiculous\",\"faith\",\"praised\",\"homes\",\"fed\",\"successful\",\"overseas\",\"minimum\",\"material\",\"hes\",\"figures\",\"schumer\",\"convicted\",\"congressman\",\"temporary\",\"sell\",\"proof\",\"players\",\"illinois\",\"taxpayers\",\"sector\",\"promote\",\"attacking\",\"their\",\"st\",\"narrative\",\"wealthy\",\"station\",\"commitment\",\"tehran\",\"returned\",\"proposals\",\"presence\",\"improve\",\"hour\",\"heavily\",\"environment\",\"donors\",\"chuck\",\"veteran\",\"space\",\"himself\",\"ethics\",\"discrimination\",\"oregon\",\"linked\",\"hall\",\"recognize\",\"range\",\"khan\",\"journal\",\"graham\",\"fell\",\"deeply\",\"banned\",\"youtube\",\"unlikely\",\"prince\",\"consequences\",\"club\",\"can\",\"benghazi\",\"navy\",\"knowledge\",\"ballot\",\"ballistic\",\"wearing\",\"waiting\",\"spread\",\"shift\",\"magazine\",\"zero\",\"marco\"],\"legendgroup\":\"\",\"marker\":{\"color\":[5,5,4,1,2,2,0,5,0,0,5,5,5,5,0,2,0,0,2,5,5,3,2,0,2,0,2,3,0,0,0,0,0,5,5,0,5,0,3,2,3,0,2,0,2,2,5,0,3,5,5,5,5,1,0,5,5,5,5,0,0,0,3,1,5,2,0,1,5,5,0,0,1,2,0,1,0,0,5,5,1,5,5,3,5,4,5,5,1,0,1,1,0,2,5,5,5,0,0,1,3,2,3,5,3,5,4,5,3,0,2,5,5,5,1,1,2,4,5,4,0,0,0,4,0,1,1,4,1,5,5,0,5,0,0,0,0,0,5,3,5,5,1,0,2,0,5,1,5,5,0,0,5,1,0,5,0,1,1,5,5,5,0,5,0,5,3,3,1,5,5,1,0,5,1,1,1,3,5,2,3,0,2,2,0,5,1,1,0,1,5,5,0,2,3,5,0,0,0,0,5,3,1,5,5,0,0,5,0,3,1,0,0,0,5,1,1,5,3,0,5,5,6,5,2,5,0,0,6,0,5,3,2,5,3,5,5,0,5,2,5,4,0,5,3,2,3,5,0,1,5,3,0,5,1,4,5,2,1,3,2,0,1,3,2,0,5,3,0,3,1,1,0,1,1,5,2,2,1,1,5,3,1,1,1,0,0,0,5,5,1,0,1,1,0,1,5,0,5,6,0,1,3,0,1,5,0,3,3,1,5,5,5,0,5,0,0,3,0,1,0,1,3,2,2,2,2,0,0,0,0,0,0,0,0,5,5,2,0,2,5,3,1,2,1,1,1,1,1,5,1,0,0,5,1,0,0,5,1,0,5,5,2,5,0,0,1,5,2,1,5,5,1,3,3,5,1,5,5,1,5,0,5,5,0,2,1,3,5,0,6,1,5,2,5,0,0,5,5,0,5,5,0,0,0,1,0,3,1,2,1,0,5,0,1,6,1,5,5,6,0,5,5,1,6,0,5,2,2,5,2,5,5,0,5,1,0,5,0,5,1,0,5,1,5,1,2,0,1,3,0,5,2,5,2,1,3,5,0,0,3,0,5,5,5,0,0,2,5,2,2,0,6,0,5,1,5,2,5,1,3,3,3,1,2,1,0,5,0,0,1,5,1,2,5,5,1,2,5,1,0,1,0,5,0,0,5,5,2,2,5,5,5,2,2,1,5,5,4,5,0,5,1,1,1,0,0,3,0,0,5,4,5,0,2,2,5,2,0,6,1,5,2,1,5,0,0,0,5,1,3,5,1,0,3,3,4,3,1,0,3,0,0,5,3,0,0,2,5,0,0,1,5,5,1,2,3,5,2,3,1,0,1,5,3,5,5,5,0,5,5,6,0,6,0,5,1,1,6,1,4,1,1,5,5,5,1,2,1,1,0,1,5,0,3,0,5,3,1,0,5,5,2,3,2,3,0,0,2,3,0,5,3,1,3,2,0,0,5,5,3,5,1,0,0,2,5,0,0,0,0,5,1,6,0,5,0,3,1,1,1,5,3,0,2,3,5,0,3,0,5,2,5,0,6,5,5,3,0,2,0,3,0,1,0,1,2,1,0,6,0,5,2,1,0,1,6,2,1,5,5,3,5,0,0,0,0,0,0,1,5,0,1,0,3,2,0,5,2,3,0,5,3,2,0,0,0,5,0,0,5,5,0,5,3,1,5,5,2,0,1,1,5,0,0,5,0,1,0,5,2,4,0,5,1,0,3,6,5,2,1,5,1,0,1,0,5,2,2,5,0,0,1,0,0,5,0,4,1,5,3,0,5,5,3,3,5,5,2,3,5,0,3,5,1,1,0,1,0,0,0,1,0,3,3,0,4,5,5,5,2,5,3,1,0,1,1,1,5,2,5,0,0,1,5,1,0,2,2,5,0,3,5,5,5,1,1,5,0,0,2,0,5,0,1,5,5,2,1,0,5,1,3,6,5,4,1,3,0,3,2,5,0,2,0,5,5,0,0,4,5,2,2,5,5,0,0,5,3,0,5,5,2,0,0,2,0,1,0,5,1,2,0,1,0,5,5,5,1,0,5,5,1,1,5,0,0,0,5,0,3,0,6,0,4,0,1,2,0,1,1,5,3,1,3,1,5,5,0,0,1,1,5,5,0,0,1,2,6,1,5,1,5,0,3,5,2,5,1,1,1,5,2,0,0,5,3,0,5,0,0,2,5,5,5,2,4,3,1,5,5,3,0,0,0,0,5,1,0,2,2,2,4,0,1,0,2,2,2,0,0,5,1,5,5,0,5,5,3,0,3,0,0,6,2,2,2,1,5,0,5,0,5,1,0,1,5,3,3,2,0,5,5,2,1,5,0,5,3,1,5,2,5,0,0,5,5,0,0,0,5,5,2,1,3,5,2,4,1,5,6,5,3,1,5,1,1,0,1,0,0,3,4,2,1,2,1,1,3,0,5,2,0,3,0,0,0,1,2,5,1,5,0,5,5,5,0,3,6,0,3,0,5,5,2,0,1,5,1,1,2,1,5,5,0,0,5,5,0,0,0,5,1,2,5,2,2,0,6,5,1,0,0,1,5,2,3,3,2,5,0,6,4,0,0,5,5,1,6,3,5,1,2,0,0,2,5,3,5,1,0,5,2,2,2,5,6,5,3,1,0,5,2,2,0,4,5,5,5,5,0,2,0,0,3,0,5,5,1,2,1,5,6,0,1,5,3,0,2,0,1,0,1,5,2,1,5,3,1,1,2,0,2,0,5,5,4,3,3,5,0,5,1,5,2,2,0,0,1,1,1,0,3,5,0,3,5,5,5,3,5,5,3,3,0,3,2,1,5,5,1,5,3,0,5,0,1,0,1,3,0,3,0,0,0,0,4,0,5,0,3,5,3,2,1,3,2,5,0,1,0,5,5,0,1,5,5,0,3,0,1,1,5,5,5,5,5,5,2,6,0,3,5,3,5,5,0,1,5,5,3,2,0,6,1,5,1,1,3,0,0,5,5,4,1,5,1,2,5,0,2,1,3,0,5,2,0,1,2,2,5,2,2,1,5,2,2,0,5,0,1,0,0,0,5,1,1,0,1,2,0,2,1,3,0,0,2,4,5,2,2,3,0,0,0,5,2,3,2,0,2,2,0,5,0,3,2,2,0,5,1,5,0,5,0,5,2,5,5,3,2,0,5,1,0,0,1,3,3,3,5,3,5,5,2,1,0,0,5,1,1,5,1,0,5,1,5,3,0,5,2,1,0,3,5,2,2,0,1,0,4,0,5,5,5,1,3,5,5,5,5,0,2,0,0,1,5,0,2,0,0,1,3,2,2,2,1,6,5,3,1,1,1,3,0,5,0,4,0,1,5,6,3,2,5,0,3,5,0,1,2,0,3,1,3,2,1,2,1,1,1,5,5,2,0,1,5,1,0,1,1,0,3,0,5,5,2,0,1,3,0,5,0,0,5,0,5,3,5,5,0,3,0,3,0,2,5,5,0,1,5,0,2,0,3,1,0,2,5,0,5,0,2,1,4,1,2,1,5,0,5,0,3,5,5,1,3,0,3,1,2,2,0,2,2,3,3,0,3,1,1,0,5,0,1,3,0,2,1,0,3,1,5,2,5,3,1,1,3,5,1,1,3,0,2,3,1,0,2,1,0,0,1,1,0,2,1,1,5,1,3,2,3,3,2,1,0,1,0,5,5,1,3,5,2,1,2,0,5,3,5,2,5,0,0,0,4,5,4,1,5,3,0,0,5,2,5,5,0,5,2,0,0,0,2,0,5,2,2,0,1,1,0,5,2,0,0,3,0,0,2,1,1,2,0,1,1,3,1,0,0,0,1,5,6,2,0,1,0,2,3,0,0,5,0,1,0,5,2,3,0,5,0,0,1,0,1,3,0,2,2,5,3,3,2,3,5,1,5,1,5,0,5,0,1,2,6,5,3,0,2,5,1,6,1,1,3,2,1,5,0,2,6,1,1,0,1,0,0,5,3,2,1,6,0,5,4,0,3,3,0,5,2,2,2,2,5,1,2,3,5,5,5,1,0,1,5,0,0,2,5,2,1,1,2,5,1,5,5,5,2,1,0,1,5,3,0,0,3,0,0,5,1,1,1,2,3,2,1,5,0,0,5,0,1,3,3,6,5,1,1,5,0,5,0,1,5,3,5,5,3,0,1,0,5,6,5,0,2,0,5,4,2,0,0,5,5,2,5,1,0,0,5,5,1,0,1,5,5,5,1,1,1,2,2,5,0,5,3,0,5,2,1,5,5,6,0,6,1,0,5,0,1,0,5,0,5,5,5,5,5,5,4,1,6,0,0,0,5,1,5,2,1,2,0,0,2,0,0,2,3,3,3,1,5,5,2,1,0,4,1,1,0,2,5,3,5,5,1,2,0,5,5,0,5,0,4,0,1,5,3,2,0,5,1,5,3,0,0,3,5,5,3,5,2,5,0,2,2,5,2,0,0,0,1,5,1,0,3,1,5,1,2,0,1,5,1,3,1,2,5,2,1,5,2,0,5,5,1,5,3,0,5,0,1,0,0,5,1,5],\"coloraxis\":\"coloraxis\",\"size\":[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],\"sizemode\":\"area\",\"sizeref\":0.02,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"showlegend\":false,\"x\":[0.021007444709539413,-0.03006669692695141,0.9809558391571045,-0.29240140318870544,0.5755254626274109,0.46790647506713867,0.27171164751052856,-0.04878242313861847,0.3110029399394989,0.18021830916404724,-0.0069916220381855965,-0.08122322708368301,0.042668476700782776,-0.12247058749198914,0.15523944795131683,0.5608217716217041,0.21012605726718903,0.13450397551059723,0.38501885533332825,-0.0679638534784317,-0.15383408963680267,-0.7442069053649902,0.5028445720672607,0.08601568639278412,0.4189518392086029,0.20785249769687653,0.7217730283737183,-0.5704789757728577,0.09740176796913147,0.19201985001564026,0.163110613822937,0.23202382028102875,0.23174689710140228,-0.14113090932369232,-0.1546383500099182,0.31489086151123047,-0.08131110668182373,0.25394153594970703,-0.4982149004936218,0.6985302567481995,-0.8101198077201843,0.358232319355011,0.38386186957359314,0.2353643774986267,0.7277406454086304,0.40014803409576416,-0.07087192684412003,0.23847977817058563,-0.6552916765213013,0.052951183170080185,0.058754850178956985,0.08077199012041092,-0.10866612195968628,-0.19715677201747894,0.11213098466396332,0.004186418838799,-0.03181685507297516,-0.10142095386981964,0.0036121648736298084,0.08738792687654495,0.1446564793586731,0.15116040408611298,-0.7421187162399292,-0.1864083856344223,-0.11286607384681702,0.6820686459541321,0.3116403818130493,-0.3886251151561737,-0.023699212819337845,-0.1079784482717514,0.3399442732334137,0.11832936853170395,-0.3238796293735504,0.38033685088157654,0.16034415364265442,-0.28836843371391296,0.14328525960445404,0.26107892394065857,-0.08345714956521988,-0.06975868344306946,-0.1729736030101776,-0.15455643832683563,-0.0292651504278183,-0.735354483127594,-0.0657370388507843,2.410391092300415,-0.06425686180591583,-0.00754222646355629,-0.19685839116573334,0.36324042081832886,-0.21449221670627594,-0.23770879209041595,0.11103598028421402,0.7162831425666809,-0.14649170637130737,-0.10602618753910065,-0.13434235751628876,0.2625653147697449,0.316720575094223,-0.1729254573583603,-0.5224820971488953,0.6705771684646606,-0.4770059585571289,-0.1254529356956482,-0.5765857100486755,0.0007538576028309762,1.088493824005127,-0.151693657040596,-0.6636121273040771,0.26773735880851746,0.465769499540329,0.07252994924783707,0.03546854853630066,-0.08044702559709549,-0.24738606810569763,-0.21647684276103973,0.5784162878990173,1.2907862663269043,-0.0924578458070755,1.0802435874938965,0.1428283452987671,0.08434690535068512,0.14873440563678741,1.2852576971054077,0.2753340005874634,-0.4040307104587555,-0.19390901923179626,0.9981157779693604,-0.23062507808208466,-0.11772260069847107,-0.006577562540769577,0.2525120973587036,0.032594069838523865,0.1933479607105255,0.3131873905658722,0.09761875122785568,0.13793949782848358,0.09934232383966446,0.04216630011796951,-0.47081103920936584,-0.1006072387099266,-0.1358364075422287,-0.17867949604988098,0.26344966888427734,0.5991075038909912,0.3596671521663666,0.00361050246283412,-0.1751551479101181,0.00442884536460042,-0.015168015845119953,0.32486552000045776,0.30712294578552246,0.03613623231649399,-0.19496984779834747,0.1576903611421585,0.0008003313560038805,0.21036826074123383,-0.30157479643821716,-0.2164444923400879,0.0142181646078825,-0.12292996793985367,0.011593042872846127,0.16306756436824799,-0.07475394755601883,0.09260949492454529,-0.044682830572128296,-0.43947938084602356,-0.46249130368232727,-0.1925531029701233,-0.03586771711707115,-0.1037478819489479,-0.3416200578212738,0.13826240599155426,-0.00835681613534689,-0.16163764894008636,-0.33297061920166016,-0.16482995450496674,-0.46260377764701843,-0.11204562336206436,0.8038428425788879,-0.48957860469818115,0.1934039294719696,0.5511507987976074,0.4421423077583313,0.10851718485355377,0.008672244846820831,-0.2850337028503418,-0.25604888796806335,0.1320352852344513,-0.22904129326343536,0.07950206845998764,0.004058314487338066,0.3020598590373993,0.7824439406394958,-0.6703659892082214,-0.042890146374702454,0.10961485654115677,0.11666368693113327,0.12152890115976334,0.1505822241306305,0.012467259541153908,-0.4680468440055847,-0.22257216274738312,-0.11507538706064224,-0.019468000158667564,0.09218599647283554,0.263714462518692,0.003930569626390934,0.1771392822265625,-0.5012398958206177,-0.2591528594493866,0.13267746567726135,0.15314285457134247,0.3211885094642639,0.04439781233668327,-0.22412560880184174,-0.28524067997932434,-0.05861729383468628,-0.5164158940315247,0.08384773135185242,-0.02228974550962448,-0.09503163397312164,-0.9229280352592468,-0.06330810487270355,0.38946273922920227,-0.06064732000231743,0.09045848995447159,0.2527426779270172,-0.8873884081840515,0.10986607521772385,-0.12586556375026703,-0.4440343379974365,0.5176569223403931,-0.0020878964569419622,-0.43482691049575806,-0.11174990236759186,-0.04172990098595619,0.16387015581130981,-0.10392984747886658,0.38380008935928345,-0.02762451581656933,0.8451718091964722,0.17159315943717957,-0.010933572426438332,-0.8030719757080078,0.8087847828865051,-0.7126936316490173,-0.008562927134335041,0.348209410905838,-0.3378118872642517,-0.05156470462679863,-0.527156412601471,0.19147889316082,-0.004974831826984882,-0.26507219672203064,1.0903559923171997,-0.034753892570734024,0.7517407536506653,-0.26337555050849915,-0.6613730788230896,0.553011417388916,0.18857042491436005,-0.18911100924015045,-0.6741605401039124,0.38960346579551697,0.11402077227830887,0.06510061770677567,-0.6696577668190002,0.17062842845916748,-0.4910707473754883,-0.33527612686157227,-0.23676201701164246,0.2791866660118103,-0.24017095565795898,-0.30167442560195923,0.04852547496557236,0.6927959322929382,0.4123288691043854,-0.36939993500709534,-0.22775967419147491,-0.11162411421537399,-0.7102958559989929,-0.30207696557044983,-0.1803629845380783,-0.20090116560459137,0.0929374247789383,0.14518994092941284,0.30920135974884033,-0.013409431092441082,-0.06910853087902069,-0.3221293091773987,0.23178298771381378,-0.16669875383377075,-0.35998377203941345,0.14133760333061218,-0.23114018142223358,-0.07556906342506409,0.21616095304489136,-0.06782680004835129,-1.5293277502059937,0.22982023656368256,-0.3353043496608734,-0.730082094669342,0.3286454975605011,-0.3892923593521118,-0.12111373990774155,0.1380382478237152,-0.6774032711982727,-0.46351179480552673,-0.3595293462276459,-0.031022705137729645,-0.10511082410812378,-0.1562965214252472,0.28224658966064453,0.03605348616838455,0.3003774583339691,0.35382258892059326,-0.5211759805679321,0.10555686801671982,-0.3223884701728821,0.10874809324741364,-0.17875559628009796,-0.6737335324287415,0.7726938128471375,0.3931216299533844,0.5229289531707764,0.5873473882675171,0.20995061099529266,0.1746087521314621,0.3209729790687561,0.33770766854286194,0.11141230911016464,0.08755112439393997,0.248877614736557,0.18759702146053314,0.03642113506793976,-0.03591649606823921,0.7109413743019104,0.24077844619750977,0.5152570009231567,-0.14496636390686035,-0.47734129428863525,-0.19371037185192108,0.594009280204773,-0.2732524275779724,-0.2905418574810028,-0.212100088596344,-0.26513752341270447,-0.23505237698554993,-0.10330521315336227,-0.34860044717788696,0.15967485308647156,0.13597047328948975,-0.1185579001903534,-0.32069918513298035,0.1861739456653595,0.08840297162532806,-0.07258263230323792,-0.30681312084198,0.08769644796848297,0.032524701207876205,-0.07161621004343033,0.42708107829093933,-0.04438191279768944,0.1401110142469406,0.1205301582813263,-0.3677758276462555,-0.1399228423833847,0.4711248576641083,-0.38138142228126526,-0.019292781129479408,-0.059814922511577606,-0.3272559344768524,-0.4301008880138397,-0.43892523646354675,-0.15258243680000305,-0.26655951142311096,-0.13236987590789795,-0.14325204491615295,-0.361166387796402,-0.07545346766710281,0.1609061062335968,-0.0692831501364708,0.04212779924273491,0.14239652454853058,0.3722479045391083,-0.35748887062072754,-0.6759044528007507,0.02482384629547596,0.24098095297813416,-1.0149297714233398,-0.3064905107021332,-0.02403787337243557,0.555810272693634,-0.028346264734864235,0.17313668131828308,0.10896417498588562,-0.04426665976643562,-0.05401306599378586,0.1953289806842804,-0.0720922127366066,0.009714113548398018,0.3528422713279724,0.20206134021282196,0.13939779996871948,-0.37923043966293335,0.165028378367424,-0.5564203262329102,-0.18007440865039825,0.4357701539993286,-0.1655794382095337,0.16292648017406464,0.0339897982776165,0.2089828997850418,-0.27877095341682434,-1.897263765335083,-0.41973677277565,-0.010425900109112263,0.07453976571559906,-1.2598261833190918,0.18815818428993225,-0.009053883142769337,-0.10726720094680786,-0.3154742419719696,-1.4963897466659546,0.15428607165813446,-0.025230953469872475,0.37533727288246155,0.4838593900203705,-0.049970414489507675,0.6480185389518738,-0.13676126301288605,-0.05354394018650055,0.33885395526885986,-0.11432011425495148,-0.22587686777114868,0.22509855031967163,-0.01983259990811348,0.2509663999080658,0.051142822951078415,-0.3226251006126404,0.2784375548362732,0.05061444640159607,-0.270346999168396,0.03413335978984833,-0.23945875465869904,0.5236978530883789,0.22053705155849457,-0.19984915852546692,-0.5081264972686768,0.19742484390735626,-0.04288956895470619,0.5206621289253235,0.06267271190881729,0.6355504989624023,-0.3206312358379364,-0.634813129901886,-0.0009577408782206476,0.1249021366238594,0.29060640931129456,-0.4600513279438019,0.25092554092407227,-0.11114413291215897,-0.004010690841823816,0.0712878555059433,0.20043684542179108,0.15719389915466309,0.46926164627075195,-0.07726527750492096,0.3761756420135498,0.4010935425758362,0.09187766909599304,-1.0803718566894531,0.29995283484458923,0.007441828027367592,-0.24317649006843567,-0.02194404974579811,0.5242787599563599,0.05588655173778534,-0.3412219285964966,-0.4602154493331909,-0.6535390615463257,-0.47005656361579895,-0.2729962170124054,0.47989439964294434,-0.35096168518066406,0.13276426494121552,-0.052225369960069656,0.10475847125053406,0.24861988425254822,-0.18130207061767578,0.04555128142237663,-0.17905353009700775,0.460361510515213,-0.13994325697422028,-0.03018096461892128,-0.4021068215370178,0.4397558271884918,0.010228754952549934,-0.23630061745643616,0.22408095002174377,-0.37281492352485657,0.08579473942518234,0.07035960257053375,0.19681932032108307,0.2671768069267273,-0.0866415873169899,0.052527014166116714,0.5986714363098145,0.6382652521133423,-0.013104893267154694,-0.02080322429537773,-0.09366196393966675,0.508207380771637,0.6824893355369568,-0.31690114736557007,-0.15775620937347412,0.037985775619745255,0.9608830809593201,-0.014127686619758606,0.28355318307876587,-0.06275065988302231,-0.22617104649543762,-0.3585430085659027,-0.18784448504447937,0.11188006401062012,0.2566177248954773,-0.44507306814193726,0.35415005683898926,0.22582875192165375,0.029403025284409523,0.892128050327301,-0.0666351243853569,0.20870475471019745,0.5217106342315674,0.44551295042037964,0.04773082584142685,0.4816078543663025,0.14555588364601135,-1.396440029144287,-0.3600020408630371,-0.14345896244049072,0.37982118129730225,-0.3832666873931885,-0.14922800660133362,0.24961014091968536,0.18279170989990234,0.1532634198665619,-0.021939072757959366,-0.33770766854286194,-0.42560094594955444,-0.048923805356025696,-0.23908469080924988,0.13739915192127228,-0.6291100382804871,-0.5624839067459106,1.112969994544983,-0.6993874907493591,-0.2343514859676361,0.35535863041877747,-0.6443555951118469,0.3257159888744354,0.12049729377031326,0.03753625229001045,-0.8101428151130676,0.20909544825553894,0.32571765780448914,0.7965506911277771,0.07958544790744781,0.09144019335508347,0.15402022004127502,-0.3863525688648224,-0.02940329909324646,-0.02996119111776352,-0.2598399221897125,0.47686779499053955,-0.606833279132843,-0.13593348860740662,0.6317113637924194,-0.7594300508499146,-0.2855090796947479,0.26202908158302307,-0.3403807282447815,-0.08162528276443481,-0.5023801922798157,-0.1544148474931717,-0.019578181207180023,-0.07754196226596832,0.2632141709327698,-0.1267908662557602,-0.03673633933067322,-0.964135468006134,0.34289392828941345,-1.0164333581924438,0.1273118406534195,-0.14987371861934662,-0.35092055797576904,-0.3501686155796051,-0.8680692315101624,-0.2731938362121582,0.844487190246582,-0.17904497683048248,-0.2057642936706543,-0.12322939187288284,-0.07691364735364914,-0.06705235689878464,-0.3606151342391968,0.719240665435791,-0.1758568435907364,-0.2743386924266815,0.1776844710111618,-0.20657382905483246,-0.02762174792587757,0.1693028211593628,-0.48255297541618347,0.14684319496154785,-0.12834013998508453,-0.6760929226875305,-0.37107881903648376,0.10666342079639435,-0.06825462728738785,-0.006202375516295433,0.3783840239048004,-0.4447222948074341,0.8135135173797607,-0.4847872257232666,0.09897115081548691,0.1035088375210762,0.42051467299461365,-0.5654946565628052,0.3172955811023712,-0.07392150908708572,-0.6322265863418579,-0.3940565288066864,-0.577370285987854,0.7416618466377258,0.3076133131980896,0.0943814069032669,-0.1530754566192627,0.006378893740475178,-0.5633913278579712,-0.12459567934274673,-0.3465389907360077,0.35200706124305725,0.24090231955051422,0.4416324198246002,-0.018491581082344055,0.09258844703435898,0.24339869618415833,0.10417412221431732,0.09583047032356262,-0.015490471385419369,-0.33082279562950134,-1.1753734350204468,0.3546728789806366,0.006662813946604729,0.2939215302467346,-0.6240206956863403,-0.18202941119670868,-0.26835086941719055,-0.18192248046398163,-0.08343327045440674,-0.46696847677230835,0.2513444423675537,0.4628234803676605,-0.5986875295639038,-0.003321791533380747,0.24077509343624115,-0.4340968728065491,0.15257100760936737,-0.10139233618974686,0.43180596828460693,-0.005461888387799263,0.1602788120508194,-1.1435534954071045,-0.017087766900658607,0.0010405415669083595,-0.6403350830078125,0.3072813153266907,0.5296283960342407,0.08653465658426285,-0.5003502368927002,0.3401641249656677,-0.379430890083313,0.12746991217136383,-0.3648360073566437,0.48707321286201477,-0.28610050678253174,0.34980785846710205,-1.561505913734436,0.0842706561088562,-0.1319160759449005,0.48418864607810974,-0.3089946508407593,0.2663559317588806,-0.3760886490345001,-1.1467900276184082,0.7460240125656128,-0.4003506302833557,-0.10819167643785477,-0.05576767027378082,-0.4399680495262146,0.01457742229104042,0.2240617573261261,0.321665495634079,0.09348351508378983,0.09895215183496475,0.308810293674469,0.15207625925540924,-0.16312390565872192,-0.04529489949345589,0.08956589549779892,-0.23430663347244263,0.09553013741970062,-0.5311702489852905,0.3930785059928894,0.15246455371379852,-0.015261180698871613,0.4433075785636902,-0.5840824246406555,0.08969637751579285,0.07179446518421173,-0.4340716302394867,0.39044007658958435,0.16426463425159454,0.25457075238227844,0.29175156354904175,-0.036753833293914795,0.14957652986049652,0.22151347994804382,0.05394645407795906,0.041487254202365875,0.17890065908432007,-0.13667838275432587,-0.5198658108711243,-0.3343714773654938,-0.036491572856903076,-0.01595328375697136,0.8242156505584717,0.12178658694028854,-0.25512608885765076,-0.28970614075660706,-0.04638891667127609,0.2473221868276596,0.31177303194999695,-0.0361505001783371,0.2312365621328354,-0.23697540163993835,0.10614825785160065,-0.11426728218793869,0.5133393406867981,0.920840859413147,0.11588750034570694,0.006538861431181431,-0.19249001145362854,0.30022993683815,-0.4920724332332611,-0.9036300182342529,-0.040589362382888794,0.37583428621292114,-0.25185099244117737,-0.10285203158855438,-0.22585362195968628,0.15958400070667267,-0.3080533444881439,0.2607947587966919,-0.06436978280544281,0.3933907747268677,0.39305034279823303,-0.0934688076376915,0.1559864580631256,0.11715726554393768,-0.3907688856124878,0.3333692252635956,0.2671292722225189,0.05697282776236534,0.1732291430234909,1.3921207189559937,-0.24365577101707458,0.02903175912797451,-0.6168479323387146,0.09838099777698517,-0.052436962723731995,-0.051094308495521545,-0.6654121279716492,-0.48067590594291687,0.05313492938876152,-0.09686880558729172,0.5461068153381348,-0.43768346309661865,0.025902947410941124,0.220485657453537,-0.7223944067955017,0.05223279073834419,-0.21246904134750366,-0.29641368985176086,0.14396071434020996,-0.2873995900154114,0.10588637739419937,0.09307006746530533,0.31740254163742065,-0.2629615366458893,0.0845380648970604,-0.726908802986145,-0.5232649445533752,0.18027310073375702,1.0239520072937012,0.03767649456858635,-0.08849482238292694,0.040027059614658356,0.5489100217819214,-0.04929792508482933,-0.49467933177948,-0.19243811070919037,0.20213790237903595,-0.4012388288974762,-0.1753845065832138,-0.348120778799057,0.07284209132194519,0.5272053480148315,-0.004676110576838255,0.3080367147922516,0.20541734993457794,-0.37061139941215515,-0.08698512613773346,-0.2721453607082367,0.1497824490070343,0.6399911046028137,0.39601001143455505,-0.10005053877830505,0.23382659256458282,-0.43844112753868103,-0.15427067875862122,-0.12242865562438965,-0.004271255806088448,-0.16527016460895538,-0.2764594256877899,-0.12047946453094482,0.08695296943187714,0.2464764267206192,0.37085089087486267,0.2409396767616272,-0.13442851603031158,0.17634885013103485,-0.18528622388839722,-0.03313343599438667,0.007852156646549702,0.43197768926620483,-0.2932432293891907,0.10118873417377472,-0.011220299638807774,-0.3630044460296631,-0.43871816992759705,-1.3894975185394287,0.08024370670318604,1.401731252670288,-0.3437139093875885,-0.46302127838134766,0.08375074714422226,-0.5939856171607971,0.5986297726631165,-0.05868498980998993,0.21504279971122742,0.44123661518096924,0.2689949870109558,-0.0377938486635685,-0.1439126580953598,0.08948306739330292,0.3324863612651825,1.1122251749038696,-0.12490776181221008,0.4072999656200409,0.4330671429634094,0.08211177587509155,-0.07351746410131454,0.09049991518259048,0.09464055299758911,0.06672970950603485,-0.47797077894210815,0.09962005168199539,-0.03448208048939705,-0.13650359213352203,0.594871997833252,0.12406671792268753,0.14252609014511108,0.46136051416397095,0.10235390067100525,-0.19258704781532288,0.09965459257364273,0.0516730397939682,-0.3086371123790741,0.4339250922203064,0.22350099682807922,-0.2494724988937378,0.15768542885780334,-0.13906461000442505,0.058264609426259995,0.034148529171943665,-0.3854244649410248,0.1216222494840622,-0.14551711082458496,-0.029817728325724602,-0.2528958022594452,-0.16265429556369781,-0.026814570650458336,0.26555800437927246,0.2774590253829956,0.17273175716400146,0.06015528365969658,0.17970287799835205,-0.5091379880905151,0.33755436539649963,-1.2223693132400513,0.16986435651779175,0.8777192831039429,0.15063810348510742,-0.4139729142189026,0.602760374546051,0.12266600877046585,-0.24126826226711273,-0.4251146912574768,0.07845436036586761,-0.6300598978996277,-0.23924703896045685,-0.5242796540260315,-0.19904355704784393,0.03666626662015915,-0.08969581127166748,0.17419083416461945,0.28211164474487305,-0.2492922842502594,-0.25272199511528015,-0.1146988719701767,0.0327143631875515,0.2565324902534485,0.26474088430404663,-0.2643362283706665,0.48179781436920166,-0.8823125958442688,-0.23876221477985382,0.07093024253845215,-0.2589026987552643,-0.12282177060842514,0.1241515651345253,-0.6102817058563232,-0.06121363118290901,0.7169379591941833,-0.03785669058561325,-0.2719224691390991,-0.16579210758209229,-0.3826376497745514,0.0014572679065167904,0.37269362807273865,0.3594725430011749,0.2891140878200531,0.008314108476042747,-0.523154616355896,0.08694609254598618,-0.06055012345314026,0.33291375637054443,0.22208577394485474,0.516776442527771,-0.1584111601114273,-0.04702595993876457,-0.09374046325683594,0.44659340381622314,1.1598149538040161,-0.5906769037246704,-0.1975306123495102,-0.0865166187286377,-0.07682820409536362,-0.8447105884552002,0.19526098668575287,0.34528636932373047,0.21325403451919556,0.29345786571502686,0.045729536563158035,-0.38683080673217773,0.13406027853488922,0.40435129404067993,0.45459115505218506,0.4628016948699951,1.9519160985946655,0.13247211277484894,-0.41590988636016846,0.30351579189300537,0.6839528679847717,0.7725982666015625,0.5753123760223389,0.17755737900733948,0.3548625111579895,0.042199112474918365,-0.22536663711071014,-0.06702776253223419,-0.030005253851413727,0.1933680772781372,-0.053949080407619476,-0.05125788599252701,-0.43606138229370117,0.1339522898197174,-0.47044646739959717,0.13404583930969238,0.11718352138996124,-1.6831774711608887,0.6565013527870178,0.37176254391670227,0.579891562461853,-0.234053373336792,-0.13020268082618713,0.3109908401966095,-0.06562680006027222,0.26290637254714966,-0.08873015642166138,-0.2961791753768921,0.3192552328109741,-0.3252995014190674,-0.11111704260110855,-0.7710191011428833,-0.6140508055686951,0.4774335026741028,0.21642817556858063,0.024417197331786156,0.07086833566427231,0.45984479784965515,-0.3146054148674011,0.07917589694261551,0.16848428547382355,0.01494851429015398,-0.48064735531806946,-0.3767903745174408,-0.07747283577919006,0.44091835618019104,0.03561225160956383,0.3189542889595032,0.2988489866256714,0.03620704635977745,-0.05209511145949364,0.2049354612827301,0.09248004853725433,0.2994697093963623,-0.06510075181722641,0.07134606689214706,0.415579229593277,-0.24915066361427307,-0.5798686146736145,-0.06445799022912979,0.48449668288230896,1.6018049716949463,-0.38299328088760376,-0.1394495815038681,-0.8889232277870178,0.027656182646751404,-0.5611011981964111,-0.2389778047800064,-0.04061316326260567,-0.2963261604309082,-0.3793991804122925,0.1503753513097763,-0.2526931166648865,0.13826727867126465,0.24774447083473206,-0.4726140797138214,0.9733542203903198,0.3881186544895172,-0.24050551652908325,0.7348852157592773,-0.3156355321407318,-0.3566182553768158,-0.48367583751678467,0.1586645245552063,0.0016421466134488583,0.48941993713378906,0.17810527980327606,-0.5251744985580444,0.3224268853664398,0.24488790333271027,0.10981396585702896,-0.19651243090629578,0.5446501970291138,-0.08887489140033722,-0.16610464453697205,-0.10430657118558884,0.352422833442688,-0.07190167903900146,0.0685584619641304,-0.09554170817136765,0.08713573962450027,-0.7387539148330688,-1.0240918397903442,0.33609095215797424,-0.4866156578063965,0.19367201626300812,0.06516198813915253,-0.12097342312335968,0.8285228610038757,0.1456674486398697,-0.24872660636901855,-0.13654117286205292,-0.2890513837337494,-0.3644436001777649,0.38634341955184937,-0.34868231415748596,0.06484916061162949,-0.04028630629181862,0.15796318650245667,0.29169762134552,0.05766469985246658,-0.08158332109451294,0.13675066828727722,0.15142251551151276,0.08798142522573471,0.025372430682182312,-0.22747009992599487,0.42160671949386597,-0.0007767823990434408,0.4490736424922943,0.4890839755535126,0.20848770439624786,-1.008773684501648,-0.01993737928569317,-0.21294187009334564,0.2810913920402527,0.1945701241493225,-0.21955233812332153,-0.06774971634149551,0.4339633882045746,-0.5990241169929504,-0.4394219219684601,0.5377093553543091,-0.09649057686328888,0.1430935263633728,-1.0939533710479736,1.0873624086380005,0.2487221211194992,0.2350352257490158,-0.10043621808290482,-0.15172402560710907,-0.3509614169597626,-0.9224905371665955,-0.6342148184776306,0.06554269045591354,-0.25378328561782837,0.6188071370124817,0.15693746507167816,0.214660182595253,0.610459566116333,-0.004322079010307789,-0.5884344577789307,0.018690815195441246,-0.20607860386371613,0.14611166715621948,0.0255619864910841,0.4225959777832031,0.3787051737308502,0.7055314779281616,-0.1421971321105957,-0.9420037865638733,-0.05802532285451889,-0.5051712393760681,-0.3593825101852417,0.09188805520534515,0.0006387805915437639,0.5508111119270325,0.651225209236145,0.3375864028930664,0.9591476321220398,0.046604499220848083,0.0661429837346077,-0.13624350726604462,-0.012719249352812767,0.2825792133808136,0.6021137237548828,0.16417448222637177,0.17569303512573242,-0.5821306705474854,0.17162898182868958,-0.013673193752765656,-0.021920692175626755,-0.28204095363616943,0.45118167996406555,-0.3131846785545349,-0.06219984218478203,-1.0297002792358398,0.3288358449935913,-0.33548393845558167,-0.06469752639532089,-0.49280184507369995,0.1085057407617569,0.5750455856323242,0.08508442342281342,-0.28273269534111023,0.2318604588508606,-0.2843937873840332,-0.03326202183961868,0.471975713968277,-0.21730297803878784,0.013634251430630684,-0.47174280881881714,-0.38406309485435486,-0.41354021430015564,0.40547671914100647,0.1094081848859787,0.5408982038497925,0.34656211733818054,0.009750881232321262,0.018561970442533493,0.9074488282203674,-0.7312719821929932,-0.4412625730037689,-0.09125078469514847,0.28234732151031494,0.029805336147546768,-0.21749508380889893,-0.15920035541057587,0.5411036014556885,0.4303538203239441,0.1658371090888977,0.19370220601558685,-0.4039669632911682,-0.2268155813217163,-0.19302436709403992,0.09290191531181335,-0.6005325317382812,0.04699469357728958,0.29441383481025696,-0.7118887901306152,-0.07602369040250778,-0.007167752832174301,0.025209594517946243,-0.45102977752685547,0.021482670679688454,-0.015740513801574707,-0.44934436678886414,-0.5700161457061768,0.13373777270317078,-0.5730730295181274,0.4671238362789154,-0.17038626968860626,-0.020134910941123962,-0.01862526684999466,-0.3535239100456238,0.04097512736916542,-0.5696738958358765,0.16793273389339447,-0.068067766726017,0.11937129497528076,-0.28577423095703125,0.24385321140289307,-0.22312846779823303,-0.4897061586380005,0.27322861552238464,-0.46397149562835693,0.1715177744626999,0.22216995060443878,0.3144155740737915,0.30861008167266846,0.8754628896713257,0.27417483925819397,-0.08690562099218369,0.3492957353591919,-0.5272551774978638,0.031333696097135544,-0.4403478801250458,0.555389404296875,-0.37633928656578064,-0.5328746438026428,0.669884204864502,-0.11575858294963837,0.273070752620697,-0.24470128118991852,0.13809870183467865,0.011166267096996307,-0.04337228089570999,0.12148261070251465,-0.19857163727283478,0.002680659294128418,-0.008038644678890705,0.26982977986335754,-0.7885693907737732,0.264666885137558,-0.36430656909942627,-0.33068278431892395,0.0007387388031929731,-0.03277778625488281,-0.08850707113742828,0.027017291635274887,-0.028394101187586784,-0.054932668805122375,0.459268182516098,-0.9209370017051697,0.2607601284980774,-0.7434160709381104,0.026695162057876587,-0.4918067753314972,-0.14863988757133484,0.03340242803096771,0.2164708822965622,-0.24856022000312805,-0.07677232474088669,0.06641794741153717,-0.7351370453834534,0.5023297667503357,0.1311296820640564,-1.2176458835601807,-0.41515496373176575,-0.02684539183974266,-0.21133820712566376,-0.2477450668811798,-0.6099242568016052,0.15750987827777863,0.2736491858959198,-0.11366414278745651,0.05275836959481239,0.8665510416030884,-0.18455097079277039,-0.0691937729716301,-0.37705373764038086,0.6426556706428528,-0.12005637586116791,0.2586863338947296,0.7114423513412476,-0.3668094277381897,-0.44154229760169983,0.3468894064426422,0.07904858887195587,0.5998355150222778,0.14842337369918823,-0.17446677386760712,0.8080821633338928,0.47152164578437805,-0.1252167671918869,0.6661825776100159,0.3703402876853943,-0.26454809308052063,-0.08666369318962097,0.6450991630554199,0.43404480814933777,0.09753507375717163,0.05587224289774895,0.31352150440216064,-0.38739335536956787,0.2945161759853363,0.22239325940608978,0.3196184039115906,0.04666004329919815,-0.4228073060512543,-0.3655443489551544,0.25863510370254517,-0.2253737896680832,0.5350134968757629,0.33735549449920654,0.4474996030330658,-0.37906602025032043,-0.4816958010196686,0.17252857983112335,0.14087341725826263,0.4767926335334778,0.9999642372131348,-0.07483059912919998,0.5058466196060181,0.3909067213535309,-0.48862460255622864,0.12453034520149231,0.1678105592727661,0.25702884793281555,-0.15022073686122894,0.5467509627342224,-0.5260016918182373,0.3825344443321228,0.3478829562664032,0.40676191449165344,0.4607829749584198,0.15496885776519775,-0.09367023408412933,0.15294604003429413,-0.49526140093803406,0.6994329690933228,0.4338865280151367,0.20354685187339783,-0.1277383416891098,-0.35821449756622314,0.013136927038431168,0.24033227562904358,-0.045159850269556046,0.32740461826324463,-0.0025134929455816746,0.4742650091648102,-0.12996408343315125,-0.0870533362030983,-0.5602059960365295,0.5258856415748596,0.27127331495285034,-0.0037619932554662228,-0.29780736565589905,0.16688300669193268,0.3433372378349304,-0.19050878286361694,-0.4828436076641083,-0.5469123125076294,-0.8148621916770935,-0.06630876660346985,-0.5878077149391174,0.055402256548404694,0.041859742254018784,0.8338590264320374,-0.35435137152671814,0.09323745965957642,0.15274621546268463,-0.017314402386546135,-0.2691655457019806,-0.1835954189300537,0.037011340260505676,-0.21779830753803253,0.11574476212263107,-0.035661377012729645,-0.25962138175964355,0.010994387790560722,-0.48938971757888794,0.20592887699604034,-0.03896508365869522,0.41348713636398315,-0.34100207686424255,0.28333303332328796,-0.7650050520896912,0.03719977289438248,0.6177325248718262,0.6357378959655762,0.2940409779548645,-0.23155388236045837,0.13762612640857697,0.9189994931221008,0.22068899869918823,-0.1497621387243271,-0.11693133413791656,-0.128504678606987,-0.2832580506801605,-0.4377029240131378,0.007706665433943272,-0.03720635920763016,0.05863788723945618,-0.10013970732688904,0.10722937434911728,0.5651994347572327,0.3003714680671692,0.2670445442199707,-0.30633142590522766,-0.002381585305556655,0.235203355550766,0.6377978920936584,0.12060236185789108,0.34626612067222595,-0.22475217282772064,-0.5660041570663452,0.40820562839508057,0.36865055561065674,0.3904581069946289,-0.2282734215259552,-0.9318536520004272,-0.02581384778022766,-0.5562581419944763,-0.3858069181442261,-0.40247461199760437,-0.2343910038471222,-0.6694322824478149,0.3125889003276825,-0.06513947248458862,0.34781932830810547,0.8762514591217041,0.30577266216278076,-0.2210778146982193,-0.12454371899366379,-0.9100920557975769,-0.49322840571403503,0.7181164026260376,0.05701305344700813,0.21253593266010284,-0.7315503358840942,-0.045434433966875076,0.19116339087486267,-0.4040771722793579,0.5411794185638428,0.10560834407806396,-0.4317127466201782,-0.4140241742134094,-0.6641684174537659,0.5008248686790466,-0.3865107595920563,0.39460092782974243,-0.2765161097049713,-0.2627737820148468,-0.3538264334201813,-0.07853245735168457,-0.14125299453735352,0.38505619764328003,0.15755628049373627,-0.406017005443573,0.04555484279990196,-0.3035001754760742,0.34717223048210144,-0.3050439953804016,-0.38808533549308777,0.26260778307914734,-0.729866623878479,0.13192123174667358,0.00826906319707632,0.015297548845410347,0.46334701776504517,0.2666538953781128,-0.23130515217781067,-0.5067695379257202,0.08614401519298553,-0.0029002593364566565,0.18119749426841736,0.18961015343666077,-0.12634599208831787,0.27357858419418335,-0.04330245777964592,-0.459316223859787,-0.157211035490036,-0.025811778381466866,0.236271470785141,-0.6006852984428406,0.3619177043437958,-0.5114741921424866,0.09319138526916504,0.3864457309246063,-0.0255416389554739,0.033765409141778946,0.23667743802070618,-0.2526072859764099,0.0006357419188134372,0.28006666898727417,0.48706531524658203,0.1434965282678604,-0.5585665702819824,-0.17122779786586761,0.22745321691036224,0.4008226990699768,-0.11756619065999985,0.15370506048202515,0.034993719309568405,0.14930790662765503,0.7117738723754883,-0.30294111371040344,2.0189459323883057,-0.42595839500427246,0.6104440689086914,-0.19975566864013672,0.03961888700723648,0.18821687996387482,-0.14431947469711304,0.18851839005947113,-0.6404515504837036,-0.07033513486385345,-0.057811252772808075,-0.20373176038265228,-0.8080421090126038,0.19093281030654907,-0.639116644859314,-0.31591537594795227,0.37226879596710205,0.42765775322914124,0.27337491512298584,0.4238716661930084,0.4555235505104065,-0.5705620050430298,-0.7186359167098999,0.14890488982200623,-0.4298304319381714,-0.3261103630065918,-0.23694251477718353,0.23389899730682373,-0.001609153812751174,0.3163100779056549,-0.20584842562675476,-0.6881484985351562,0.23085950314998627,0.46551862359046936,-0.242882639169693,0.17692211270332336,-0.6511085629463196,-0.24519281089305878,-0.0007209003088064492,0.5648451447486877,-0.1290738880634308,-0.48845791816711426,-0.2590544521808624,-0.36973243951797485,-0.5963693857192993,-0.15350432693958282,-0.3950954079627991,-0.3251241445541382,-0.6337018609046936,0.11165807396173477,0.40678197145462036,-0.43289539217948914,-0.3772581219673157,0.2742496728897095,0.6081014275550842,-0.2723451554775238,0.2889777421951294,0.19854846596717834,-0.3317859172821045,-0.1611635535955429,0.2710299491882324,0.600959062576294,-0.29959070682525635,-0.23940889537334442,0.03057560697197914,-0.22329755127429962,-0.5387405157089233,0.40769100189208984,-0.835466742515564,-0.5147595405578613,0.6735711693763733,-0.2867950201034546,0.25574684143066406,-0.1872439980506897,0.09215792268514633,0.016811572015285492,0.03838379681110382,-0.22185178101062775,-0.5880140066146851,-0.1280813068151474,0.465305358171463,-0.2197427898645401,0.49643129110336304,0.0934351459145546,-0.020443474873900414,-0.4647606313228607,-0.04107048362493515,0.4235337972640991,-0.010905684903264046,0.12305686622858047,0.298408567905426,0.28867217898368835,1.951491117477417,1.9261084162280895e-05,1.104135513305664,-0.25294384360313416,0.056762874126434326,-0.5263929963111877,0.22294844686985016,0.35812047123908997,-0.15942063927650452,0.4772226810455322,-0.0258274395018816,-0.1505291610956192,0.33094725012779236,0.00541171757504344,0.6113002896308899,0.3228128254413605,0.3195772171020508,0.12791596353054047,0.41247960925102234,0.21878650784492493,-0.1598261296749115,0.5661444664001465,0.6493648290634155,0.10836391896009445,-0.3740347623825073,-0.18612314760684967,0.19927337765693665,-0.05928392335772514,0.5249261260032654,0.15660154819488525,0.12215102463960648,-0.5389139652252197,0.20660582184791565,0.26974862813949585,0.6815354228019714,-0.32430702447891235,-0.3027915060520172,0.52583247423172,0.3124130070209503,-0.16581156849861145,-0.23241518437862396,-0.45642396807670593,-0.2947249114513397,0.14121724665164948,0.2765769958496094,0.11592856794595718,-0.34667858481407166,-0.025402791798114777,-1.061123251914978,0.6088377237319946,0.3484480082988739,-0.21002084016799927,0.12269966304302216,0.535415530204773,-0.5288679599761963,0.33312317728996277,0.21652129292488098,-0.12360421568155289,0.1012963354587555,-0.2557525336742401,0.23359641432762146,-0.09964672476053238,0.7377352118492126,-0.5847603678703308,0.193622887134552,-0.13610070943832397,0.21324793994426727,0.12641946971416473,-0.4086591601371765,0.3283618986606598,-0.23511625826358795,-0.6434227228164673,0.3319564163684845,0.7434538006782532,0.6256142854690552,0.07557763904333115,-0.5013753175735474,-0.569790780544281,0.36880844831466675,-0.7124069333076477,0.007690855767577887,-0.2664696276187897,-0.010935445316135883,-0.3652128279209137,0.05942241847515106,0.09551748633384705,-0.09406640380620956,0.24566297233104706,-0.3288968503475189,0.5685005187988281,-0.8811085820198059,0.038049839437007904,-0.48463520407676697,0.20764657855033875,0.458499014377594,-0.11955469846725464,-0.3028988540172577,-0.908507227897644,-0.23281139135360718,-0.32243454456329346,-0.6702898144721985,0.6987854838371277,-0.41539475321769714,0.056796520948410034,0.16386017203330994,0.5594576001167297,-1.435229778289795,-0.3806264102458954,-0.26042768359184265,0.32349738478660583,-0.17100252211093903,0.3330017626285553,0.15325947105884552,-0.05659125745296478,-0.461517333984375,0.47579216957092285,-0.3171752691268921,-1.3177317380905151,0.13752390444278717,-0.026215147227048874,1.120640754699707,0.3202109932899475,-0.6274091601371765,-0.4441697597503662,0.2693052291870117,-0.09831079840660095,0.5105209946632385,0.4176208972930908,0.38498684763908386,0.40710851550102234,-0.021826915442943573,-0.394511342048645,0.5032864212989807,-0.6395226120948792,0.06639236211776733,-0.1365460306406021,-0.13115781545639038,-0.4161880612373352,0.25432610511779785,-0.2164650410413742,0.0018261780496686697,0.1683904230594635,0.22901126742362976,0.6920161843299866,0.007570188958197832,0.40191856026649475,-0.24714568257331848,-0.29622602462768555,0.4209401309490204,0.01843235269188881,-0.2115645855665207,0.04677953943610191,-0.032566312700510025,-0.05806386470794678,0.5042049884796143,-0.16737765073776245,0.28721171617507935,-0.3340376019477844,-0.10440628230571747,-0.6370776891708374,0.22372765839099884,0.19721251726150513,-0.5621208548545837,0.20033228397369385,0.10203777253627777,-0.05730856955051422,-0.37520551681518555,-0.1929437220096588,-0.27237269282341003,0.39420753717422485,-0.5456698536872864,0.7130975127220154,-0.21775512397289276,-0.0019233138300478458,0.14852046966552734,0.11307211965322495,-0.035215023905038834,0.24527473747730255,-0.40203019976615906,-0.5536983609199524,-0.442412793636322,-1.101335883140564,-0.04954243078827858,-0.16843779385089874,-0.37853872776031494,-0.15320292115211487,0.08849978446960449,0.0638323575258255,0.12500393390655518,-0.294931560754776,0.07704257220029831,-0.696758508682251,-0.010892625898122787,-0.005358003545552492,-0.6432323455810547,0.216934934258461,-0.1646495908498764,0.3553173840045929,-0.08764832466840744,-0.9896183013916016,-0.12142079323530197,0.1505311131477356,0.4174536466598511,0.11315195262432098,-0.15129458904266357,0.9513666033744812,0.504997730255127,0.35430222749710083,0.13653168082237244,-0.0012015777174383402,0.006440069526433945,0.39574745297431946,-0.04398626089096069,-0.24399025738239288,0.3519649803638458,0.23812754452228546,0.08137334138154984,-0.12579156458377838,-0.3307342827320099,0.3071196675300598,-0.21990588307380676,-0.0774838849902153,0.06453276425600052,0.05923690274357796,-0.21180640161037445,-0.24284937977790833,-0.24003911018371582,0.7936163544654846,0.3699028193950653,0.04637070745229721,0.0924244225025177,-0.019714249297976494,-0.6471735835075378,0.1018470823764801,-0.0389830656349659,0.4403822720050812,-0.329502671957016,0.06835206598043442,-0.13493631780147552,-0.9997585415840149,0.2238815724849701,-1.1165798902511597,-0.19542773067951202,0.15528027713298798,0.02466551959514618,0.12067259848117828,-0.41502177715301514,0.16761089861392975,-0.13637767732143402,0.18978190422058105,0.06889474391937256,-0.09701427817344666,-0.10614843666553497,-0.1155368760228157,-0.027086997404694557,-0.06177699193358421,1.040170669555664,-0.2522314488887787,-1.2090898752212524,0.27607306838035583,0.29466137290000916,0.29335376620292664,-0.016487743705511093,-0.2548600435256958,-0.10648176074028015,0.5846608877182007,-0.2622649371623993,0.4401110112667084,0.23608791828155518,0.18959887325763702,0.6216586828231812,0.23752634227275848,0.2607840299606323,0.3923254609107971,-0.44641202688217163,-0.6670709252357483,-0.48763930797576904,-0.42049044370651245,-0.07468496263027191,0.07069873809814453,0.6037235856056213,-0.25919461250305176,0.24635657668113708,0.8455485701560974,-0.35445937514305115,-0.21966256201267242,0.25168827176094055,0.7328657507896423,-0.003386611817404628,-0.6197729110717773,-0.057791586965322495,-0.05463583394885063,-0.41494524478912354,0.6763990521430969,0.31787919998168945,-0.0625656321644783,-0.04420657828450203,0.22822417318820953,-0.0616091825067997,0.08552394807338715,1.3871188163757324,0.3295060098171234,-0.1980571448802948,-0.06460649520158768,-0.580908477306366,0.5545845627784729,0.23631413280963898,-0.1348670870065689,-0.36611855030059814,-0.027530549094080925,-0.6245937347412109,0.14840714633464813,0.24629849195480347,-0.6144421100616455,-0.03577214479446411,0.075717993080616,-0.539550244808197,-0.030677301809191704,0.4032718539237976,0.06501734256744385,0.2583636939525604,0.38220667839050293,0.6537428498268127,0.04306252300739288,0.42612311244010925,0.2921123802661896,0.10649338364601135,0.0887971818447113,-0.33658134937286377,-0.1227506771683693,-0.22486262023448944,0.09775587171316147,-0.5906406044960022,-0.16654063761234283,0.013860099017620087,-0.4073864221572876,0.38498058915138245,0.26885154843330383,-0.25063541531562805,0.018534773960709572,-0.4153194725513458,-0.44055360555648804,-0.16968056559562683,0.43813684582710266,-0.06517830491065979,0.432466596364975,-0.33311349153518677,-0.09483899176120758,0.3864099383354187,0.1476050317287445,-0.10990002006292343,-0.009632203727960587,-0.17263321578502655,-0.11839572340250015,-0.5217944383621216,0.12191913276910782,-0.1408967822790146,0.27631551027297974,-0.1884276270866394,0.12167180329561234,0.23433060944080353,-0.02662847377359867,-0.3548419177532196,0.0673447847366333],\"xaxis\":\"x\",\"y\":[-0.00020846941333729774,-0.002437826944515109,0.005132079124450684,0.04389399290084839,0.005025553051382303,-0.03243511915206909,-0.021834103390574455,0.01724853366613388,-0.006388693582266569,-0.011249268427491188,0.025823866948485374,-0.02708706073462963,0.037737008184194565,0.03039371408522129,0.004002529662102461,-0.025539573282003403,-0.000606840243563056,-0.007340901996940374,0.03154383972287178,-0.009596077725291252,0.04526764899492264,-0.04283330589532852,0.06745856255292892,0.0045895203948020935,0.015958111733198166,-0.02778596244752407,0.004471430554986,0.016450200229883194,0.014138010330498219,0.019626688212156296,0.02219952456653118,0.022022685036063194,0.03619932010769844,0.021389411762356758,-0.023650331422686577,-0.004596169572323561,0.03643707558512688,0.003970797639340162,-0.01296458300203085,-0.001978909829631448,-0.029259804636240005,-0.028648026287555695,0.007506685797125101,0.007808549329638481,0.06487397849559784,0.06742557883262634,-0.0486704483628273,-0.037374097853899,-0.012397252023220062,0.02940414659678936,-0.041335463523864746,0.06087096408009529,0.05900577828288078,-0.0008127879118546844,0.013366798870265484,0.005408725701272488,-0.00605031568557024,-0.0276802871376276,0.019212743267416954,-0.0039076185785233974,0.05002956464886665,0.008429644629359245,-0.06583058834075928,0.024473149329423904,-0.042584411799907684,0.04693964868783951,0.03437311202287674,-0.04195144772529602,-0.03910108655691147,0.014437898993492126,0.023316828534007072,-0.006138830911368132,0.021229606121778488,0.013301653787493706,-0.021936532109975815,-0.005901457276195288,-0.02247626893222332,0.023228321224451065,0.03641610965132713,0.06153613701462746,-0.03175295144319534,0.020939575508236885,-0.02175925485789776,-0.07067353278398514,0.07165612280368805,-0.055065255612134933,-0.0035797262098640203,-0.03183867409825325,0.011937959119677544,0.04564294219017029,0.05516229569911957,0.030050843954086304,0.06170311197638512,0.0074189770966768265,0.02426287718117237,-0.035869717597961426,-0.015094427391886711,0.034958820790052414,0.025378147140145302,-0.03474177420139313,-0.004011634737253189,-0.013779466971755028,-0.023600120097398758,-0.10481510311365128,0.03863230720162392,-0.03210970759391785,-0.006382815074175596,0.001692509395070374,0.021525545045733452,-0.0010742265731096268,0.012654151767492294,0.054350197315216064,-0.01485631987452507,-0.02550474926829338,-0.03261958062648773,0.047274183481931686,-0.01964254304766655,-0.03304722532629967,-0.0009672247688286006,-0.08954223990440369,-0.006839773152023554,-3.659708090708591e-05,-0.029335280880331993,0.0015573098789900541,-0.00825768057256937,-0.0006663902313448489,0.05277421325445175,-0.013865054585039616,0.013654309324920177,-0.062342409044504166,-0.0407368503510952,0.030509768053889275,0.031193144619464874,0.01659686118364334,-0.010710558854043484,0.10055265575647354,0.008255952969193459,0.02033049426972866,0.003364743199199438,0.011676582507789135,0.05652753636240959,0.026827944442629814,0.02146810106933117,-0.03887007385492325,0.02052784338593483,-0.02618575096130371,-0.08800417929887772,-0.019986461848020554,0.03777174651622772,-0.020520752295851707,0.07311460375785828,0.060852374881505966,0.02684270404279232,-0.008155863732099533,6.212061998667195e-05,-0.00429531279951334,0.05379125103354454,0.06584476679563522,-0.07421326637268066,-0.041903071105480194,-0.0019045694498345256,0.052127838134765625,0.0033580921590328217,-0.006082842592149973,0.05382801964879036,0.02257932350039482,-0.032786671072244644,-0.022941458970308304,-0.004282334353774786,0.0002777528716251254,0.06215449795126915,-0.0025132782757282257,-0.04385719075798988,-0.02565077878534794,-0.06514294445514679,0.004864913411438465,-0.03233613073825836,0.02181592769920826,-0.03178168088197708,0.009095804765820503,0.06849923729896545,0.013469554483890533,0.11079252511262894,-0.0423307903110981,0.007466564886271954,0.015130661427974701,0.006836793385446072,0.050010353326797485,0.04423195496201515,-0.030062386766076088,-0.008607297204434872,-0.011222973465919495,-0.024677634239196777,-0.05436442047357559,-0.004296626430004835,-0.01565164513885975,-0.0012103107292205095,0.005306597333401442,-0.0032725974451750517,-0.04171490669250488,0.046270620077848434,-0.07505803555250168,0.03520729020237923,0.0339922197163105,0.027739403769373894,0.022250913083553314,-0.027413683012127876,-0.023216193541884422,0.0717860609292984,-0.0064218128100037575,-0.03091299720108509,-0.0038906133268028498,-0.026101989671587944,-0.013357022777199745,0.004339882172644138,-0.07026635855436325,0.02828177809715271,-0.0394832007586956,-0.033768054097890854,-0.027228808030486107,-0.025416230782866478,0.122760109603405,-0.1252889335155487,-0.03826838731765747,0.0193508081138134,0.04251031205058098,-0.03220857307314873,-0.011169495061039925,0.08272533118724823,-0.014122674241662025,-0.06345172971487045,-0.04230028763413429,0.021028388291597366,0.02766086906194687,0.03326951339840889,-0.010904688388109207,-0.03175816684961319,0.0017416509799659252,-0.00020027928985655308,-0.0012537118745967746,0.0269487202167511,-0.006684823893010616,-0.0021924858447164297,0.03389032185077667,0.0467885360121727,0.04973338544368744,0.03422663360834122,0.03199697285890579,0.016702810302376747,0.009427090175449848,-0.00849371962249279,-0.003976225852966309,0.0003174222947563976,-0.014923312701284885,0.00792301818728447,0.030841389670968056,-0.021315565332770348,0.043646231293678284,0.057550814002752304,-0.07895347476005554,0.02510949783027172,0.014873117208480835,-0.04000551253557205,0.027161147445440292,0.010492188856005669,0.01951310597360134,0.008702052757143974,-0.05599529296159744,0.0427311509847641,-0.10718175768852234,0.0001074199826689437,0.00819644145667553,0.023602774366736412,-0.08942034840583801,-0.0061900862492620945,0.05652781203389168,0.007558307144790888,0.07987518608570099,-0.001169343013316393,0.029830539599061012,0.07876162230968475,-0.04728800058364868,0.017062557861208916,0.032248739153146744,0.004209349397569895,0.02299502119421959,-0.11133036017417908,0.019689282402396202,0.045563243329524994,-0.055572185665369034,-0.032672058790922165,-0.0027164865750819445,-0.06573545187711716,0.007401700131595135,0.030823400244116783,-0.041859861463308334,0.0903133749961853,-0.04166104272007942,-0.02188325859606266,0.017904670909047127,-0.0329139418900013,-0.024480318650603294,0.02441379427909851,-0.020839938893914223,0.04484405368566513,0.010918905958533287,-0.09074428677558899,0.010399692691862583,-0.015048141591250896,-0.05442080646753311,-0.07761308550834656,0.027891134843230247,0.013636707328259945,0.04735395312309265,0.00714261457324028,-0.062234483659267426,0.08108669519424438,-0.07623741775751114,0.08565489202737808,0.022020872682332993,-0.03103022277355194,0.005002453923225403,-0.03517767786979675,-0.011947725899517536,-0.026970138773322105,0.0020193022210150957,-0.11142964661121368,-0.021859370172023773,0.0386485829949379,-0.10318822413682938,0.047692909836769104,0.033008016645908356,0.07735925912857056,0.0323723703622818,-0.016599440947175026,-0.041524823755025864,-0.012068333104252815,-0.01665886677801609,0.012168233282864094,0.050166770815849304,-0.03148210793733597,0.010924894362688065,-0.024796785786747932,-0.004300926811993122,-0.07157883048057556,0.005926637910306454,0.0497765988111496,0.022436922416090965,0.0181876327842474,-0.01347003411501646,0.03798079490661621,-0.033944323658943176,0.05602047219872475,0.05614471063017845,0.01924874074757099,-0.07808483392000198,-0.016603592783212662,0.010556637309491634,-0.0377228669822216,-0.027784671634435654,0.04165850952267647,0.013038392178714275,0.006000102963298559,0.057669028639793396,0.02560347132384777,-0.031331222504377365,-0.031542662531137466,-0.027317125350236893,-0.02966357208788395,-0.05186306685209274,0.01211341843008995,0.045863911509513855,0.03279270604252815,0.06927710026502609,0.035534076392650604,-0.00571417948231101,-0.020129164680838585,-0.006545009557157755,0.02701021544635296,0.010487448424100876,0.011291969567537308,0.07103001326322556,0.012705839239060879,-0.02180493250489235,-0.015928003937005997,-0.09131446480751038,-0.004869495984166861,0.03699628263711929,-0.028448129072785378,0.01762663759291172,0.05124959722161293,0.011493753641843796,-0.015739867463707924,-0.02418801188468933,0.013605508022010326,0.03019019216299057,-0.019827701151371002,-0.0523080937564373,-0.016246331855654716,0.03153962269425392,0.009450037032365799,0.029752502217888832,0.02391880378127098,0.033568065613508224,-0.010621101595461369,-0.001825805869884789,0.06129341199994087,-0.044048380106687546,-0.06678462028503418,0.03054407797753811,-0.09158165007829666,0.09246929734945297,-0.05875560641288757,0.02417968027293682,-0.006328111048787832,0.11706653982400894,0.004422682803124189,-0.05261868238449097,-0.03547890856862068,0.014126196503639221,0.057939060032367706,0.0060532838106155396,0.07040153443813324,0.024943822994828224,0.05528935790061951,-0.061141520738601685,0.041342802345752716,-0.03845830261707306,0.04619111865758896,0.007054728921502829,0.0006210471619851887,0.0010353209218010306,-0.01573105901479721,0.056040115654468536,0.055352333933115005,-0.012241736985743046,-0.0031274312641471624,-0.02857210487127304,-0.038529105484485626,-0.005020886659622192,0.043661922216415405,-0.03336476534605026,0.01591820828616619,-0.026610173285007477,-0.0039100381545722485,-0.03579198569059372,0.04142816364765167,-0.012800521217286587,-0.057099658995866776,-0.05815955996513367,-0.016168592497706413,0.029245536774396896,0.04393221065402031,-0.06733818352222443,-0.05488734319806099,0.013093524612486362,0.03481153026223183,0.004614152945578098,0.013275985606014729,0.07556253671646118,-0.05908571556210518,0.08502618223428726,0.045770205557346344,0.05331428349018097,-0.028023026883602142,-0.004869573749601841,-0.009611010551452637,0.009154011495411396,0.021884510293602943,-0.07836796343326569,0.002389570465311408,-0.0034447757061570883,-0.024144478142261505,-0.006159024313092232,0.004213033709675074,-0.06340819597244263,0.03654705733060837,0.02150685340166092,-0.03712783381342888,0.010517190210521221,-0.02469111606478691,-0.0016966242110356688,0.05409249663352966,0.015693793073296547,0.017209066078066826,0.04387745261192322,0.08126803487539291,0.03145858645439148,0.042982373386621475,-0.012139450758695602,0.022006738930940628,0.0023564426228404045,-0.0016172713367268443,-0.03155761957168579,-0.03651396557688713,0.09223949164152145,-0.057589612901210785,0.001897555310279131,-0.028034117072820663,0.039824049919843674,0.0353076308965683,-0.026571067050099373,0.009199319407343864,-0.02385145239531994,0.09752305597066879,-0.077543705701828,0.07637692987918854,0.03421667218208313,0.04792913794517517,-0.00938116479665041,-0.035070858895778656,-0.007278243079781532,0.0008625983609817922,0.01876482367515564,-0.025403648614883423,-0.08150729537010193,-0.021010402590036392,0.09008553624153137,-0.021533401682972908,0.012326747179031372,-0.052069827914237976,-0.014865787699818611,0.011805692687630653,0.017141617834568024,-0.007587617728859186,-0.05014503002166748,0.002787154633551836,0.04337165504693985,-0.038960449397563934,0.05509105324745178,0.03210591524839401,-0.01324428804218769,-0.09806076437234879,-0.06735722720623016,0.007973483763635159,-0.003226165659725666,-0.04214846342802048,-0.0514107383787632,-0.03371710330247879,0.05170133337378502,0.058582913130521774,-0.09024445712566376,-0.0021342248655855656,-0.07678163051605225,-0.021574947983026505,-0.014547771774232388,0.021568981930613518,-0.01588047482073307,-0.06098764389753342,0.015910515561699867,-0.0272111464291811,-0.0037237636279314756,0.004616554360836744,-0.06602541357278824,0.06421038508415222,-0.0018156885635107756,0.014582287520170212,-0.12663081288337708,0.028868574649095535,0.049492448568344116,0.05837370455265045,-0.00581080699339509,0.04948940500617027,-0.03500483185052872,0.013624269515275955,0.0506562739610672,-0.061978910118341446,-0.035244785249233246,0.12339469790458679,0.02068992517888546,0.10444989055395126,-0.039974793791770935,0.05983741581439972,0.0046197278425097466,-0.014941668137907982,0.04109878093004227,-0.0593685619533062,-0.024368254467844963,0.024874264374375343,0.024484895169734955,-0.0077703907154500484,-0.011529398150742054,0.03576009348034859,1.1048003216274083e-05,-0.007138602435588837,0.02572920173406601,0.035655152052640915,0.03725481405854225,0.01731407642364502,0.007788106799125671,-0.0035831723362207413,-0.04842148348689079,-0.0181137528270483,0.05725283920764923,0.048212386667728424,-0.0030442909337580204,0.01597798988223076,-0.00987126026302576,-0.09102535992860794,-0.050566378980875015,-0.010597352869808674,0.10242589563131332,0.013004299253225327,-0.02605186216533184,-0.00297928717918694,0.05097606033086777,-0.0451570488512516,0.04412241280078888,0.05005921423435211,-0.04400891065597534,0.026952320709824562,-0.0465860515832901,-0.07605613023042679,-0.08430631458759308,-0.0020345000084489584,-0.016902271658182144,0.0047913603484630585,0.03393545001745224,0.00957897212356329,0.0040004681795835495,0.056961238384246826,0.0598163940012455,0.03004603087902069,-0.003607101971283555,0.05200528725981712,-0.06786121428012848,0.000949226669035852,0.06024627387523651,-0.09657955169677734,0.05977959930896759,-0.02656904235482216,-0.03329090401530266,0.011131292209029198,0.006294907070696354,-0.018612472340464592,0.045546937733888626,-0.011992238461971283,0.06958137452602386,-0.033678729087114334,0.042116839438676834,0.0014273134293034673,-0.06911369413137436,0.029004395008087158,0.03845172002911568,0.05989694222807884,0.11798525601625443,-0.061903055757284164,-0.06476551294326782,0.0014293949352577329,0.04305405542254448,0.027797430753707886,0.05112693831324577,-0.026673879474401474,0.028697406873106956,0.04501882940530777,0.08872822672128677,0.036279212683439255,0.02700170874595642,0.018076086416840553,0.005392947234213352,-0.026544591411948204,0.035281646996736526,0.03699711710214615,-0.039152149111032486,0.0016842707991600037,0.030981138348579407,0.08483633399009705,-0.06712708622217178,-0.035803984850645065,-0.012794173322618008,-0.005070654209703207,-0.08242454379796982,0.06464237719774246,-0.049589503556489944,0.028662722557783127,-0.0206484105437994,-0.051961906254291534,-0.06629138439893723,0.0033145975321531296,-0.04944893345236778,0.0670175552368164,-0.01050348300486803,-0.00948885828256607,0.09786069393157959,0.0567004419863224,0.07237072288990021,0.013646503910422325,-0.036716654896736145,0.013021081686019897,0.002509942278265953,-0.028591183945536613,0.04379807040095329,0.033901918679475784,0.013898368924856186,-0.00624716654419899,-0.05170636624097824,0.02191181667149067,0.07399962842464447,0.03524370491504669,-0.09546837955713272,-0.05805453285574913,-0.013565354980528355,0.07592908293008804,0.006627316121011972,-0.01060525979846716,0.055438753217458725,-0.02396656572818756,0.04632999002933502,0.014145542867481709,0.018627531826496124,0.001098146429285407,0.04717889428138733,0.011584602296352386,-0.12184736132621765,-0.03675026819109917,-0.040869489312171936,-0.01899861916899681,-0.04758496582508087,0.05235976353287697,-0.06041897088289261,0.05191824957728386,0.028246091678738594,0.008402576670050621,-0.10349356383085251,-0.0196122657507658,-0.03437233716249466,0.0014735377626493573,-0.004649218171834946,-0.012599456124007702,-0.12194474041461945,-0.06751927733421326,-0.03444533422589302,0.056502826511859894,0.02052277885377407,-0.05641315132379532,-0.05661367252469063,0.09657426923513412,-0.05156201124191284,-0.04349668696522713,0.04858849197626114,-0.03838339075446129,0.05655112862586975,-0.007492720615118742,-0.08943474292755127,0.07018423080444336,-0.05949322506785393,-0.03991977125406265,0.027284342795610428,0.09209983795881271,0.04978605732321739,-0.01987331360578537,-0.02065214142203331,-0.016359250992536545,-0.05315207690000534,0.05411004275083542,0.010295446962118149,0.021832410246133804,0.04238712415099144,-0.1065685898065567,-0.045711737126111984,-0.015379173681139946,0.06339515000581741,-0.017887791618704796,-0.02786005288362503,-0.013887567445635796,0.019121114164590836,-0.03921772539615631,0.008474289439618587,0.04331550374627113,0.029688697308301926,0.07313806563615799,-0.022227760404348373,0.00465940497815609,0.013548769988119602,0.05987062305212021,0.06795264035463333,-0.01834399625658989,-0.1124928891658783,-0.06789682805538177,0.017844432964920998,0.03138793259859085,0.008801193907856941,0.0403631255030632,-0.04825950786471367,0.07596874237060547,0.007563794031739235,0.020732669159770012,-0.0031493022106587887,-0.036075275391340256,-0.048389676958322525,-0.0064369067549705505,0.0547177717089653,0.01666921004652977,-0.05499719828367233,0.10587044060230255,0.05778203532099724,-0.016049182042479515,-0.006777764763683081,0.029605837538838387,-0.021758899092674255,0.10522901266813278,0.08878772705793381,0.031147371977567673,0.03787599503993988,-0.08212275803089142,0.00878741592168808,0.024859169498085976,-0.11754819005727768,-0.0053237853571772575,-0.029714420437812805,-0.009834163822233677,0.03249134495854378,0.07036424428224564,-0.03919491544365883,-0.05759216845035553,-0.04611777886748314,0.008307309821248055,0.018170321360230446,-0.04199734702706337,0.06676033139228821,0.02603749744594097,-0.0822596475481987,-0.018030958250164986,-0.08288592845201492,-0.015716232359409332,0.017339035868644714,0.0997655987739563,0.0008574411040171981,-0.006883278489112854,-0.03319776430726051,0.026754900813102722,-0.017154326662421227,-0.07460855692625046,0.031817562878131866,-0.0650804340839386,0.00806308165192604,0.04423217475414276,0.013676725327968597,0.06706799566745758,-0.029514798894524574,0.04644958674907684,-0.018276259303092957,-0.005496754311025143,0.04028491675853729,0.13665154576301575,0.030019911006093025,0.06428418308496475,0.05775580182671547,-0.002326743211597204,0.022416364401578903,0.049534257501363754,-0.08359789103269577,0.021017875522375107,0.10370436310768127,-0.019244203343987465,-0.012089553289115429,-0.01004614494740963,-0.012075630016624928,0.03953392058610916,-0.10984686762094498,-0.01948682963848114,0.019560646265745163,0.0572534017264843,-0.032665152102708817,-0.03418693691492081,0.05533347278833389,0.045126620680093765,-0.06738916784524918,0.08271117508411407,-0.031061023473739624,0.023155972361564636,0.09290830045938492,-0.006167251616716385,-0.07365603744983673,0.015394936315715313,0.03587428107857704,-0.01694973185658455,0.08964557945728302,0.038352299481630325,0.023005923256278038,0.032237160950899124,0.06345616281032562,-0.008679322898387909,-0.08166693150997162,0.03656836971640587,-0.03129664808511734,-0.05769870802760124,0.08916506171226501,0.07749336212873459,-0.0022755563259124756,0.054362911731004715,-0.08830728381872177,-0.02552422136068344,-0.027470283210277557,-0.0562705434858799,0.03913499787449837,-0.025809340178966522,-0.03307902812957764,0.008577975444495678,-0.06771508604288101,-0.016863537952303886,-0.13943232595920563,-0.04126066341996193,-0.0803823173046112,0.036189936101436615,0.04444983974099159,-0.04583051800727844,0.06667596846818924,-0.06085062026977539,0.0160200297832489,-0.0026856805197894573,0.006468707229942083,0.009436882100999355,-0.03835759684443474,-0.056161798536777496,-0.001462378422729671,-0.04281487688422203,-0.024496624246239662,-0.027345219627022743,0.08762593567371368,0.0008108801557682455,-0.030503202229738235,-0.006941074971109629,0.1073571965098381,-0.016209911555051804,0.06450024992227554,-0.026651639491319656,-0.022197358310222626,0.06104029715061188,-0.0004591376055032015,-0.033355776220560074,-0.07464107871055603,0.015531955286860466,-0.038507647812366486,0.026869701221585274,-0.049275316298007965,-0.046599891036748886,0.02289574220776558,-0.013924228958785534,-0.015686864033341408,-0.007698289584368467,0.05062398687005043,-0.040160320699214935,-0.05386574938893318,-0.11603645980358124,-0.06669753789901733,-0.026043932884931564,0.06273889541625977,0.05512595549225807,-0.058893464505672455,0.011990263126790524,-0.03737001121044159,-0.01919017732143402,0.024401463568210602,0.027317525818943977,0.062364012002944946,0.05774455517530441,0.05975867807865143,-0.03179057687520981,-0.0467253178358078,-0.011276237666606903,0.016372211277484894,-0.039177559316158295,0.048332661390304565,0.07810036092996597,-0.05687250941991806,-0.024654360488057137,0.0034845671616494656,0.0704084113240242,0.051116980612277985,0.00794588215649128,-0.0052092960104346275,0.0096563920378685,-0.04145439341664314,-0.010270693339407444,0.008874132297933102,0.03683992102742195,0.028352228924632072,-0.07811100780963898,-0.02126290649175644,-0.008587769232690334,-0.0731387510895729,0.04243316873908043,0.03991204872727394,0.030864771455526352,0.06774848699569702,0.0011343623045831919,-0.0372549332678318,0.026908550411462784,-0.08309272676706314,1.1626981176959816e-05,0.02160617709159851,0.012921476736664772,0.01681060530245304,-0.006987650878727436,-0.050691504031419754,0.04488814249634743,0.0784604474902153,-0.01222151704132557,-0.019866706803441048,0.042013019323349,-0.020505420863628387,0.06367117911577225,-0.011179104447364807,0.03423139080405235,-0.03432207927107811,0.0336601696908474,0.023112697526812553,-0.004579923115670681,0.05294228717684746,0.0028938790783286095,0.11930016428232193,0.06883595883846283,0.03789292648434639,-0.031409524381160736,-0.037982381880283356,-0.08058344572782516,0.04520599916577339,-0.03564603999257088,0.02233179286122322,0.09258510917425156,-0.030918482691049576,-0.0012204619124531746,-0.018494218587875366,0.14549879729747772,0.022776402533054352,-0.014958394691348076,0.07776074856519699,-0.0487305223941803,-0.01146868895739317,0.018444184213876724,0.020326683297753334,0.015101478435099125,-0.09486169368028641,-0.02243323251605034,0.08631979674100876,0.022361833602190018,0.062483593821525574,-0.012592227198183537,-0.04457937926054001,0.022016210481524467,-0.06269112229347229,-0.03294374793767929,-0.09966778010129929,0.028075620532035828,0.002061976119875908,-0.05294787138700485,-0.05678236111998558,-0.10248316824436188,0.006181477103382349,-0.048434048891067505,0.05923324078321457,0.09883055835962296,0.05166691541671753,0.04476642608642578,-0.010022293776273727,-0.020543042570352554,-0.07232897728681564,0.014863510616123676,0.019114086404442787,-0.004851889796555042,-0.0721956267952919,0.015449012629687786,0.026775695383548737,0.009490693919360638,-0.005367602687329054,-0.00019714153313543648,0.014234348200261593,-0.048813022673130035,0.055334482342004776,0.01785082183778286,-0.035845085978507996,-0.003777110017836094,-0.08390893787145615,0.006594935432076454,-0.048800330609083176,-0.022685278207063675,-0.11395987868309021,0.05772985890507698,0.026509929448366165,-0.03827507793903351,0.019896596670150757,0.007231175899505615,-0.06881126761436462,0.01603531278669834,-0.057354290038347244,-0.035904642194509506,-0.042851854115724564,-0.053093072026968,-0.00914781168103218,-0.05384315922856331,0.08033403009176254,-0.024750757962465286,0.02142411097884178,0.007352905813604593,0.06063348799943924,-0.0018450012430548668,-0.015240820124745369,-0.013011634349822998,-0.0932370200753212,0.03294709697365761,-0.05435248836874962,0.010064669884741306,0.03008275479078293,-0.06799684464931488,0.01118081621825695,-0.010511551052331924,0.020987529307603836,0.018115578219294548,-0.02874777838587761,0.06542360782623291,0.057194605469703674,0.023515740409493446,-0.0580146349966526,-0.023670947179198265,-0.02809096686542034,0.09229771047830582,-0.053449805825948715,-0.09124875068664551,-0.0012734729098156095,0.023086996749043465,1.9035347577300854e-05,-0.050690241158008575,-0.01634734869003296,-0.054565105587244034,-0.0179202388972044,0.011247669346630573,0.06349652260541916,-0.022158341482281685,-0.04675819352269173,-0.014002411626279354,-0.08412935584783554,0.05757282301783562,-0.020498326048254967,0.03668857365846634,0.021434236317873,-0.06936068832874298,0.08243793249130249,-0.05375377833843231,0.015128014609217644,-0.0645824670791626,-0.0013130708830431104,-0.021305855363607407,-0.07436998933553696,-0.07661359012126923,-0.021370859816670418,0.07875450700521469,-0.03365872800350189,0.0146724758669734,-0.0689726397395134,-0.056878138333559036,0.024895504117012024,0.024731501936912537,-0.061564527451992035,0.013011147268116474,0.06827298551797867,0.0020905572455376387,0.026921944692730904,-0.04986003786325455,-0.02767987735569477,-0.027987012639641762,-0.004002929665148258,-0.021418394520878792,-0.09107010811567307,0.04075435549020767,0.005599336698651314,-0.03089129738509655,-0.09984132647514343,-0.04731609299778938,-0.0219984482973814,-0.010212208144366741,-0.010186257772147655,0.011901254765689373,0.017460834234952927,0.09874261170625687,1.2000401511613745e-05,0.0020189040806144476,0.02522922493517399,0.00125973136164248,-0.04798693582415581,0.0010234600631520152,-0.0067580826580524445,0.0033823393750935793,0.03362400457262993,-0.08928629010915756,-0.008283954113721848,-0.03811613842844963,-0.026643993332982063,-0.030749410390853882,0.027287859469652176,0.023642750456929207,0.0038796360604465008,0.055540405213832855,0.06286381930112839,-0.01774020306766033,0.051734015345573425,-0.020586542785167694,-0.028261153027415276,0.1582610160112381,0.01336747594177723,-0.038442742079496384,-0.04161117970943451,0.0004794653214048594,0.03609083592891693,0.0477672703564167,-0.006876674480736256,-0.010367924347519875,0.05378174036741257,0.10002603381872177,-0.0012530825333669782,0.0018071811646223068,-0.011871401220560074,0.0002645525673869997,-0.024211907759308815,0.01574971154332161,-0.1001780778169632,-0.059562746435403824,-0.0303689893335104,0.03018012084066868,-0.05553091689944267,0.021827660501003265,-0.04668952152132988,0.039057131856679916,0.0970066487789154,-0.00334223173558712,-0.09065000712871552,-0.04177406057715416,-0.032880350947380066,-0.05554598197340965,-0.015091018751263618,0.02060128003358841,0.005667736753821373,-0.09660986810922623,-0.03531487658619881,-0.0618869811296463,0.029538288712501526,-0.010356135666370392,0.07105377316474915,-0.001470130286179483,0.06501643359661102,-0.004430912900716066,-0.01033023651689291,-0.024587295949459076,-0.03376200422644615,0.11936061829328537,-0.014294465072453022,-0.027458062395453453,0.0692979171872139,-0.03938046842813492,-0.03193510323762894,-0.00019328566850163043,-0.006142064929008484,-0.005877216812223196,0.011833577416837215,-0.06444914638996124,0.037680480629205704,0.013857515528798103,0.043915074318647385,-0.019561737775802612,-0.05382023751735687,0.02785230241715908,0.02495497092604637,0.004898982122540474,-0.05490961670875549,0.03275443613529205,0.023119080811738968,0.04403900355100632,0.03136463463306427,0.11602727323770523,-0.040565136820077896,0.01620781607925892,-0.019663270562887192,-0.023887937888503075,-0.014881469309329987,0.005714242346584797,0.045318134129047394,-0.05739612504839897,0.1040123924612999,0.036914896219968796,-0.05502843111753464,-0.02275184914469719,-0.07164642959833145,-0.12315217405557632,-0.03977600857615471,-0.02104252576828003,0.050261110067367554,0.04190652072429657,0.01589190401136875,0.004949019756168127,-0.07453009486198425,-0.02989937551319599,-0.036669038236141205,0.08829792588949203,-0.02001042664051056,-0.08403906226158142,0.012975428253412247,0.04326576367020607,-0.04253615811467171,0.017738182097673416,0.006512740161269903,0.05077482387423515,0.0021307882852852345,0.11799611896276474,-0.060068998485803604,-0.023999221622943878,-0.01428302749991417,0.006814718246459961,-0.07932839542627335,-0.045014962553977966,0.04199188947677612,0.08105875551700592,0.050327982753515244,0.0014975343365222216,-0.04622504115104675,-0.05728619173169136,-0.07657716423273087,-0.019055182114243507,0.01322865765541792,0.1054450199007988,-0.09648356586694717,-0.06763320416212082,-0.010853498242795467,-0.0007077623158693314,0.018528064712882042,0.048491593450307846,-0.07081940025091171,-0.010267623700201511,-0.029120033606886864,0.1100861057639122,0.04858127236366272,0.09279575198888779,0.01803252287209034,-0.027649223804473877,-0.003142743604257703,0.028753697872161865,-0.04239331930875778,-0.00344845955260098,0.039445746690034866,-0.010088798590004444,-0.034144144505262375,0.0196500476449728,-0.04838454723358154,-0.018688572570681572,-0.09815070033073425,-0.04402431845664978,-0.06183771789073944,-0.0605945847928524,0.00931202620267868,-0.003232113551348448,-0.03147135302424431,-0.02173640765249729,-0.06669769436120987,0.004654502496123314,0.08508019894361496,-0.021482544019818306,0.0011558716651052237,-0.0659918263554573,0.04599123075604439,-0.04067233204841614,0.02283680997788906,-0.04137200862169266,0.006964207161217928,0.0675588995218277,0.008386357687413692,-0.018664099276065826,0.014019369147717953,0.0016488437540829182,-0.01356069277971983,-0.029761815443634987,0.007186941336840391,-0.018296435475349426,0.11962325870990753,0.0777163952589035,-0.10674884170293808,-0.010601513087749481,-0.05397878959774971,-0.008207201957702637,0.04431811347603798,0.057605721056461334,0.09327578544616699,0.006751133594661951,0.04300929978489876,0.10294829308986664,0.03834925964474678,-0.028171157464385033,0.009930265136063099,-0.03628374636173248,0.05012748762965202,-0.07955841720104218,-0.04269952327013016,-0.10373932123184204,0.016068264842033386,-0.053825974464416504,0.02821224555373192,-0.06370766460895538,0.008522612042725086,0.06576945632696152,0.10356296598911285,-0.059885647147893906,-0.02305898815393448,0.08819606155157089,-0.005509034730494022,0.058365583419799805,-0.011747682467103004,-0.0027912685181945562,-0.017959898337721825,0.04149805009365082,0.05576236918568611,0.03141283243894577,0.05183451250195503,0.05944206938147545,-0.006468936800956726,-0.04088045284152031,-0.06067092716693878,-0.07294540107250214,0.033282581716775894,-0.04199711233377457,0.014690653420984745,0.04540739208459854,0.042027801275253296,-0.05466475337743759,0.02644672803580761,0.06827829778194427,0.07883349061012268,-0.08306767046451569,0.05659431219100952,0.03384275361895561,-0.04972514882683754,-0.0538833811879158,0.027060456573963165,-0.03086833842098713,0.001631931052543223,0.06453549861907959,0.014696717262268066,0.06657776236534119,0.011464743874967098,0.008244985714554787,0.009935088455677032,0.06980272382497787,0.007364990189671516,0.023648520931601524,0.09735068678855896,0.034723103046417236,-0.05248071998357773,-0.02740861102938652,-0.01185793150216341,-0.035028375685214996,0.004787870217114687,0.012950312346220016,-0.01341638807207346,-0.050563663244247437,-0.05245138332247734,-0.008206679485738277,0.062326326966285706,0.0024533167015761137,0.03518804907798767,-0.1040087640285492,0.017815429717302322,-0.002571864752098918,-0.0460803396999836,0.09667012095451355,-0.0696813240647316,0.019580164924263954,-0.03740131855010986,-0.08511470258235931,-0.0036503931041806936,-0.08664777129888535,-0.03554391860961914,0.05256178602576256,0.055590108036994934,-0.026982983574271202,-0.03158506751060486,-0.040259744971990585,0.008635043166577816,0.04608609899878502,0.02741539292037487,0.06273731589317322,0.08395754545927048,-0.04219257831573486,-0.006153685972094536,-0.013261349871754646,-0.036610011011362076,0.019204072654247284,0.025704961270093918,-0.02822762541472912,0.016041386872529984,0.07608445733785629,-0.02893282100558281,-0.03699333593249321,-0.044185612350702286,-0.1012999564409256,0.11668413877487183,-0.06228035315871239,-0.030463218688964844,-0.016929898411035538,0.0012493786634877324,0.03658697009086609,-0.025342853739857674,0.05373018980026245,-0.025830451399087906,-0.02805451489984989,-0.002477760426700115,0.044681087136268616,0.002823165152221918,-0.037195295095443726,-0.00022387092758435756,-0.0009506146889179945,-0.06286782771348953,-0.0427226759493351,0.025470435619354248,0.03869641572237015,0.005406197626143694,0.06963349133729935,-0.0327322855591774,-0.021262306720018387,-0.05033900961279869,-0.002785904798656702,0.006447966210544109,-0.10949062556028366,0.015477088280022144,-0.012033488601446152,0.04295118898153305,-0.0076263584196567535,-0.054019395262002945,-0.02193405292928219,0.015229932963848114,0.05740208178758621,-0.061307597905397415,0.03225525841116905,0.11623204499483109,-0.03149380534887314,-0.04100315272808075,0.004221729468554258,-0.06192043796181679,-0.02257554605603218,0.02404562570154667,0.05329231545329094,0.10110744088888168,0.030265459790825844,0.008197981864213943,-0.09852822870016098,0.03292188420891762,-0.023305991664528847,0.06829741597175598,0.033459149301052094,-0.0004566983843687922,-0.021629564464092255,0.02315988391637802,0.05412376672029495,-0.004163971170783043,-0.0540219247341156,-0.0383925624191761,0.030644018203020096,-0.09922921657562256,-0.07122144103050232,-0.006848189048469067,0.06306970864534378,-0.0339336171746254,-0.06014695018529892,-0.03178096190094948,-0.022566579282283783,-0.005660105496644974,0.017513088881969452,-0.024422353133559227,-0.034402646124362946,0.02432396076619625,-0.02258255146443844,0.02118597738444805,-0.03699425980448723,-0.021684646606445312,-0.044744815677404404,0.028163760900497437,-0.08999476581811905,0.1389172375202179,-0.05121895298361778,0.03451179713010788,0.013063494116067886,-0.0040983771905303,-0.05688106641173363,0.0025593070313334465,0.03820822387933731,0.007958897389471531,0.03949924558401108,0.021668657660484314,-0.01983756385743618,-0.005107662174850702,0.05713001638650894,-0.056349385529756546,-0.07846380770206451,-0.05379351228475571,0.07298596948385239,0.05901513621211052,0.015126966871321201,-0.021032599732279778,0.005492819473147392,-0.022179946303367615,0.01999376341700554,-0.11599690467119217,0.03657323867082596,-0.01903633400797844,0.1142207533121109,0.1063801571726799,-0.031029807403683662,0.008217044174671173,-0.00694684311747551,0.05247382074594498,0.057539038360118866,-0.005409030709415674,0.014518681913614273,0.054362718015909195,0.016147563233971596,-0.054066531360149384,0.032186318188905716,0.012095886282622814,-0.013018617406487465,-0.05893845856189728,-0.04360579699277878,-0.01769564114511013,-0.08669278025627136,-0.008004121482372284,0.052159592509269714,0.002279105130583048,-0.05425892770290375,-0.08547715842723846,-0.06346303969621658,-0.09482082724571228,0.08781478554010391,-0.01968926005065441,0.020196858793497086,-0.08044757694005966,-0.026101727038621902,0.03243251144886017,0.030552148818969727,0.06823184341192245,-0.027353554964065552,0.008763467893004417,-0.020878121256828308,-0.06558182835578918,-0.12397973984479904,-0.02983006276190281,-0.008352573029696941,0.009279021993279457,0.01475480291992426,-0.07387111335992813,0.015412818640470505,0.051448892802000046,-0.03318177908658981,-0.01529289223253727,0.023527035489678383,-0.05233054980635643,-0.04184865579009056,0.01057897973805666,-0.08167927712202072,-0.004737782292068005,0.04872533306479454,0.021129269152879715,0.062626414000988,0.03496889770030975,0.05461374670267105,0.05473044887185097,-0.03758092597126961,0.04906227067112923,-0.04199374467134476,-0.02092895656824112,-0.018137482926249504,-0.02597993053495884,0.03365207090973854,0.027144616469740868,-0.07067937403917313,-0.015804436057806015,-0.03391942381858826,-0.10259965807199478,-0.02783198654651642,-0.03838903084397316,0.012202232144773006,0.007014712784439325,-0.003800211939960718,-0.105845607817173,-0.013158712536096573,-0.02386116236448288,0.016421331092715263,-5.601736847893335e-05,-0.004934342112392187,-0.01104481890797615,-0.0831146091222763,-0.03238846734166145,0.021774010732769966,0.03536950796842575,-0.044993869960308075,0.03437880799174309,0.021299973130226135,0.09123475849628448,-0.03266824781894684,-0.01566162519156933,-0.045226167887449265,-0.024234503507614136,-0.0003877710842061788,-0.02589794062077999,-0.0012531944084912539,0.004735851194709539,-0.0033899350091814995,0.04142560437321663,0.017130542546510696,0.0016124009853228927,-0.026020515710115433,0.08959128707647324,0.029420996084809303,-0.04840068146586418,0.047505952417850494,-0.00042995737749151886,-0.0166687723249197,-0.03525572270154953,-0.01052155438810587,0.02021305076777935,-0.008723793551325798,0.0254786629229784,-0.006369445007294416,-0.07413703203201294,-0.03720008209347725,-0.029039796441793442,-0.06056583672761917,-0.01276811957359314,-0.0776904970407486,0.009771575219929218,-0.021066980436444283,-0.016446130350232124,0.08995536714792252,-0.0441528744995594,-0.028363166376948357,-0.02263678051531315,-0.021410994231700897,-0.0606212317943573,-0.03324604779481888,0.016857990995049477,-0.03680525720119476,-0.09292864054441452,-0.011705543845891953,0.0010326317278668284,0.01980826072394848,0.03071129508316517,-0.07113776355981827,-0.048932455480098724,-0.03439515456557274,0.030453374609351158,0.05031958594918251,0.012748315930366516,0.030590342357754707,0.0008953158976510167,0.047938279807567596,-0.0486595518887043,-0.07247444242238998,-0.005823737941682339,0.018676960840821266,0.023257024586200714,0.04902566224336624,-0.06556607782840729,0.006821179296821356,-0.008722033351659775,0.008730179630219936,-0.05505678430199623,0.021105393767356873,0.024368425831198692,0.024113666266202927,0.10016464442014694,0.06476007401943207,-0.0597805380821228,0.04543990269303322,0.04829920828342438,0.07007138431072235,0.0034192767925560474,0.036864444613456726,-0.0733204111456871,0.04064919799566269,0.03901592642068863,0.046776168048381805,0.02629554457962513,0.05524981766939163,-0.08388545364141464,0.0024842654820531607,-0.032120268791913986,-0.009708101861178875,0.038159068673849106,0.032196640968322754,-0.03275236859917641,-0.03286976367235184,0.08642803877592087,-0.07133656740188599,-0.01631227880716324,-0.04664669930934906,0.02933507226407528,-0.05867643281817436,0.06519765406847,0.021322447806596756,-0.044793590903282166,-0.06408719718456268,-0.04062676802277565,-0.0028058469761162996,-0.028592776507139206,0.005072282161563635,-0.009405572898685932,0.028777586296200752,0.05414443835616112,0.0300894882529974,0.03205560892820358,0.02290095016360283,0.010005989111959934,0.014535283669829369,-0.042085450142621994,-0.001464726054109633,0.013615732081234455,-0.11384189128875732,-0.00042912911158055067,0.08822790533304214,0.04280075803399086,-0.01226085051894188,0.026778416708111763,0.06271817535161972,-0.06438057869672775,-0.1239776536822319,0.045660391449928284,-0.03915092721581459,0.00794632826000452,0.022321349009871483,-0.015362784266471863,0.05997699126601219,-0.03121812641620636,-0.0006633733864873648,-0.04524560645222664,0.012438323348760605,0.06505558639764786,-0.02489045076072216,0.05295230820775032,-0.00769166462123394,-0.008832722902297974,0.031238717958331108,-0.008500429801642895,0.009982902556657791,0.051779814064502716,0.029861627146601677,-0.013033019378781319,0.009921200573444366,-0.03626679256558418,-0.036831118166446686,0.03474488481879234,-0.009977473877370358,-0.023460041731595993,0.09108279645442963,-0.005205919966101646,0.005788924638181925,0.06162775307893753,-0.0481146015226841,0.01560547761619091,0.01379909086972475,0.04920094460248947,-0.05623611435294151,-0.04384352266788483,0.05252086743712425,0.0022265356965363026,-0.042598482221364975,0.027391720563173294,0.04578041285276413,-0.03665218874812126,0.002009084215387702,-0.04297209903597832,-0.008007500320672989,0.0049215457402169704,-0.013268934562802315,0.07584219425916672,-0.059234075248241425,0.008327149786055088,-0.0922081470489502,0.006343155633658171,0.004155779257416725,0.01312166266143322,0.038432054221630096,0.07916969060897827,0.015575542114675045,0.03577173873782158,0.042593345046043396,-0.025287924334406853,0.04714491590857506,-0.01158407237380743,-0.03669248893857002,0.01137405913323164,-0.03295503556728363,-0.03317665681242943,0.06509853154420853,0.07457707077264786,0.018907638266682625,0.015631265938282013,0.01024606917053461,0.0009987996891140938,-0.08519618213176727,0.028892241418361664,-0.07281159609556198,-0.0003842141595669091,0.039774902164936066,0.015161339193582535,0.054802797734737396,0.022265704348683357,0.001860613701865077,0.07117106765508652,0.09585531055927277,-0.011276071891188622,0.02751881256699562,0.005319100338965654,-0.0006132054841145873,-0.03934885933995247,-0.027732644230127335,-0.0011327051324769855,-0.011431753635406494,-0.05574849620461464,-0.060108330100774765,-0.006574437953531742,0.020600298419594765,0.023172322660684586,0.04145871102809906,0.0025174510665237904,-0.008627906441688538,0.0654897689819336,0.058482568711042404,0.049655940383672714,-0.0045619807206094265,0.07189827412366867,-0.01894783042371273,-0.06835969537496567,-0.042958080768585205,-0.0531807616353035,0.05445312336087227,-0.11697392165660858,-0.1104491651058197,-0.010981154628098011,-0.07523003220558167,0.0007447918178513646,8.249147504102439e-05,0.06576711684465408,-0.02350740134716034,0.037259504199028015,-0.003554146969690919,0.044223230332136154,0.03585909307003021,0.002789992606267333,-0.0662342980504036,0.011705189011991024,0.024845177307724953,-0.04629207402467728,0.03883520886301994,-0.04617921635508537,-0.0824350118637085,0.05054232105612755,-0.02822905220091343,-0.09810731559991837,0.008560804650187492,0.03157161548733711,0.0010064705274999142,0.03927933797240257,-0.0875353291630745,0.034024372696876526,0.06510000675916672,-0.06704115867614746,0.004561623092740774,-0.06686442345380783,-0.030281683430075645,0.05936837196350098,0.004761362448334694,-0.013935865834355354,0.025520356371998787,-0.08170535415410995,-0.038062915205955505,0.05828667804598808,0.10200252383947372,0.010392384603619576,-0.027096925303339958,0.014793802984058857,-0.016970276832580566,0.10762068629264832,-0.02264973148703575,0.002544223330914974,0.008625561371445656,0.06299641728401184,0.014048479497432709,0.022868897765874863,-0.004897008184343576,0.0890488252043724,0.01589413732290268,0.010817458853125572,-0.12598943710327148,0.015487843193113804,-0.03958861902356148,0.03329867124557495,0.17098231613636017,-0.08821547031402588,0.046161361038684845,-0.07933276891708374,0.03449876233935356,-0.07283443957567215,0.030551712960004807,0.00929484236985445,-0.02283621020615101,0.022194871678948402],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x0\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x1\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"color\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f4af6e93-2687-41d8-81da-3b8622f7dcee');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import plotly.express as px\n",
        "fig = px.scatter(embedding_df,\n",
        "                 x='x0',\n",
        "                 y='x1',\n",
        "                 size=[2]*len(embedding_df),\n",
        "                 size_max = 10,\n",
        "                 hover_name = 'word',\n",
        "                 color='color'\n",
        "                 )\n",
        "fig.show()\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBmvULcaHczx"
      },
      "source": [
        "1. The clusterings simply divide the words based on their x0 weights, so the variations mainly exist there. Some of the significant outliers of the x0 axis are **Trumps** (Trump's), rep, gop, j, im, more, mr, that, nov. Some of them are more like stop words (I'm, that before standardization) that appear a lot but don't really mean anything. Trump is an outlier because obviously he's one of his kind in terms of spreading false information, making false claims, and creating chaos on the social network (Trump's twitter).  \n",
        "\n",
        "2. Meanwhile, **trump** appears somewhere in the middle orange cluster as well as some other politician last name (like **clinton**). The standardization does not collapse trumps and trump into the same thing. So politician last names mostly appear in the same group. \n",
        "\n",
        "3. In terms of the x1 axis, one word that gets weighted heavily is **knowledge**. It's quite easy to think of sentences like 'currently, scientists have xx knowledge on...', 'officials claimed no knowledge of...' to be in a supposedly 'informative' piece of news.  \n",
        "\n",
        "4. The rightest cluster has words like **friday, tuesday, thurdsay** that belong to the same category. To the right, there are adverbs like **allegedly, apparently, recently** that seems to be common in all news articles. \n",
        "\n",
        "5. The middle clusters are mostly noun, proper nouns, and verb that are not weighted heavily.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZBVK1wxZq8k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
