<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>fake</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="fake_files/libs/clipboard/clipboard.min.js"></script>
<script src="fake_files/libs/quarto-html/quarto.js"></script>
<script src="fake_files/libs/quarto-html/popper.min.js"></script>
<script src="fake_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="fake_files/libs/quarto-html/anchor.min.js"></script>
<link href="fake_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="fake_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="fake_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="fake_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="fake_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="fake-news-classification-with-tensorflow" class="level1">
<h1>Fake News Classification with Tensorflow</h1>
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro:</h2>
<p>Following the previous blog post on image classification, we would be using tensorflow again to perform machine learning tasks —— on text data this time.</p>
<p>While Plague Inc.&nbsp;went ‘viral’ again during the beginning of the COVID-19 outbreak due to public attention on the pandemics, my favourite game scenario in Plague In. has always been the fake news mode. Albeit some headlines are straight-out troll, the existence of such scenario still speaks volume about how the deliberate use of modern technology and psychological tricks could be used to infect the world with false information and cause extreme consequences to the democracy and health of the soceity.</p>
<p>In this blog, we would be getting some hand-on combat experience against fake news through the creation of a ML &amp; N-Gram based fake news classification model.</p>
</section>
<section id="data-acquisition" class="level2">
<h2 class="anchored" data-anchor-id="data-acquisition">Data acquisition</h2>
<p>The following data is a small segment of a Kaggle fakenews dataset. Each row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper：</p>
<p><em>Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp.&nbsp;127-138).</em></p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.io <span class="im">as</span> pio</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>pio.renderers.default <span class="op">=</span> <span class="st">"notebook_connected"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>train_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"</span> </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>news <span class="op">=</span> pd.read_csv(train_url)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="9f26858a-cf4e-49ce-fcb1-840e6ab2287d" data-execution_count="18">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(news.dtypes)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>news.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Unnamed: 0     int64
title         object
text          object
fake           int64
dtype: object</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="18">


  <div id="df-cd2f7173-ec7e-45bc-8799-102f2813ad0f">
    <div class="colab-df-container">
      <div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-cd2f7173-ec7e-45bc-8799-102f2813ad0f')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-cd2f7173-ec7e-45bc-8799-102f2813ad0f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-cd2f7173-ec7e-45bc-8799-102f2813ad0f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
</section>
<section id="create-dataset" class="level2">
<h2 class="anchored" data-anchor-id="create-dataset">Create Dataset</h2>
<p>To parse the text and perform topic analysis, the first step is removing stopwords, i.e., uninformative words like ‘and’,‘the’,‘a’, etc. We make use of the nltk library to perform such task.</p>
<div class="cell" data-outputid="1b128035-e3be-4e29-b531-a62a3e8be4b7" data-execution_count="19">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>True</code></pre>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we will create a tensorflow dataset to host our news data. Tensorflow datasets are iterable and well-integrated with the machine learning pipeline. We would write a create_database function that does the following 2 things:</p>
<ol type="1">
<li><p>Remove stopwords from the article text and title.</p></li>
<li><p>Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column.</p></li>
</ol>
<p>For a tf dataset: - Elements refer to a single output from calling next() on a dataset iterator. Elements may be nested structures containing multiple components. For example, the element (1, (3, “apple”)) has one tuple nested in another tuple. The components are 1, 3, and “apple”.<br>
- Components refers to the leaves in the nested structure of an element. - To set up a dataset for ML training, we need something in this format:<br>
ds = tf.data.Dataset.from_tensor_slices((features_dict, labels))</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_dataset(df):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  stop <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># retain the rest of words, separated by space</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  df[<span class="st">'text'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>                                                        <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> (stop)]))</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>  df[<span class="st">'title'</span>] <span class="op">=</span> df[<span class="st">'title'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>                                                        <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> (stop)]))</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Construct tf dataset</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>  tfds <span class="op">=</span> tf.data.Dataset.from_tensor_slices(({<span class="st">"title"</span>:df[[<span class="st">"title"</span>]],<span class="st">"text"</span>:df[<span class="st">"text"</span>]},<span class="co">#feature_dict</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>            {<span class="st">'fake'</span>:df[[<span class="st">"fake"</span>]]}))<span class="co">#labels</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># batch </span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>  tfds <span class="op">=</span> tfds.batch(<span class="dv">100</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> tfds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> make_dataset(news)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="96a1ead6-380f-4a84-d426-cc9357b63ff4">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx,lbl <span class="kw">in</span> ds.take(<span class="dv">1</span>): <span class="co"># similar to data[:5]</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(idx[<span class="st">'text'</span>])</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(idx[<span class="st">'title'</span>])</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(lbl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="validation-data" class="level2">
<h2 class="anchored" data-anchor-id="validation-data">Validation Data</h2>
<p>After constructing the primary dataset, we split off 20% for validation using skip and take.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span> <span class="op">*</span> <span class="bu">len</span>(ds))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> ds.shuffle(buffer_size <span class="op">=</span> <span class="bu">len</span>(ds))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> ds.take(val_size)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> ds.skip(val_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="c8b944e7-462d-487c-f6b3-7b0adcfeab8a" data-execution_count="142">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(val_ds),<span class="bu">len</span>(train_ds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="142">
<pre><code>(45, 180)</code></pre>
</div>
</div>
</section>
<section id="base-rate" class="level2">
<h2 class="anchored" data-anchor-id="base-rate">Base Rate</h2>
<p>Base rate refers to the accuracy of a model that always makes the same guess. Determine the base rate for this data set by examining the labels on the training set —— the base rate would be the proportion of the label with the highest frequency in the label pool: - 1: fake - 0: non-fake</p>
<p>In this case, the base rate is <strong>0.5230</strong> (52.30% of the entries are fake news in this dataset.)</p>
<div class="cell" data-outputid="8d74eed5-7442-48b3-8a2c-534037173d89" data-execution_count="144">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>news.fake.value_counts()<span class="op">/</span><span class="bu">len</span>(news)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="144">
<pre><code>1    0.522963
0    0.477037
Name: fake, dtype: float64</code></pre>
</div>
</div>
</section>
<section id="textvectorization" class="level2">
<h2 class="anchored" data-anchor-id="textvectorization">TextVectorization</h2>
<p>Preprocess text and then map words to integers: we would create a frequency dictionary that encodes words with their total numbers of appearances in the dataset. And we set a limit of 2000 to only use the most frequent 2000 words to set up our word dictionary for training.</p>
<div class="cell" data-outputid="50915af9-abd8-46a0-c0e8-df64a448b143" data-execution_count="24">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">#preparing a title vectorization layer for tf model</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>size_vocabulary <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">#convert all strings to lower cases,</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">#get rid of all puncuations</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standardization(input_data):</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    lowercase <span class="op">=</span> tf.strings.lower(input_data)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    no_punctuation <span class="op">=</span> tf.strings.regex_replace(lowercase,</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>                                  <span class="st">'[</span><span class="sc">%s</span><span class="st">]'</span> <span class="op">%</span> re.escape(string.punctuation),<span class="st">''</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> no_punctuation </span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer <span class="op">=</span> layers.TextVectorization(</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    standardize<span class="op">=</span>standardization,</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span>size_vocabulary, <span class="co"># only consider this many words</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">'int'</span>,</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    output_sequence_length<span class="op">=</span><span class="dv">500</span>) </span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co">#this will make the layer 'learn' whatever words we've included from the titles</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer.adapt(train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="st">"title"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.
Instructions for updating:
Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089</code></pre>
</div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co">#preparing a text vectorization layer for tf model</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standardization(input_data):</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    lowercase <span class="op">=</span> tf.strings.lower(input_data)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    no_punctuation <span class="op">=</span> tf.strings.regex_replace(lowercase,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>                                  <span class="st">'[</span><span class="sc">%s</span><span class="st">]'</span> <span class="op">%</span> re.escape(string.punctuation),<span class="st">''</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> no_punctuation </span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>text_vectorize_layer <span class="op">=</span> layers.TextVectorization(</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    standardize<span class="op">=</span>standardization,</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span>size_vocabulary, <span class="co"># only consider this many words</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">'int'</span>,</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    output_sequence_length<span class="op">=</span><span class="dv">500</span>) </span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>text_vectorize_layer.adapt(train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="st">"text"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="create-a-model" class="level2">
<h2 class="anchored" data-anchor-id="create-a-model">Create a Model</h2>
<p>We would be building three models (using the functional API of keras) that train on only title, only text, both title and text respectively to answer the question:</p>
<p><strong>When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?</strong></p>
<ul>
<li>In the first model, you should use only the article title as an input.</li>
<li>In the second model, you should use only the article text as an input.</li>
<li>In the third model, you should use both the article title and the article text as input.</li>
</ul>
<p>(Applied to text vectorization layer adaptation as well)</p>
<p>As suggested, we define an embedding layer that would be shared by all three models.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>max_tokens <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>output_sequence_length <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> layers.Embedding(max_tokens, output_dim <span class="op">=</span> <span class="dv">3</span>, name<span class="op">=</span><span class="st">"embedding"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="article-title-only" class="level3">
<h3 class="anchored" data-anchor-id="article-title-only">Article title only</h3>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="237161b6-653b-4340-87ea-4b5c326efbc5" data-execution_count="27">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>title_in <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,),name <span class="op">=</span> <span class="st">"title"</span>, dtype <span class="op">=</span> <span class="st">"string"</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>title_layer <span class="op">=</span> title_vectorize_layer(title_in) <span class="co">#vectorize title</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>title_layer <span class="op">=</span> emb(title_layer) <span class="co">#shared embedding</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>title_layer <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(title_layer) <span class="co">#randomaly drop 20% of the connections to reduce overfitting</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>title_layer <span class="op">=</span> layers.GlobalAveragePooling1D()(title_layer) <span class="co">#take the average of embedding vectors along the time axis</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>title_layer <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(title_layer)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>title_layer <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(title_layer)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># output layer</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> layers.Dense(<span class="dv">2</span>, name <span class="op">=</span> <span class="st">"fake"</span>)(title_layer)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> keras.Model(inputs <span class="op">=</span> title_in,outputs <span class="op">=</span> output,name<span class="op">=</span><span class="st">'title_only'</span>)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>model1.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "title_only"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 title (InputLayer)          [(None, 1)]               0         
                                                                 
 text_vectorization (TextVec  (None, 500)              0         
 torization)                                                     
                                                                 
 embedding (Embedding)       (None, 500, 3)            6000      
                                                                 
 dropout (Dropout)           (None, 500, 3)            0         
                                                                 
 global_average_pooling1d (G  (None, 3)                0         
 lobalAveragePooling1D)                                          
                                                                 
 dropout_1 (Dropout)         (None, 3)                 0         
                                                                 
 dense (Dense)               (None, 32)                128       
                                                                 
 fake (Dense)                (None, 2)                 66        
                                                                 
=================================================================
Total params: 6,194
Trainable params: 6,194
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-execution_count="287">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model1.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="c5b1a1cb-657d-409a-82d6-02d3da439299" data-execution_count="263">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>history1 <span class="op">=</span> model1.fit(train_ds, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val_ds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>180/180 [==============================] - 5s 19ms/step - loss: 0.6918 - accuracy: 0.5204 - val_loss: 0.6895 - val_accuracy: 0.5248
Epoch 2/20
180/180 [==============================] - 3s 14ms/step - loss: 0.6809 - accuracy: 0.5791 - val_loss: 0.6627 - val_accuracy: 0.6393
Epoch 3/20
180/180 [==============================] - 3s 18ms/step - loss: 0.6163 - accuracy: 0.7934 - val_loss: 0.5440 - val_accuracy: 0.9353
Epoch 4/20
180/180 [==============================] - 6s 34ms/step - loss: 0.4662 - accuracy: 0.9038 - val_loss: 0.3705 - val_accuracy: 0.9411
Epoch 5/20
180/180 [==============================] - 2s 12ms/step - loss: 0.3244 - accuracy: 0.9332 - val_loss: 0.2537 - val_accuracy: 0.9504
Epoch 6/20
180/180 [==============================] - 2s 12ms/step - loss: 0.2380 - accuracy: 0.9452 - val_loss: 0.1815 - val_accuracy: 0.9618
Epoch 7/20
180/180 [==============================] - 3s 16ms/step - loss: 0.1870 - accuracy: 0.9529 - val_loss: 0.1344 - val_accuracy: 0.9711
Epoch 8/20
180/180 [==============================] - 2s 11ms/step - loss: 0.1579 - accuracy: 0.9562 - val_loss: 0.1206 - val_accuracy: 0.9674
Epoch 9/20
180/180 [==============================] - 2s 11ms/step - loss: 0.1364 - accuracy: 0.9608 - val_loss: 0.0928 - val_accuracy: 0.9762
Epoch 10/20
180/180 [==============================] - 2s 12ms/step - loss: 0.1171 - accuracy: 0.9671 - val_loss: 0.0967 - val_accuracy: 0.9667
Epoch 11/20
180/180 [==============================] - 2s 12ms/step - loss: 0.1102 - accuracy: 0.9659 - val_loss: 0.0836 - val_accuracy: 0.9718
Epoch 12/20
180/180 [==============================] - 3s 18ms/step - loss: 0.1007 - accuracy: 0.9684 - val_loss: 0.0698 - val_accuracy: 0.9813
Epoch 13/20
180/180 [==============================] - 2s 11ms/step - loss: 0.0932 - accuracy: 0.9702 - val_loss: 0.0697 - val_accuracy: 0.9818
Epoch 14/20
180/180 [==============================] - 2s 12ms/step - loss: 0.0884 - accuracy: 0.9715 - val_loss: 0.0608 - val_accuracy: 0.9831
Epoch 15/20
180/180 [==============================] - 2s 11ms/step - loss: 0.0850 - accuracy: 0.9706 - val_loss: 0.0641 - val_accuracy: 0.9827
Epoch 16/20
180/180 [==============================] - 3s 18ms/step - loss: 0.0792 - accuracy: 0.9737 - val_loss: 0.0552 - val_accuracy: 0.9827
Epoch 17/20
180/180 [==============================] - 2s 12ms/step - loss: 0.0746 - accuracy: 0.9747 - val_loss: 0.0609 - val_accuracy: 0.9782
Epoch 18/20
180/180 [==============================] - 2s 12ms/step - loss: 0.0756 - accuracy: 0.9738 - val_loss: 0.0476 - val_accuracy: 0.9824
Epoch 19/20
180/180 [==============================] - 2s 12ms/step - loss: 0.0690 - accuracy: 0.9770 - val_loss: 0.0466 - val_accuracy: 0.9842
Epoch 20/20
180/180 [==============================] - 2s 11ms/step - loss: 0.0682 - accuracy: 0.9753 - val_loss: 0.0424 - val_accuracy: 0.9860</code></pre>
</div>
</div>
<p>From the model fitting history, the title-input only fake news classification reaches a <strong>98.60%</strong> validation accuracy, which is slightly higher than the training accuracy <strong>97.53%</strong>.</p>
</section>
</section>
<section id="article-text-only." class="level2">
<h2 class="anchored" data-anchor-id="article-text-only.">Article text only.</h2>
<div class="cell" data-outputid="d0b76846-62ac-4e63-b947-f95fd40df8f0" data-execution_count="28">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>text_in <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,),name <span class="op">=</span> <span class="st">"text"</span>, dtype <span class="op">=</span> <span class="st">"string"</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>text_layer <span class="op">=</span> text_vectorize_layer(text_in) <span class="co">#vectorize title</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>text_layer <span class="op">=</span> emb(text_layer) <span class="co">#shared embedding</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>text_layer <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(text_layer) <span class="co">#randomaly drop 20% of the connections to reduce overfitting</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>text_layer <span class="op">=</span> layers.GlobalAveragePooling1D()(text_layer) <span class="co">#take the average of embedding vectors along the time axis</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>text_layer <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(text_layer)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>text_layer <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(text_layer)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co"># output layer</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> layers.Dense(<span class="dv">2</span>, name <span class="op">=</span> <span class="st">"fake"</span>)(text_layer)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> keras.Model(inputs <span class="op">=</span> text_in,outputs <span class="op">=</span> output,name<span class="op">=</span><span class="st">'text_only'</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>model2.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "text_only"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 text (InputLayer)           [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 500)              0         
 ectorization)                                                   
                                                                 
 embedding (Embedding)       (None, 500, 3)            6000      
                                                                 
 dropout_2 (Dropout)         (None, 500, 3)            0         
                                                                 
 global_average_pooling1d_1   (None, 3)                0         
 (GlobalAveragePooling1D)                                        
                                                                 
 dropout_3 (Dropout)         (None, 3)                 0         
                                                                 
 dense_1 (Dense)             (None, 32)                128       
                                                                 
 fake (Dense)                (None, 2)                 66        
                                                                 
=================================================================
Total params: 6,194
Trainable params: 6,194
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-execution_count="288">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>model2.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="0f3e71a5-f1f4-4f3c-d398-adea71e54fc7" data-execution_count="267">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>history2 <span class="op">=</span> model2.fit(train_ds, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val_ds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>180/180 [==============================] - 5s 20ms/step - loss: 0.6887 - accuracy: 0.5390 - val_loss: 0.6803 - val_accuracy: 0.5831
Epoch 2/20
180/180 [==============================] - 4s 20ms/step - loss: 0.6538 - accuracy: 0.6232 - val_loss: 0.5856 - val_accuracy: 0.8478
Epoch 3/20
180/180 [==============================] - 4s 23ms/step - loss: 0.5290 - accuracy: 0.7648 - val_loss: 0.3800 - val_accuracy: 0.8930
Epoch 4/20
180/180 [==============================] - 4s 20ms/step - loss: 0.3859 - accuracy: 0.8456 - val_loss: 0.2583 - val_accuracy: 0.9236
Epoch 5/20
180/180 [==============================] - 5s 27ms/step - loss: 0.3007 - accuracy: 0.8802 - val_loss: 0.2244 - val_accuracy: 0.9271
Epoch 6/20
180/180 [==============================] - 4s 20ms/step - loss: 0.2516 - accuracy: 0.9063 - val_loss: 0.1965 - val_accuracy: 0.9413
Epoch 7/20
180/180 [==============================] - 4s 20ms/step - loss: 0.2223 - accuracy: 0.9219 - val_loss: 0.1795 - val_accuracy: 0.9476
Epoch 8/20
180/180 [==============================] - 5s 26ms/step - loss: 0.1967 - accuracy: 0.9338 - val_loss: 0.1423 - val_accuracy: 0.9580
Epoch 9/20
180/180 [==============================] - 4s 20ms/step - loss: 0.1759 - accuracy: 0.9415 - val_loss: 0.1314 - val_accuracy: 0.9611
Epoch 10/20
180/180 [==============================] - 4s 20ms/step - loss: 0.1654 - accuracy: 0.9447 - val_loss: 0.1214 - val_accuracy: 0.9667
Epoch 11/20
180/180 [==============================] - 5s 27ms/step - loss: 0.1584 - accuracy: 0.9474 - val_loss: 0.1189 - val_accuracy: 0.9682
Epoch 12/20
180/180 [==============================] - 3s 19ms/step - loss: 0.1469 - accuracy: 0.9519 - val_loss: 0.0997 - val_accuracy: 0.9719
Epoch 13/20
180/180 [==============================] - 4s 24ms/step - loss: 0.1371 - accuracy: 0.9566 - val_loss: 0.1065 - val_accuracy: 0.9707
Epoch 14/20
180/180 [==============================] - 4s 20ms/step - loss: 0.1400 - accuracy: 0.9560 - val_loss: 0.1080 - val_accuracy: 0.9709
Epoch 15/20
180/180 [==============================] - 3s 19ms/step - loss: 0.1325 - accuracy: 0.9571 - val_loss: 0.0907 - val_accuracy: 0.9747
Epoch 16/20
180/180 [==============================] - 4s 24ms/step - loss: 0.1227 - accuracy: 0.9605 - val_loss: 0.0901 - val_accuracy: 0.9731
Epoch 17/20
180/180 [==============================] - 4s 20ms/step - loss: 0.1206 - accuracy: 0.9610 - val_loss: 0.0938 - val_accuracy: 0.9749
Epoch 18/20
180/180 [==============================] - 3s 19ms/step - loss: 0.1154 - accuracy: 0.9621 - val_loss: 0.0848 - val_accuracy: 0.9762
Epoch 19/20
180/180 [==============================] - 4s 23ms/step - loss: 0.1116 - accuracy: 0.9645 - val_loss: 0.0798 - val_accuracy: 0.9782
Epoch 20/20
180/180 [==============================] - 4s 20ms/step - loss: 0.1072 - accuracy: 0.9649 - val_loss: 0.0790 - val_accuracy: 0.9800</code></pre>
</div>
</div>
<p>From the model fitting history, the title-input only fake news classification reaches a <strong>98.00%</strong> validation accuracy, which is slightly higher than the training accuracy <strong>96.49%</strong>, performing a little bit worse than the title-only model.</p>
</section>
<section id="combining-text-and-title" class="level2">
<h2 class="anchored" data-anchor-id="combining-text-and-title">Combining text and title</h2>
<p>We use <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate">concatenate</a> to combine the above two model</p>
<div class="cell" data-outputid="8003cb2a-d134-468d-f0a6-645df282776a" data-execution_count="29">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>both <span class="op">=</span> layers.concatenate([title_layer, text_layer], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>both <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(both)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> layers.Dense(<span class="dv">2</span>, name <span class="op">=</span> <span class="st">"fake"</span>)(both)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>model3 <span class="op">=</span> keras.Model(inputs <span class="op">=</span> [title_in,text_in],outputs <span class="op">=</span> output,name<span class="op">=</span><span class="st">'both'</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>model3.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "both"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                  
 text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                  
 text_vectorization (TextVector  (None, 500)         0           ['title[0][0]']                  
 ization)                                                                                         
                                                                                                  
 text_vectorization_1 (TextVect  (None, 500)         0           ['text[0][0]']                   
 orization)                                                                                       
                                                                                                  
 embedding (Embedding)          (None, 500, 3)       6000        ['text_vectorization[0][0]',     
                                                                  'text_vectorization_1[0][0]']   
                                                                                                  
 dropout (Dropout)              (None, 500, 3)       0           ['embedding[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 500, 3)       0           ['embedding[1][0]']              
                                                                                                  
 global_average_pooling1d (Glob  (None, 3)           0           ['dropout[0][0]']                
 alAveragePooling1D)                                                                              
                                                                                                  
 global_average_pooling1d_1 (Gl  (None, 3)           0           ['dropout_2[0][0]']              
 obalAveragePooling1D)                                                                            
                                                                                                  
 dropout_1 (Dropout)            (None, 3)            0           ['global_average_pooling1d[0][0]'
                                                                 ]                                
                                                                                                  
 dropout_3 (Dropout)            (None, 3)            0           ['global_average_pooling1d_1[0][0
                                                                 ]']                              
                                                                                                  
 dense (Dense)                  (None, 32)           128         ['dropout_1[0][0]']              
                                                                                                  
 dense_1 (Dense)                (None, 32)           128         ['dropout_3[0][0]']              
                                                                                                  
 concatenate (Concatenate)      (None, 64)           0           ['dense[0][0]',                  
                                                                  'dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 32)           2080        ['concatenate[0][0]']            
                                                                                                  
 fake (Dense)                   (None, 2)            66          ['dense_2[0][0]']                
                                                                                                  
==================================================================================================
Total params: 8,402
Trainable params: 8,402
Non-trainable params: 0
__________________________________________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>model3.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="b8175600-05ec-4292-a8d1-1f061091bc7c" data-execution_count="36">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>history3 <span class="op">=</span> model3.fit(train_ds, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val_ds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 [==============================] - 5s 21ms/step - loss: 0.6377 - accuracy: 0.6446 - val_loss: 0.4093 - val_accuracy: 0.9131
Epoch 2/20
180/180 [==============================] - 5s 25ms/step - loss: 0.2503 - accuracy: 0.9271 - val_loss: 0.1526 - val_accuracy: 0.9618
Epoch 3/20
180/180 [==============================] - 4s 19ms/step - loss: 0.1470 - accuracy: 0.9570 - val_loss: 0.1071 - val_accuracy: 0.9707
Epoch 4/20
180/180 [==============================] - 5s 26ms/step - loss: 0.1141 - accuracy: 0.9665 - val_loss: 0.0888 - val_accuracy: 0.9776
Epoch 5/20
180/180 [==============================] - 4s 20ms/step - loss: 0.0986 - accuracy: 0.9694 - val_loss: 0.0654 - val_accuracy: 0.9834
Epoch 6/20
180/180 [==============================] - 4s 20ms/step - loss: 0.0909 - accuracy: 0.9730 - val_loss: 0.0627 - val_accuracy: 0.9867
Epoch 7/20
180/180 [==============================] - 4s 20ms/step - loss: 0.0773 - accuracy: 0.9765 - val_loss: 0.0599 - val_accuracy: 0.9864
Epoch 8/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0756 - accuracy: 0.9770 - val_loss: 0.0550 - val_accuracy: 0.9858
Epoch 9/20
180/180 [==============================] - 4s 23ms/step - loss: 0.0661 - accuracy: 0.9814 - val_loss: 0.0408 - val_accuracy: 0.9904
Epoch 10/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0573 - accuracy: 0.9818 - val_loss: 0.0403 - val_accuracy: 0.9904
Epoch 11/20
180/180 [==============================] - 4s 24ms/step - loss: 0.0586 - accuracy: 0.9822 - val_loss: 0.0433 - val_accuracy: 0.9911
Epoch 12/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0581 - accuracy: 0.9823 - val_loss: 0.0336 - val_accuracy: 0.9933
Epoch 13/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0499 - accuracy: 0.9838 - val_loss: 0.0409 - val_accuracy: 0.9911
Epoch 14/20
180/180 [==============================] - 4s 23ms/step - loss: 0.0509 - accuracy: 0.9845 - val_loss: 0.0336 - val_accuracy: 0.9916
Epoch 15/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0468 - accuracy: 0.9847 - val_loss: 0.0279 - val_accuracy: 0.9929
Epoch 16/20
180/180 [==============================] - 5s 27ms/step - loss: 0.0450 - accuracy: 0.9852 - val_loss: 0.0284 - val_accuracy: 0.9922
Epoch 17/20
180/180 [==============================] - 4s 20ms/step - loss: 0.0429 - accuracy: 0.9850 - val_loss: 0.0246 - val_accuracy: 0.9936
Epoch 18/20
180/180 [==============================] - 5s 26ms/step - loss: 0.0395 - accuracy: 0.9869 - val_loss: 0.0253 - val_accuracy: 0.9940
Epoch 19/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0426 - accuracy: 0.9845 - val_loss: 0.0236 - val_accuracy: 0.9955
Epoch 20/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0407 - accuracy: 0.9864 - val_loss: 0.0206 - val_accuracy: 0.9964</code></pre>
</div>
</div>
<p>We reached the highest validation accuracy so far - <strong>99.64%</strong> with a training accuracy of <strong>98.64%</strong> using both text and title.</p>
</section>
<section id="model-evaluation." class="level2">
<h2 class="anchored" data-anchor-id="model-evaluation.">Model Evaluation.</h2>
<p>Let’s examine how well our classification model performs on unforseen data.</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>test_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>testdf <span class="op">=</span> pd.read_csv(test_url)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> make_dataset(testdf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="2a7e8777-7bbb-4b5f-9c10-caa032b91222" data-execution_count="34">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>model3.metrics_names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>[]</code></pre>
</div>
</div>
<div class="cell" data-outputid="16216696-afa7-4710-d5d9-72494bc7a20a" data-execution_count="38">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>model3.evaluate(test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>225/225 [==============================] - 2s 8ms/step - loss: 0.0765 - accuracy: 0.9815</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>[0.07650567591190338, 0.9815136790275574]</code></pre>
</div>
</div>
<p>We achieved a <strong>98.15% accuracy</strong> in fake news classification on the test data.</p>
</section>
<section id="embedding-visualization" class="level2">
<h2 class="anchored" data-anchor-id="embedding-visualization">Embedding Visualization</h2>
<p>A word embedding is a learned representation for text where words that have the same meaning have a similar representation. One of the ways to learn word embedding is through an embedding layer, a word embedding that is learned jointly with a neural network model on a specific natural language processing task, such as fake news classification.</p>
<p>We will use PCA (principal component analysis) to distill the embedding down to two dimensions for ease of visualization while perserving the variations among words.</p>
<div class="cell" data-execution_count="305">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>text_vectorize_layer.adapt(train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="st">"title"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> text_vectorize_layer.get_vocabulary() <span class="co"># keeps track of mapping from word to integer</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> model3.get_layer(<span class="st">"embedding"</span>).get_weights()[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="8c070271-d0fb-40c9-971d-ae6715cdd795" data-execution_count="41">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>weights.shape <span class="co"># 2000 vocabs x 3 dimensional space</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>(2000, 3)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA </span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co"># principal components analysis - </span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co"># project things to lower dimension such that the variance of the dataset is most preserved</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pca.fit_transform(weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>embedding_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word'</span>: vocab,</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x0'</span>: weights[:, <span class="dv">0</span>],</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x1'</span>: weights[:, <span class="dv">1</span>]</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We proceed to color the embedding [KMeans(https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)]</p>
<div class="cell" data-outputid="3f849148-e7ef-4d64-eeb0-1e32142e4dd8" data-execution_count="59">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> embedding_df[[<span class="st">'x0'</span>,<span class="st">'x1'</span>]]</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">7</span>, random_state<span class="op">=</span><span class="dv">0</span>, n_init<span class="op">=</span><span class="st">"auto"</span>).fit(X)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>embedding_df[<span class="st">'color'</span>] <span class="op">=</span> kmeans.labels_</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>kmeans.cluster_centers_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>array([[ 2.0610277e-01,  1.6437012e-03],
       [-2.8306237e-01, -3.1798152e-04],
       [ 5.2419913e-01, -1.2679024e-04],
       [-5.6891716e-01, -5.1641576e-03],
       [ 1.1591365e+00, -1.5719092e-02],
       [-3.8863741e-02,  2.0008855e-03],
       [-1.1274347e+00, -2.8834003e-03]], dtype=float32)</code></pre>
</div>
</div>
<div class="cell" data-outputid="79299485-c9ae-41e4-a2f7-1686d622ae2b" data-execution_count="60">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(embedding_df,</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>                 x<span class="op">=</span><span class="st">'x0'</span>,</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>                 y<span class="op">=</span><span class="st">'x1'</span>,</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>                 size<span class="op">=</span>[<span class="dv">2</span>]<span class="op">*</span><span class="bu">len</span>(embedding_df),</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>                 size_max <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>                 hover_name <span class="op">=</span> <span class="st">'word'</span>,</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">'color'</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">


<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.8.3.min.js"></script>                <div id="f4af6e93-2687-41d8-81da-3b8622f7dcee" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("f4af6e93-2687-41d8-81da-3b8622f7dcee")) {                    Plotly.newPlot(                        "f4af6e93-2687-41d8-81da-3b8622f7dcee",                        [{"hovertemplate":"<b>%{hovertext}</b><br><br>x0=%{x}<br>x1=%{y}<br>size=%{marker.size}<br>color=%{marker.color}<extra></extra>","hovertext":["","[UNK]","said","trump","the","us","i","would","president","people","it","one","state","also","new","donald","states","house","government","clinton","he","obama","republican","could","told","united","in","like","white","campaign","we","two","last","time","news","election","party","first","this","a","even","former","year","country","but","years","that","many","hillary","security","media","say","political","may","national","get","make","made","since","court","law","police","american","going","republicans","presidential","percent","and","back","bill","democratic","support","administration","senate","week","know","think","including","north","vote","way","officials","russia","america","public","trumps","take","group","according","office","federal","they","called","statement","right","world","military","foreign","million","want","department","saying","washington","well","you","see","tuesday","much","says","still","tax","congress","part","there","day","another","minister","wednesday","russian","friday","if","women","work","thursday","asked","go","policy","monday","2016","democrats","need","city","war","next","china","secretary","committee","rights","deal","americans","black","official","help","three","whether","general","case","never","york","order","around","leader","show","man","on","korea","candidate","took","members","use","come","power","senator","good","countries","without","left","really","put","she","meeting","report","end","used","every","times","attack","fbi","intelligence","trade","money","top","month","justice","investigation","change","already","decision","reported","information","family","syria","groups","twitter","fact","business","days","plan","leaders","long","iran","story","several","far","conservative","nuclear","international","voters","interview","here","now","months","children","south","speech","so","however","place","something","director","as","likely","fox","call","among","clear","must","health","press","social","came","believe","agency","program","things","move","recent","major","might","chief","issue","barack","border","home","got","killed","is","immigration","control","number","least","john","islamic","though","act","reporters","school","sunday","him","matter","seen","billion","trying","went","earlier","supporters","found","actually","executive","today","post","sanders","great","yet","thing","system","spokesman","later","point","become","look","nation","real","march","win","making","set","at","away","give","keep","added","little","past","economic","free","working","legal","senior","more","for","all","let","them","january","defense","violence","what","muslim","stop","democrat","july","big","prime","ever","four","companies","nothing","attacks","member","2015","issues","not","comment","forces","european","following","company","nations","local","lawmakers","taking","held","expected","cruz","no","human","opposition","eu","head","illegal","care","to","talks","enough","person","action","june","given","across","supreme","process","known","governor","force","high","better","sanctions","gun","legislation","nominee","continue","others","when","un","possible","taken","released","source","community","financial","wall","majority","woman","night","done","judge","men","evidence","course","lot","job","history","pay","important","wrote","response","reports","life","union","anyone","team","open","second","run","close","face","attorney","10","private","wants","plans","conference","ago","20","special","budget","question","using","university","fight","mexico","ban","debate","refugees","gop","anything","1","despite","watch","air","able","comments","lives","mr","iraq","november","staff","accused","syrian","saturday","email","ryan","early","find","best","less","efforts","while","letter","someone","agreement","along","death","after","calling","race","role","instead","sure","putin","five","crisis","behind","region","full","announced","current","name","future","within","due","council","weeks","students","visit","service","hard","economy","lead","his","event","comes","civil","israel","coming","jobs","with","sources","getting","december","effort","facebook","live","congressional","running","saudi","sent","global","2014","nearly","votes","elections","authorities","allow","candidates","october","rules","talk","line","coalition","citizens","september","young","thousands","chairman","out","center","britain","8","problem","relations","comey","ruling","capital","emails","texas","ties","representatives","paul","army","wanted","needs","street","muslims","middle","hold","position","late","politics","immediately","some","leave","means","together","led","weapons","central","15","daily","everyone","claims","peace","officers","climate","rule","services","east","gave","criminal","tell","based","august","outside","began","april","showed","liberal","thought","florida","2017","policies","lost","failed","words","questions","cannot","healthcare","different","start","obamacare","bush","latest","access","2","whose","february","tried","turkey","county","do","workers","rather","up","decided","message","elected","bad","reform","immigrants","try","read","speaking","morning","spending","almost","district","threat","recently","reason","ministry","strong","millions","conservatives","organization","six","allowed","parliament","voting","received","hope","george","again","laws","everything","sexual","release","racist","idea","freedom","rally","cut","meet","makes","ahead","list","concerns","met","agencies","involved","her","calls","stand","of","kind","often","charges","germany","bring","always","happened","enforcement","planned","allegations","allies","denied","protect","poll","parties","needed","30","energy","shot","entire","nomination","fire","situation","century","side","voted","talking","key","europe","industry","market","hate","chinese","looking","shooting","provide","especially","bank","james","funding","seems","presidency","movement","host","oil","realdonaldtrump","fake","claim","large","include","vice","officer","agreed","3","representative","j","either","missile","data","spoke","although","personal","s","adding","room","near","12","old","area","small","step","hearing","arrested","west","hours","actions","hit","confirmed","shows","insurance","terrorist","me","request","water","cases","probably","feel","forward","serious","2012","fighting","return","decades","clearly","address","worked","leading","western","polls","term","true","wrong","interest","protesters","british","biggest","front","5","documents","california","potential","wife","terrorism","foundation","appeared","travel","moscow","main","tweet","crime","board","building","alleged","claimed","families","commission","result","korean","love","record","david","25","paid","declined","fired","relationship","adviser","soon","simply","review","pressure","passed","turned","nov","issued","food","continued","11","mean","started","guy","network","leadership","details","posted","dollars","saw","raised","pretty","included","attempt","previously","tillerson","spent","taxes","aid","toward","short","mike","truth","mark","signed","myanmar","college","turn","michael","brought","victory","primary","mccain","influence","bernie","father","forced","points","independence","became","seeking","final","hand","total","4","sign","deputy","religious","level","half","currently","2013","child","incident","popular","friends","longer","account","article","proposed","pass","our","rubio","push","protest","view","an","whole","sessions","hundreds","giving","agenda","21st","protests","obamas","phone","mayor","town","these","conflict","created","arabia","changes","areas","ambassador","ted","largest","independent","clintons","armed","remarks","regional","lawyer","increase","described","san","third","else","son","repeatedly","published","criticized","helped","pence","respond","heard","18","education","constitution","flynn","website","speaker","cia","speak","fund","debt","victims","living","hands","ask","firm","reality","example","troops","convention","seven","medical","programs","rate","violent","build","apparently","funds","militants","similar","secret","spokeswoman","iraqi","cost","inside","robert","goes","by","telling","carolina","24","remain","warned","criticism","absolutely","single","mass","crowd","opinion","100","understand","quickly","appears","refugee","german","employees","then","lower","photo","fear","television","risk","completely","asking","how","base","radio","experts","land","proposal","joe","form","focus","businesses","merkel","tv","politicians","individuals","research","previous","urged","are","interests","stay","happen","isis","discuss","13","served","online","events","voter","nato","france","concern","problems","its","safe","mainstream","leaving","presidentelect","northern","japan","dangerous","fellow","7","numbers","consider","king","ground","results","6","tweeted","southern","page","senators","seek","rep","died","parents","coverage","christian","share","project","16","measures","prevent","johnson","island","student","prison","why","flag","corruption","safety","down","respect","presidents","create","charged","french","development","exactly","certainly","cause","governments","church","transition","powerful","committed","society","ready","internet","drug","choice","responsible","concerned","attention","who","50","poor","were","provided","certain","gets","al","moment","answer","trip","threats","reporter","fraud","critical","considered","book","14","expressed","residents","moore","kurdish","knew","assault","responded","diplomatic","series","named","ensure","yes","mother","eight","holding","false","measure","target","knows","kelly","sides","22","mcconnell","filed","chance","terrorists","refused","organizations","because","terms","husband","threatened","brexit","approved","amendment","9","takes","hear","panel","expect","charge","schools","believed","backed","worst","beyond","activists","impact","difficult","cuts","standing","views","affairs","referendum","ordered","serve","radical","paris","favor","cities","growing","reached","dead","xi","just","class","democracy","complete","population","o","london","play","negotiations","abortion","rhetoric","low","terror","records","direct","agents","parts","god","courts","repeal","offered","massive","send","agree","operations","virginia","critics","afghanistan","newspaper","suggested","rest","protection","individual","believes","before","about","progress","sought","investment","statements","guns","ability","maybe","28","gay","car","cabinet","ways","huge","status","offer","corporate","17","strategy","behavior","exchange","screen","common","sean","27","finally","authority","2018","red","opportunity","my","defend","attacked","additional","21","lack","supporting","per","willing","higher","supported","joint","remains","killing","counsel","rohingya","dnc","continues","jerusalem","beijing","weekend","w","related","inc","body","arrest","puerto","caused","word","includes","raise","publicly","test","gas","regarding","quite","domestic","2011","michigan","credit","costs","chicago","gone","regulations","promised","crimes","during","star","mexican","cover","scandal","perhaps","significant","lose","environmental","spicer","labor","announcement","rich","income","effect","sense","avoid","period","replace","referring","reach","operation","finance","towards","block","iranian","fiscal","considering","arms","jan","directly","canada","supporter","accept","looks","2010","2008","worth","becoming","appear","particularly","multiple","challenge","worse","chris","remember","opposed","mostly","declared","arab","fair","turkish","capture","lawsuit","putting","israeli","decide","noted","kim","socalled","showing","cyber","summit","dc","buy","growth","sen","seem","approval","19","upon","thinks","join","followed","from","electoral","citing","rise","facts","daughter","oct","deep","necessary","macron","lies","kids","association","battle","lawyers","launched","shut","sept","cooperation","mind","responsibility","legislative","friend","language","stage","soldiers","subject","minority","journalists","was","vladimir","trust","reporting","green","steve","jeff","communications","facing","cuba","tough","probe","establishment","required","over","available","aimed","rival","26","sea","largely","begin","other","emergency","yemen","delegates","eastern","conspiracy","discussed","deals","banks","accusations","ended","regime","price","blame","meetings","ally","pyongyang","positions","23","infrastructure","ruled","ohio","manager","homeland","trial","various","seriously","hollywood","communities","transgender","light","did","classified","both","ran","meant","kill","revealed","professor","jr","decisions","winning","briefing","break","sex","approach","alabama","migrants","helping","bannon","reasons","goal","follow","caught","most","embassy","de","site","separate","powers","investigating","hill","couple","amid","alliance","tensions","scheduled","be","accounts","played","moving","joined","conversation","wikileaks","property","libya","appeal","bit","will","hurt","ukraine","those","technology","jones","estate","murder","judges","version","pick","tweets","throughout","prosecutors","moved","experience","acting","controversial","constitutional","racism","nbc","propaganda","too","paying","cast","appeals","islam","fully","guilty","condition","carry","stated","lie","african","resolution","values","supposed","mission","investigations","pm","damage","bureau","amount","allowing","removed","rejected","benefits","positive","shared","demand","arizona","average","pushed","game","changed","alone","coal","29","hospital","rightwing","pointed","planning","literally","sentence","scott","doubt","treasury","felt","argued","addition","40","figure","stopped","broke","targeted","cited","written","lady","jail","dropped","billionaire","present","heart","claiming","seat","bid","60","fall","hopes","borders","2009","familiar","carried","career","piece","msnbc","focused","annual","voice","rico","partner","identified","age","warning","im","angry","africa","beginning","steps","promise","born","taiwan","below","activities","resources","female","please","conditions","brown","romney","faced","closed","aides","under","reduce","picture","featured","stories","russians","mueller","highly","sarah","matters","treatment","religion","none","document","confirmation","zone","territory","particular","lying","defeat","decade","internal","campus","abc","whatever","possibly","pentagon","analysis","victim","uk","solution","investors","explain","designed","audience","lebanon","judicial","conduct","compared","ceo","biden","works","opponents","surprise","increased","disaster","carson","girl","province","t","sort","dozens","strike","require","kept","institute","happy","entering","warren","or","bringing","basis","veterans","restrictions","requests","possibility","unless","server","management","losing","judiciary","frontrunner","iowa","thats","minutes","spain","playing","islamist","hannity","bills","drew","affordable","prosecutor","highest","hacking","australia","inauguration","bloc","systems","starting","receive","dec","partners","save","militant","faces","nine","racial","negative","totally","specific","suspected","discussion","sales","numerous","illegally","humanitarian","very","scene","flint","built","airport","pushing","clean","behalf","arrived","detained","collusion","prior","ongoing","martin","allegedly","search","involvement","intended","hotel","markets","happening","vowed","campaigns","kushner","field","eventually","opened","hell","bangladesh","ad","watching","remove","reforms","prepared","loss","asia","offensive","note","jim","erdogan","sometimes","secure","dr","associated","assembly","admitted","ones","housing","wisconsin","visa","ultimately","thank","testimony","mattis","wait","miles","leftist","ben","progressive","keeping","brussels","activist","parenthood","destroy","conducted","correct","listen","bomb","thinking","increasingly","31","aide","reportedly","benefit","agent","backing","seemed","polling","palestinian","medicaid","happens","streets","greater","getty","fighters","assistance","administrations","immediate","explained","truly","prices","attempted","worried","resignation","looked","levels","jersey","tom","stance","innocent","estimated","epa","contact","cash","study","sitting","republic","raising","shown","richard","relief","production","paper","mention","fuel","communist","appointed","names","limited","evening","rates","rape","marriage","lines","door","choose","treated","pakistan","liberals","institutions","wounded","eric","diplomats","aware","training","spend","prove","path","2017realdonaldtrump","otherwise","orders","guard","extremely","alternative","detroit","check","capitol","21wire","drop","conway","citizen","christmas","blocked","abuse","vehicle","politically","payments","oh","nobody","drive","don","vietnam","tells","kremlin","gives","actual","stood","easy","boost","strongly","girls","ethnic","draft","standards","serving","seats","investigators","heads","christie","boy","broadcast","gender","attend","google","failing","confidence","park","failure","dismissed","activity","platform","neither","natural","hurricane","advance","road","potentially","off","thanks","strikes","signs","retired","blamed","attempts","andrew","yesterday","strategic","screenshot","pledged","floor","fine","providing","does","date","simple","products","civilians","camp","suspect","35","sweden","pennsylvania","catalan","asylum","wonder","involving","interior","culture","investigate","attended","assad","posts","missiles","links","lee","injured","dialogue","prominent","projects","opening","places","learned","have","grant","appearance","carrying","widely","seeing","drugs","denies","code","chair","wage","venezuela","surveillance","sheriff","session","ridiculous","faith","praised","homes","fed","successful","overseas","minimum","material","hes","figures","schumer","convicted","congressman","temporary","sell","proof","players","illinois","taxpayers","sector","promote","attacking","their","st","narrative","wealthy","station","commitment","tehran","returned","proposals","presence","improve","hour","heavily","environment","donors","chuck","veteran","space","himself","ethics","discrimination","oregon","linked","hall","recognize","range","khan","journal","graham","fell","deeply","banned","youtube","unlikely","prince","consequences","club","can","benghazi","navy","knowledge","ballot","ballistic","wearing","waiting","spread","shift","magazine","zero","marco"],"legendgroup":"","marker":{"color":[5,5,4,1,2,2,0,5,0,0,5,5,5,5,0,2,0,0,2,5,5,3,2,0,2,0,2,3,0,0,0,0,0,5,5,0,5,0,3,2,3,0,2,0,2,2,5,0,3,5,5,5,5,1,0,5,5,5,5,0,0,0,3,1,5,2,0,1,5,5,0,0,1,2,0,1,0,0,5,5,1,5,5,3,5,4,5,5,1,0,1,1,0,2,5,5,5,0,0,1,3,2,3,5,3,5,4,5,3,0,2,5,5,5,1,1,2,4,5,4,0,0,0,4,0,1,1,4,1,5,5,0,5,0,0,0,0,0,5,3,5,5,1,0,2,0,5,1,5,5,0,0,5,1,0,5,0,1,1,5,5,5,0,5,0,5,3,3,1,5,5,1,0,5,1,1,1,3,5,2,3,0,2,2,0,5,1,1,0,1,5,5,0,2,3,5,0,0,0,0,5,3,1,5,5,0,0,5,0,3,1,0,0,0,5,1,1,5,3,0,5,5,6,5,2,5,0,0,6,0,5,3,2,5,3,5,5,0,5,2,5,4,0,5,3,2,3,5,0,1,5,3,0,5,1,4,5,2,1,3,2,0,1,3,2,0,5,3,0,3,1,1,0,1,1,5,2,2,1,1,5,3,1,1,1,0,0,0,5,5,1,0,1,1,0,1,5,0,5,6,0,1,3,0,1,5,0,3,3,1,5,5,5,0,5,0,0,3,0,1,0,1,3,2,2,2,2,0,0,0,0,0,0,0,0,5,5,2,0,2,5,3,1,2,1,1,1,1,1,5,1,0,0,5,1,0,0,5,1,0,5,5,2,5,0,0,1,5,2,1,5,5,1,3,3,5,1,5,5,1,5,0,5,5,0,2,1,3,5,0,6,1,5,2,5,0,0,5,5,0,5,5,0,0,0,1,0,3,1,2,1,0,5,0,1,6,1,5,5,6,0,5,5,1,6,0,5,2,2,5,2,5,5,0,5,1,0,5,0,5,1,0,5,1,5,1,2,0,1,3,0,5,2,5,2,1,3,5,0,0,3,0,5,5,5,0,0,2,5,2,2,0,6,0,5,1,5,2,5,1,3,3,3,1,2,1,0,5,0,0,1,5,1,2,5,5,1,2,5,1,0,1,0,5,0,0,5,5,2,2,5,5,5,2,2,1,5,5,4,5,0,5,1,1,1,0,0,3,0,0,5,4,5,0,2,2,5,2,0,6,1,5,2,1,5,0,0,0,5,1,3,5,1,0,3,3,4,3,1,0,3,0,0,5,3,0,0,2,5,0,0,1,5,5,1,2,3,5,2,3,1,0,1,5,3,5,5,5,0,5,5,6,0,6,0,5,1,1,6,1,4,1,1,5,5,5,1,2,1,1,0,1,5,0,3,0,5,3,1,0,5,5,2,3,2,3,0,0,2,3,0,5,3,1,3,2,0,0,5,5,3,5,1,0,0,2,5,0,0,0,0,5,1,6,0,5,0,3,1,1,1,5,3,0,2,3,5,0,3,0,5,2,5,0,6,5,5,3,0,2,0,3,0,1,0,1,2,1,0,6,0,5,2,1,0,1,6,2,1,5,5,3,5,0,0,0,0,0,0,1,5,0,1,0,3,2,0,5,2,3,0,5,3,2,0,0,0,5,0,0,5,5,0,5,3,1,5,5,2,0,1,1,5,0,0,5,0,1,0,5,2,4,0,5,1,0,3,6,5,2,1,5,1,0,1,0,5,2,2,5,0,0,1,0,0,5,0,4,1,5,3,0,5,5,3,3,5,5,2,3,5,0,3,5,1,1,0,1,0,0,0,1,0,3,3,0,4,5,5,5,2,5,3,1,0,1,1,1,5,2,5,0,0,1,5,1,0,2,2,5,0,3,5,5,5,1,1,5,0,0,2,0,5,0,1,5,5,2,1,0,5,1,3,6,5,4,1,3,0,3,2,5,0,2,0,5,5,0,0,4,5,2,2,5,5,0,0,5,3,0,5,5,2,0,0,2,0,1,0,5,1,2,0,1,0,5,5,5,1,0,5,5,1,1,5,0,0,0,5,0,3,0,6,0,4,0,1,2,0,1,1,5,3,1,3,1,5,5,0,0,1,1,5,5,0,0,1,2,6,1,5,1,5,0,3,5,2,5,1,1,1,5,2,0,0,5,3,0,5,0,0,2,5,5,5,2,4,3,1,5,5,3,0,0,0,0,5,1,0,2,2,2,4,0,1,0,2,2,2,0,0,5,1,5,5,0,5,5,3,0,3,0,0,6,2,2,2,1,5,0,5,0,5,1,0,1,5,3,3,2,0,5,5,2,1,5,0,5,3,1,5,2,5,0,0,5,5,0,0,0,5,5,2,1,3,5,2,4,1,5,6,5,3,1,5,1,1,0,1,0,0,3,4,2,1,2,1,1,3,0,5,2,0,3,0,0,0,1,2,5,1,5,0,5,5,5,0,3,6,0,3,0,5,5,2,0,1,5,1,1,2,1,5,5,0,0,5,5,0,0,0,5,1,2,5,2,2,0,6,5,1,0,0,1,5,2,3,3,2,5,0,6,4,0,0,5,5,1,6,3,5,1,2,0,0,2,5,3,5,1,0,5,2,2,2,5,6,5,3,1,0,5,2,2,0,4,5,5,5,5,0,2,0,0,3,0,5,5,1,2,1,5,6,0,1,5,3,0,2,0,1,0,1,5,2,1,5,3,1,1,2,0,2,0,5,5,4,3,3,5,0,5,1,5,2,2,0,0,1,1,1,0,3,5,0,3,5,5,5,3,5,5,3,3,0,3,2,1,5,5,1,5,3,0,5,0,1,0,1,3,0,3,0,0,0,0,4,0,5,0,3,5,3,2,1,3,2,5,0,1,0,5,5,0,1,5,5,0,3,0,1,1,5,5,5,5,5,5,2,6,0,3,5,3,5,5,0,1,5,5,3,2,0,6,1,5,1,1,3,0,0,5,5,4,1,5,1,2,5,0,2,1,3,0,5,2,0,1,2,2,5,2,2,1,5,2,2,0,5,0,1,0,0,0,5,1,1,0,1,2,0,2,1,3,0,0,2,4,5,2,2,3,0,0,0,5,2,3,2,0,2,2,0,5,0,3,2,2,0,5,1,5,0,5,0,5,2,5,5,3,2,0,5,1,0,0,1,3,3,3,5,3,5,5,2,1,0,0,5,1,1,5,1,0,5,1,5,3,0,5,2,1,0,3,5,2,2,0,1,0,4,0,5,5,5,1,3,5,5,5,5,0,2,0,0,1,5,0,2,0,0,1,3,2,2,2,1,6,5,3,1,1,1,3,0,5,0,4,0,1,5,6,3,2,5,0,3,5,0,1,2,0,3,1,3,2,1,2,1,1,1,5,5,2,0,1,5,1,0,1,1,0,3,0,5,5,2,0,1,3,0,5,0,0,5,0,5,3,5,5,0,3,0,3,0,2,5,5,0,1,5,0,2,0,3,1,0,2,5,0,5,0,2,1,4,1,2,1,5,0,5,0,3,5,5,1,3,0,3,1,2,2,0,2,2,3,3,0,3,1,1,0,5,0,1,3,0,2,1,0,3,1,5,2,5,3,1,1,3,5,1,1,3,0,2,3,1,0,2,1,0,0,1,1,0,2,1,1,5,1,3,2,3,3,2,1,0,1,0,5,5,1,3,5,2,1,2,0,5,3,5,2,5,0,0,0,4,5,4,1,5,3,0,0,5,2,5,5,0,5,2,0,0,0,2,0,5,2,2,0,1,1,0,5,2,0,0,3,0,0,2,1,1,2,0,1,1,3,1,0,0,0,1,5,6,2,0,1,0,2,3,0,0,5,0,1,0,5,2,3,0,5,0,0,1,0,1,3,0,2,2,5,3,3,2,3,5,1,5,1,5,0,5,0,1,2,6,5,3,0,2,5,1,6,1,1,3,2,1,5,0,2,6,1,1,0,1,0,0,5,3,2,1,6,0,5,4,0,3,3,0,5,2,2,2,2,5,1,2,3,5,5,5,1,0,1,5,0,0,2,5,2,1,1,2,5,1,5,5,5,2,1,0,1,5,3,0,0,3,0,0,5,1,1,1,2,3,2,1,5,0,0,5,0,1,3,3,6,5,1,1,5,0,5,0,1,5,3,5,5,3,0,1,0,5,6,5,0,2,0,5,4,2,0,0,5,5,2,5,1,0,0,5,5,1,0,1,5,5,5,1,1,1,2,2,5,0,5,3,0,5,2,1,5,5,6,0,6,1,0,5,0,1,0,5,0,5,5,5,5,5,5,4,1,6,0,0,0,5,1,5,2,1,2,0,0,2,0,0,2,3,3,3,1,5,5,2,1,0,4,1,1,0,2,5,3,5,5,1,2,0,5,5,0,5,0,4,0,1,5,3,2,0,5,1,5,3,0,0,3,5,5,3,5,2,5,0,2,2,5,2,0,0,0,1,5,1,0,3,1,5,1,2,0,1,5,1,3,1,2,5,2,1,5,2,0,5,5,1,5,3,0,5,0,1,0,0,5,1,5],"coloraxis":"coloraxis","size":[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],"sizemode":"area","sizeref":0.02,"symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[0.021007444709539413,-0.03006669692695141,0.9809558391571045,-0.29240140318870544,0.5755254626274109,0.46790647506713867,0.27171164751052856,-0.04878242313861847,0.3110029399394989,0.18021830916404724,-0.0069916220381855965,-0.08122322708368301,0.042668476700782776,-0.12247058749198914,0.15523944795131683,0.5608217716217041,0.21012605726718903,0.13450397551059723,0.38501885533332825,-0.0679638534784317,-0.15383408963680267,-0.7442069053649902,0.5028445720672607,0.08601568639278412,0.4189518392086029,0.20785249769687653,0.7217730283737183,-0.5704789757728577,0.09740176796913147,0.19201985001564026,0.163110613822937,0.23202382028102875,0.23174689710140228,-0.14113090932369232,-0.1546383500099182,0.31489086151123047,-0.08131110668182373,0.25394153594970703,-0.4982149004936218,0.6985302567481995,-0.8101198077201843,0.358232319355011,0.38386186957359314,0.2353643774986267,0.7277406454086304,0.40014803409576416,-0.07087192684412003,0.23847977817058563,-0.6552916765213013,0.052951183170080185,0.058754850178956985,0.08077199012041092,-0.10866612195968628,-0.19715677201747894,0.11213098466396332,0.004186418838799,-0.03181685507297516,-0.10142095386981964,0.0036121648736298084,0.08738792687654495,0.1446564793586731,0.15116040408611298,-0.7421187162399292,-0.1864083856344223,-0.11286607384681702,0.6820686459541321,0.3116403818130493,-0.3886251151561737,-0.023699212819337845,-0.1079784482717514,0.3399442732334137,0.11832936853170395,-0.3238796293735504,0.38033685088157654,0.16034415364265442,-0.28836843371391296,0.14328525960445404,0.26107892394065857,-0.08345714956521988,-0.06975868344306946,-0.1729736030101776,-0.15455643832683563,-0.0292651504278183,-0.735354483127594,-0.0657370388507843,2.410391092300415,-0.06425686180591583,-0.00754222646355629,-0.19685839116573334,0.36324042081832886,-0.21449221670627594,-0.23770879209041595,0.11103598028421402,0.7162831425666809,-0.14649170637130737,-0.10602618753910065,-0.13434235751628876,0.2625653147697449,0.316720575094223,-0.1729254573583603,-0.5224820971488953,0.6705771684646606,-0.4770059585571289,-0.1254529356956482,-0.5765857100486755,0.0007538576028309762,1.088493824005127,-0.151693657040596,-0.6636121273040771,0.26773735880851746,0.465769499540329,0.07252994924783707,0.03546854853630066,-0.08044702559709549,-0.24738606810569763,-0.21647684276103973,0.5784162878990173,1.2907862663269043,-0.0924578458070755,1.0802435874938965,0.1428283452987671,0.08434690535068512,0.14873440563678741,1.2852576971054077,0.2753340005874634,-0.4040307104587555,-0.19390901923179626,0.9981157779693604,-0.23062507808208466,-0.11772260069847107,-0.006577562540769577,0.2525120973587036,0.032594069838523865,0.1933479607105255,0.3131873905658722,0.09761875122785568,0.13793949782848358,0.09934232383966446,0.04216630011796951,-0.47081103920936584,-0.1006072387099266,-0.1358364075422287,-0.17867949604988098,0.26344966888427734,0.5991075038909912,0.3596671521663666,0.00361050246283412,-0.1751551479101181,0.00442884536460042,-0.015168015845119953,0.32486552000045776,0.30712294578552246,0.03613623231649399,-0.19496984779834747,0.1576903611421585,0.0008003313560038805,0.21036826074123383,-0.30157479643821716,-0.2164444923400879,0.0142181646078825,-0.12292996793985367,0.011593042872846127,0.16306756436824799,-0.07475394755601883,0.09260949492454529,-0.044682830572128296,-0.43947938084602356,-0.46249130368232727,-0.1925531029701233,-0.03586771711707115,-0.1037478819489479,-0.3416200578212738,0.13826240599155426,-0.00835681613534689,-0.16163764894008636,-0.33297061920166016,-0.16482995450496674,-0.46260377764701843,-0.11204562336206436,0.8038428425788879,-0.48957860469818115,0.1934039294719696,0.5511507987976074,0.4421423077583313,0.10851718485355377,0.008672244846820831,-0.2850337028503418,-0.25604888796806335,0.1320352852344513,-0.22904129326343536,0.07950206845998764,0.004058314487338066,0.3020598590373993,0.7824439406394958,-0.6703659892082214,-0.042890146374702454,0.10961485654115677,0.11666368693113327,0.12152890115976334,0.1505822241306305,0.012467259541153908,-0.4680468440055847,-0.22257216274738312,-0.11507538706064224,-0.019468000158667564,0.09218599647283554,0.263714462518692,0.003930569626390934,0.1771392822265625,-0.5012398958206177,-0.2591528594493866,0.13267746567726135,0.15314285457134247,0.3211885094642639,0.04439781233668327,-0.22412560880184174,-0.28524067997932434,-0.05861729383468628,-0.5164158940315247,0.08384773135185242,-0.02228974550962448,-0.09503163397312164,-0.9229280352592468,-0.06330810487270355,0.38946273922920227,-0.06064732000231743,0.09045848995447159,0.2527426779270172,-0.8873884081840515,0.10986607521772385,-0.12586556375026703,-0.4440343379974365,0.5176569223403931,-0.0020878964569419622,-0.43482691049575806,-0.11174990236759186,-0.04172990098595619,0.16387015581130981,-0.10392984747886658,0.38380008935928345,-0.02762451581656933,0.8451718091964722,0.17159315943717957,-0.010933572426438332,-0.8030719757080078,0.8087847828865051,-0.7126936316490173,-0.008562927134335041,0.348209410905838,-0.3378118872642517,-0.05156470462679863,-0.527156412601471,0.19147889316082,-0.004974831826984882,-0.26507219672203064,1.0903559923171997,-0.034753892570734024,0.7517407536506653,-0.26337555050849915,-0.6613730788230896,0.553011417388916,0.18857042491436005,-0.18911100924015045,-0.6741605401039124,0.38960346579551697,0.11402077227830887,0.06510061770677567,-0.6696577668190002,0.17062842845916748,-0.4910707473754883,-0.33527612686157227,-0.23676201701164246,0.2791866660118103,-0.24017095565795898,-0.30167442560195923,0.04852547496557236,0.6927959322929382,0.4123288691043854,-0.36939993500709534,-0.22775967419147491,-0.11162411421537399,-0.7102958559989929,-0.30207696557044983,-0.1803629845380783,-0.20090116560459137,0.0929374247789383,0.14518994092941284,0.30920135974884033,-0.013409431092441082,-0.06910853087902069,-0.3221293091773987,0.23178298771381378,-0.16669875383377075,-0.35998377203941345,0.14133760333061218,-0.23114018142223358,-0.07556906342506409,0.21616095304489136,-0.06782680004835129,-1.5293277502059937,0.22982023656368256,-0.3353043496608734,-0.730082094669342,0.3286454975605011,-0.3892923593521118,-0.12111373990774155,0.1380382478237152,-0.6774032711982727,-0.46351179480552673,-0.3595293462276459,-0.031022705137729645,-0.10511082410812378,-0.1562965214252472,0.28224658966064453,0.03605348616838455,0.3003774583339691,0.35382258892059326,-0.5211759805679321,0.10555686801671982,-0.3223884701728821,0.10874809324741364,-0.17875559628009796,-0.6737335324287415,0.7726938128471375,0.3931216299533844,0.5229289531707764,0.5873473882675171,0.20995061099529266,0.1746087521314621,0.3209729790687561,0.33770766854286194,0.11141230911016464,0.08755112439393997,0.248877614736557,0.18759702146053314,0.03642113506793976,-0.03591649606823921,0.7109413743019104,0.24077844619750977,0.5152570009231567,-0.14496636390686035,-0.47734129428863525,-0.19371037185192108,0.594009280204773,-0.2732524275779724,-0.2905418574810028,-0.212100088596344,-0.26513752341270447,-0.23505237698554993,-0.10330521315336227,-0.34860044717788696,0.15967485308647156,0.13597047328948975,-0.1185579001903534,-0.32069918513298035,0.1861739456653595,0.08840297162532806,-0.07258263230323792,-0.30681312084198,0.08769644796848297,0.032524701207876205,-0.07161621004343033,0.42708107829093933,-0.04438191279768944,0.1401110142469406,0.1205301582813263,-0.3677758276462555,-0.1399228423833847,0.4711248576641083,-0.38138142228126526,-0.019292781129479408,-0.059814922511577606,-0.3272559344768524,-0.4301008880138397,-0.43892523646354675,-0.15258243680000305,-0.26655951142311096,-0.13236987590789795,-0.14325204491615295,-0.361166387796402,-0.07545346766710281,0.1609061062335968,-0.0692831501364708,0.04212779924273491,0.14239652454853058,0.3722479045391083,-0.35748887062072754,-0.6759044528007507,0.02482384629547596,0.24098095297813416,-1.0149297714233398,-0.3064905107021332,-0.02403787337243557,0.555810272693634,-0.028346264734864235,0.17313668131828308,0.10896417498588562,-0.04426665976643562,-0.05401306599378586,0.1953289806842804,-0.0720922127366066,0.009714113548398018,0.3528422713279724,0.20206134021282196,0.13939779996871948,-0.37923043966293335,0.165028378367424,-0.5564203262329102,-0.18007440865039825,0.4357701539993286,-0.1655794382095337,0.16292648017406464,0.0339897982776165,0.2089828997850418,-0.27877095341682434,-1.897263765335083,-0.41973677277565,-0.010425900109112263,0.07453976571559906,-1.2598261833190918,0.18815818428993225,-0.009053883142769337,-0.10726720094680786,-0.3154742419719696,-1.4963897466659546,0.15428607165813446,-0.025230953469872475,0.37533727288246155,0.4838593900203705,-0.049970414489507675,0.6480185389518738,-0.13676126301288605,-0.05354394018650055,0.33885395526885986,-0.11432011425495148,-0.22587686777114868,0.22509855031967163,-0.01983259990811348,0.2509663999080658,0.051142822951078415,-0.3226251006126404,0.2784375548362732,0.05061444640159607,-0.270346999168396,0.03413335978984833,-0.23945875465869904,0.5236978530883789,0.22053705155849457,-0.19984915852546692,-0.5081264972686768,0.19742484390735626,-0.04288956895470619,0.5206621289253235,0.06267271190881729,0.6355504989624023,-0.3206312358379364,-0.634813129901886,-0.0009577408782206476,0.1249021366238594,0.29060640931129456,-0.4600513279438019,0.25092554092407227,-0.11114413291215897,-0.004010690841823816,0.0712878555059433,0.20043684542179108,0.15719389915466309,0.46926164627075195,-0.07726527750492096,0.3761756420135498,0.4010935425758362,0.09187766909599304,-1.0803718566894531,0.29995283484458923,0.007441828027367592,-0.24317649006843567,-0.02194404974579811,0.5242787599563599,0.05588655173778534,-0.3412219285964966,-0.4602154493331909,-0.6535390615463257,-0.47005656361579895,-0.2729962170124054,0.47989439964294434,-0.35096168518066406,0.13276426494121552,-0.052225369960069656,0.10475847125053406,0.24861988425254822,-0.18130207061767578,0.04555128142237663,-0.17905353009700775,0.460361510515213,-0.13994325697422028,-0.03018096461892128,-0.4021068215370178,0.4397558271884918,0.010228754952549934,-0.23630061745643616,0.22408095002174377,-0.37281492352485657,0.08579473942518234,0.07035960257053375,0.19681932032108307,0.2671768069267273,-0.0866415873169899,0.052527014166116714,0.5986714363098145,0.6382652521133423,-0.013104893267154694,-0.02080322429537773,-0.09366196393966675,0.508207380771637,0.6824893355369568,-0.31690114736557007,-0.15775620937347412,0.037985775619745255,0.9608830809593201,-0.014127686619758606,0.28355318307876587,-0.06275065988302231,-0.22617104649543762,-0.3585430085659027,-0.18784448504447937,0.11188006401062012,0.2566177248954773,-0.44507306814193726,0.35415005683898926,0.22582875192165375,0.029403025284409523,0.892128050327301,-0.0666351243853569,0.20870475471019745,0.5217106342315674,0.44551295042037964,0.04773082584142685,0.4816078543663025,0.14555588364601135,-1.396440029144287,-0.3600020408630371,-0.14345896244049072,0.37982118129730225,-0.3832666873931885,-0.14922800660133362,0.24961014091968536,0.18279170989990234,0.1532634198665619,-0.021939072757959366,-0.33770766854286194,-0.42560094594955444,-0.048923805356025696,-0.23908469080924988,0.13739915192127228,-0.6291100382804871,-0.5624839067459106,1.112969994544983,-0.6993874907493591,-0.2343514859676361,0.35535863041877747,-0.6443555951118469,0.3257159888744354,0.12049729377031326,0.03753625229001045,-0.8101428151130676,0.20909544825553894,0.32571765780448914,0.7965506911277771,0.07958544790744781,0.09144019335508347,0.15402022004127502,-0.3863525688648224,-0.02940329909324646,-0.02996119111776352,-0.2598399221897125,0.47686779499053955,-0.606833279132843,-0.13593348860740662,0.6317113637924194,-0.7594300508499146,-0.2855090796947479,0.26202908158302307,-0.3403807282447815,-0.08162528276443481,-0.5023801922798157,-0.1544148474931717,-0.019578181207180023,-0.07754196226596832,0.2632141709327698,-0.1267908662557602,-0.03673633933067322,-0.964135468006134,0.34289392828941345,-1.0164333581924438,0.1273118406534195,-0.14987371861934662,-0.35092055797576904,-0.3501686155796051,-0.8680692315101624,-0.2731938362121582,0.844487190246582,-0.17904497683048248,-0.2057642936706543,-0.12322939187288284,-0.07691364735364914,-0.06705235689878464,-0.3606151342391968,0.719240665435791,-0.1758568435907364,-0.2743386924266815,0.1776844710111618,-0.20657382905483246,-0.02762174792587757,0.1693028211593628,-0.48255297541618347,0.14684319496154785,-0.12834013998508453,-0.6760929226875305,-0.37107881903648376,0.10666342079639435,-0.06825462728738785,-0.006202375516295433,0.3783840239048004,-0.4447222948074341,0.8135135173797607,-0.4847872257232666,0.09897115081548691,0.1035088375210762,0.42051467299461365,-0.5654946565628052,0.3172955811023712,-0.07392150908708572,-0.6322265863418579,-0.3940565288066864,-0.577370285987854,0.7416618466377258,0.3076133131980896,0.0943814069032669,-0.1530754566192627,0.006378893740475178,-0.5633913278579712,-0.12459567934274673,-0.3465389907360077,0.35200706124305725,0.24090231955051422,0.4416324198246002,-0.018491581082344055,0.09258844703435898,0.24339869618415833,0.10417412221431732,0.09583047032356262,-0.015490471385419369,-0.33082279562950134,-1.1753734350204468,0.3546728789806366,0.006662813946604729,0.2939215302467346,-0.6240206956863403,-0.18202941119670868,-0.26835086941719055,-0.18192248046398163,-0.08343327045440674,-0.46696847677230835,0.2513444423675537,0.4628234803676605,-0.5986875295639038,-0.003321791533380747,0.24077509343624115,-0.4340968728065491,0.15257100760936737,-0.10139233618974686,0.43180596828460693,-0.005461888387799263,0.1602788120508194,-1.1435534954071045,-0.017087766900658607,0.0010405415669083595,-0.6403350830078125,0.3072813153266907,0.5296283960342407,0.08653465658426285,-0.5003502368927002,0.3401641249656677,-0.379430890083313,0.12746991217136383,-0.3648360073566437,0.48707321286201477,-0.28610050678253174,0.34980785846710205,-1.561505913734436,0.0842706561088562,-0.1319160759449005,0.48418864607810974,-0.3089946508407593,0.2663559317588806,-0.3760886490345001,-1.1467900276184082,0.7460240125656128,-0.4003506302833557,-0.10819167643785477,-0.05576767027378082,-0.4399680495262146,0.01457742229104042,0.2240617573261261,0.321665495634079,0.09348351508378983,0.09895215183496475,0.308810293674469,0.15207625925540924,-0.16312390565872192,-0.04529489949345589,0.08956589549779892,-0.23430663347244263,0.09553013741970062,-0.5311702489852905,0.3930785059928894,0.15246455371379852,-0.015261180698871613,0.4433075785636902,-0.5840824246406555,0.08969637751579285,0.07179446518421173,-0.4340716302394867,0.39044007658958435,0.16426463425159454,0.25457075238227844,0.29175156354904175,-0.036753833293914795,0.14957652986049652,0.22151347994804382,0.05394645407795906,0.041487254202365875,0.17890065908432007,-0.13667838275432587,-0.5198658108711243,-0.3343714773654938,-0.036491572856903076,-0.01595328375697136,0.8242156505584717,0.12178658694028854,-0.25512608885765076,-0.28970614075660706,-0.04638891667127609,0.2473221868276596,0.31177303194999695,-0.0361505001783371,0.2312365621328354,-0.23697540163993835,0.10614825785160065,-0.11426728218793869,0.5133393406867981,0.920840859413147,0.11588750034570694,0.006538861431181431,-0.19249001145362854,0.30022993683815,-0.4920724332332611,-0.9036300182342529,-0.040589362382888794,0.37583428621292114,-0.25185099244117737,-0.10285203158855438,-0.22585362195968628,0.15958400070667267,-0.3080533444881439,0.2607947587966919,-0.06436978280544281,0.3933907747268677,0.39305034279823303,-0.0934688076376915,0.1559864580631256,0.11715726554393768,-0.3907688856124878,0.3333692252635956,0.2671292722225189,0.05697282776236534,0.1732291430234909,1.3921207189559937,-0.24365577101707458,0.02903175912797451,-0.6168479323387146,0.09838099777698517,-0.052436962723731995,-0.051094308495521545,-0.6654121279716492,-0.48067590594291687,0.05313492938876152,-0.09686880558729172,0.5461068153381348,-0.43768346309661865,0.025902947410941124,0.220485657453537,-0.7223944067955017,0.05223279073834419,-0.21246904134750366,-0.29641368985176086,0.14396071434020996,-0.2873995900154114,0.10588637739419937,0.09307006746530533,0.31740254163742065,-0.2629615366458893,0.0845380648970604,-0.726908802986145,-0.5232649445533752,0.18027310073375702,1.0239520072937012,0.03767649456858635,-0.08849482238292694,0.040027059614658356,0.5489100217819214,-0.04929792508482933,-0.49467933177948,-0.19243811070919037,0.20213790237903595,-0.4012388288974762,-0.1753845065832138,-0.348120778799057,0.07284209132194519,0.5272053480148315,-0.004676110576838255,0.3080367147922516,0.20541734993457794,-0.37061139941215515,-0.08698512613773346,-0.2721453607082367,0.1497824490070343,0.6399911046028137,0.39601001143455505,-0.10005053877830505,0.23382659256458282,-0.43844112753868103,-0.15427067875862122,-0.12242865562438965,-0.004271255806088448,-0.16527016460895538,-0.2764594256877899,-0.12047946453094482,0.08695296943187714,0.2464764267206192,0.37085089087486267,0.2409396767616272,-0.13442851603031158,0.17634885013103485,-0.18528622388839722,-0.03313343599438667,0.007852156646549702,0.43197768926620483,-0.2932432293891907,0.10118873417377472,-0.011220299638807774,-0.3630044460296631,-0.43871816992759705,-1.3894975185394287,0.08024370670318604,1.401731252670288,-0.3437139093875885,-0.46302127838134766,0.08375074714422226,-0.5939856171607971,0.5986297726631165,-0.05868498980998993,0.21504279971122742,0.44123661518096924,0.2689949870109558,-0.0377938486635685,-0.1439126580953598,0.08948306739330292,0.3324863612651825,1.1122251749038696,-0.12490776181221008,0.4072999656200409,0.4330671429634094,0.08211177587509155,-0.07351746410131454,0.09049991518259048,0.09464055299758911,0.06672970950603485,-0.47797077894210815,0.09962005168199539,-0.03448208048939705,-0.13650359213352203,0.594871997833252,0.12406671792268753,0.14252609014511108,0.46136051416397095,0.10235390067100525,-0.19258704781532288,0.09965459257364273,0.0516730397939682,-0.3086371123790741,0.4339250922203064,0.22350099682807922,-0.2494724988937378,0.15768542885780334,-0.13906461000442505,0.058264609426259995,0.034148529171943665,-0.3854244649410248,0.1216222494840622,-0.14551711082458496,-0.029817728325724602,-0.2528958022594452,-0.16265429556369781,-0.026814570650458336,0.26555800437927246,0.2774590253829956,0.17273175716400146,0.06015528365969658,0.17970287799835205,-0.5091379880905151,0.33755436539649963,-1.2223693132400513,0.16986435651779175,0.8777192831039429,0.15063810348510742,-0.4139729142189026,0.602760374546051,0.12266600877046585,-0.24126826226711273,-0.4251146912574768,0.07845436036586761,-0.6300598978996277,-0.23924703896045685,-0.5242796540260315,-0.19904355704784393,0.03666626662015915,-0.08969581127166748,0.17419083416461945,0.28211164474487305,-0.2492922842502594,-0.25272199511528015,-0.1146988719701767,0.0327143631875515,0.2565324902534485,0.26474088430404663,-0.2643362283706665,0.48179781436920166,-0.8823125958442688,-0.23876221477985382,0.07093024253845215,-0.2589026987552643,-0.12282177060842514,0.1241515651345253,-0.6102817058563232,-0.06121363118290901,0.7169379591941833,-0.03785669058561325,-0.2719224691390991,-0.16579210758209229,-0.3826376497745514,0.0014572679065167904,0.37269362807273865,0.3594725430011749,0.2891140878200531,0.008314108476042747,-0.523154616355896,0.08694609254598618,-0.06055012345314026,0.33291375637054443,0.22208577394485474,0.516776442527771,-0.1584111601114273,-0.04702595993876457,-0.09374046325683594,0.44659340381622314,1.1598149538040161,-0.5906769037246704,-0.1975306123495102,-0.0865166187286377,-0.07682820409536362,-0.8447105884552002,0.19526098668575287,0.34528636932373047,0.21325403451919556,0.29345786571502686,0.045729536563158035,-0.38683080673217773,0.13406027853488922,0.40435129404067993,0.45459115505218506,0.4628016948699951,1.9519160985946655,0.13247211277484894,-0.41590988636016846,0.30351579189300537,0.6839528679847717,0.7725982666015625,0.5753123760223389,0.17755737900733948,0.3548625111579895,0.042199112474918365,-0.22536663711071014,-0.06702776253223419,-0.030005253851413727,0.1933680772781372,-0.053949080407619476,-0.05125788599252701,-0.43606138229370117,0.1339522898197174,-0.47044646739959717,0.13404583930969238,0.11718352138996124,-1.6831774711608887,0.6565013527870178,0.37176254391670227,0.579891562461853,-0.234053373336792,-0.13020268082618713,0.3109908401966095,-0.06562680006027222,0.26290637254714966,-0.08873015642166138,-0.2961791753768921,0.3192552328109741,-0.3252995014190674,-0.11111704260110855,-0.7710191011428833,-0.6140508055686951,0.4774335026741028,0.21642817556858063,0.024417197331786156,0.07086833566427231,0.45984479784965515,-0.3146054148674011,0.07917589694261551,0.16848428547382355,0.01494851429015398,-0.48064735531806946,-0.3767903745174408,-0.07747283577919006,0.44091835618019104,0.03561225160956383,0.3189542889595032,0.2988489866256714,0.03620704635977745,-0.05209511145949364,0.2049354612827301,0.09248004853725433,0.2994697093963623,-0.06510075181722641,0.07134606689214706,0.415579229593277,-0.24915066361427307,-0.5798686146736145,-0.06445799022912979,0.48449668288230896,1.6018049716949463,-0.38299328088760376,-0.1394495815038681,-0.8889232277870178,0.027656182646751404,-0.5611011981964111,-0.2389778047800064,-0.04061316326260567,-0.2963261604309082,-0.3793991804122925,0.1503753513097763,-0.2526931166648865,0.13826727867126465,0.24774447083473206,-0.4726140797138214,0.9733542203903198,0.3881186544895172,-0.24050551652908325,0.7348852157592773,-0.3156355321407318,-0.3566182553768158,-0.48367583751678467,0.1586645245552063,0.0016421466134488583,0.48941993713378906,0.17810527980327606,-0.5251744985580444,0.3224268853664398,0.24488790333271027,0.10981396585702896,-0.19651243090629578,0.5446501970291138,-0.08887489140033722,-0.16610464453697205,-0.10430657118558884,0.352422833442688,-0.07190167903900146,0.0685584619641304,-0.09554170817136765,0.08713573962450027,-0.7387539148330688,-1.0240918397903442,0.33609095215797424,-0.4866156578063965,0.19367201626300812,0.06516198813915253,-0.12097342312335968,0.8285228610038757,0.1456674486398697,-0.24872660636901855,-0.13654117286205292,-0.2890513837337494,-0.3644436001777649,0.38634341955184937,-0.34868231415748596,0.06484916061162949,-0.04028630629181862,0.15796318650245667,0.29169762134552,0.05766469985246658,-0.08158332109451294,0.13675066828727722,0.15142251551151276,0.08798142522573471,0.025372430682182312,-0.22747009992599487,0.42160671949386597,-0.0007767823990434408,0.4490736424922943,0.4890839755535126,0.20848770439624786,-1.008773684501648,-0.01993737928569317,-0.21294187009334564,0.2810913920402527,0.1945701241493225,-0.21955233812332153,-0.06774971634149551,0.4339633882045746,-0.5990241169929504,-0.4394219219684601,0.5377093553543091,-0.09649057686328888,0.1430935263633728,-1.0939533710479736,1.0873624086380005,0.2487221211194992,0.2350352257490158,-0.10043621808290482,-0.15172402560710907,-0.3509614169597626,-0.9224905371665955,-0.6342148184776306,0.06554269045591354,-0.25378328561782837,0.6188071370124817,0.15693746507167816,0.214660182595253,0.610459566116333,-0.004322079010307789,-0.5884344577789307,0.018690815195441246,-0.20607860386371613,0.14611166715621948,0.0255619864910841,0.4225959777832031,0.3787051737308502,0.7055314779281616,-0.1421971321105957,-0.9420037865638733,-0.05802532285451889,-0.5051712393760681,-0.3593825101852417,0.09188805520534515,0.0006387805915437639,0.5508111119270325,0.651225209236145,0.3375864028930664,0.9591476321220398,0.046604499220848083,0.0661429837346077,-0.13624350726604462,-0.012719249352812767,0.2825792133808136,0.6021137237548828,0.16417448222637177,0.17569303512573242,-0.5821306705474854,0.17162898182868958,-0.013673193752765656,-0.021920692175626755,-0.28204095363616943,0.45118167996406555,-0.3131846785545349,-0.06219984218478203,-1.0297002792358398,0.3288358449935913,-0.33548393845558167,-0.06469752639532089,-0.49280184507369995,0.1085057407617569,0.5750455856323242,0.08508442342281342,-0.28273269534111023,0.2318604588508606,-0.2843937873840332,-0.03326202183961868,0.471975713968277,-0.21730297803878784,0.013634251430630684,-0.47174280881881714,-0.38406309485435486,-0.41354021430015564,0.40547671914100647,0.1094081848859787,0.5408982038497925,0.34656211733818054,0.009750881232321262,0.018561970442533493,0.9074488282203674,-0.7312719821929932,-0.4412625730037689,-0.09125078469514847,0.28234732151031494,0.029805336147546768,-0.21749508380889893,-0.15920035541057587,0.5411036014556885,0.4303538203239441,0.1658371090888977,0.19370220601558685,-0.4039669632911682,-0.2268155813217163,-0.19302436709403992,0.09290191531181335,-0.6005325317382812,0.04699469357728958,0.29441383481025696,-0.7118887901306152,-0.07602369040250778,-0.007167752832174301,0.025209594517946243,-0.45102977752685547,0.021482670679688454,-0.015740513801574707,-0.44934436678886414,-0.5700161457061768,0.13373777270317078,-0.5730730295181274,0.4671238362789154,-0.17038626968860626,-0.020134910941123962,-0.01862526684999466,-0.3535239100456238,0.04097512736916542,-0.5696738958358765,0.16793273389339447,-0.068067766726017,0.11937129497528076,-0.28577423095703125,0.24385321140289307,-0.22312846779823303,-0.4897061586380005,0.27322861552238464,-0.46397149562835693,0.1715177744626999,0.22216995060443878,0.3144155740737915,0.30861008167266846,0.8754628896713257,0.27417483925819397,-0.08690562099218369,0.3492957353591919,-0.5272551774978638,0.031333696097135544,-0.4403478801250458,0.555389404296875,-0.37633928656578064,-0.5328746438026428,0.669884204864502,-0.11575858294963837,0.273070752620697,-0.24470128118991852,0.13809870183467865,0.011166267096996307,-0.04337228089570999,0.12148261070251465,-0.19857163727283478,0.002680659294128418,-0.008038644678890705,0.26982977986335754,-0.7885693907737732,0.264666885137558,-0.36430656909942627,-0.33068278431892395,0.0007387388031929731,-0.03277778625488281,-0.08850707113742828,0.027017291635274887,-0.028394101187586784,-0.054932668805122375,0.459268182516098,-0.9209370017051697,0.2607601284980774,-0.7434160709381104,0.026695162057876587,-0.4918067753314972,-0.14863988757133484,0.03340242803096771,0.2164708822965622,-0.24856022000312805,-0.07677232474088669,0.06641794741153717,-0.7351370453834534,0.5023297667503357,0.1311296820640564,-1.2176458835601807,-0.41515496373176575,-0.02684539183974266,-0.21133820712566376,-0.2477450668811798,-0.6099242568016052,0.15750987827777863,0.2736491858959198,-0.11366414278745651,0.05275836959481239,0.8665510416030884,-0.18455097079277039,-0.0691937729716301,-0.37705373764038086,0.6426556706428528,-0.12005637586116791,0.2586863338947296,0.7114423513412476,-0.3668094277381897,-0.44154229760169983,0.3468894064426422,0.07904858887195587,0.5998355150222778,0.14842337369918823,-0.17446677386760712,0.8080821633338928,0.47152164578437805,-0.1252167671918869,0.6661825776100159,0.3703402876853943,-0.26454809308052063,-0.08666369318962097,0.6450991630554199,0.43404480814933777,0.09753507375717163,0.05587224289774895,0.31352150440216064,-0.38739335536956787,0.2945161759853363,0.22239325940608978,0.3196184039115906,0.04666004329919815,-0.4228073060512543,-0.3655443489551544,0.25863510370254517,-0.2253737896680832,0.5350134968757629,0.33735549449920654,0.4474996030330658,-0.37906602025032043,-0.4816958010196686,0.17252857983112335,0.14087341725826263,0.4767926335334778,0.9999642372131348,-0.07483059912919998,0.5058466196060181,0.3909067213535309,-0.48862460255622864,0.12453034520149231,0.1678105592727661,0.25702884793281555,-0.15022073686122894,0.5467509627342224,-0.5260016918182373,0.3825344443321228,0.3478829562664032,0.40676191449165344,0.4607829749584198,0.15496885776519775,-0.09367023408412933,0.15294604003429413,-0.49526140093803406,0.6994329690933228,0.4338865280151367,0.20354685187339783,-0.1277383416891098,-0.35821449756622314,0.013136927038431168,0.24033227562904358,-0.045159850269556046,0.32740461826324463,-0.0025134929455816746,0.4742650091648102,-0.12996408343315125,-0.0870533362030983,-0.5602059960365295,0.5258856415748596,0.27127331495285034,-0.0037619932554662228,-0.29780736565589905,0.16688300669193268,0.3433372378349304,-0.19050878286361694,-0.4828436076641083,-0.5469123125076294,-0.8148621916770935,-0.06630876660346985,-0.5878077149391174,0.055402256548404694,0.041859742254018784,0.8338590264320374,-0.35435137152671814,0.09323745965957642,0.15274621546268463,-0.017314402386546135,-0.2691655457019806,-0.1835954189300537,0.037011340260505676,-0.21779830753803253,0.11574476212263107,-0.035661377012729645,-0.25962138175964355,0.010994387790560722,-0.48938971757888794,0.20592887699604034,-0.03896508365869522,0.41348713636398315,-0.34100207686424255,0.28333303332328796,-0.7650050520896912,0.03719977289438248,0.6177325248718262,0.6357378959655762,0.2940409779548645,-0.23155388236045837,0.13762612640857697,0.9189994931221008,0.22068899869918823,-0.1497621387243271,-0.11693133413791656,-0.128504678606987,-0.2832580506801605,-0.4377029240131378,0.007706665433943272,-0.03720635920763016,0.05863788723945618,-0.10013970732688904,0.10722937434911728,0.5651994347572327,0.3003714680671692,0.2670445442199707,-0.30633142590522766,-0.002381585305556655,0.235203355550766,0.6377978920936584,0.12060236185789108,0.34626612067222595,-0.22475217282772064,-0.5660041570663452,0.40820562839508057,0.36865055561065674,0.3904581069946289,-0.2282734215259552,-0.9318536520004272,-0.02581384778022766,-0.5562581419944763,-0.3858069181442261,-0.40247461199760437,-0.2343910038471222,-0.6694322824478149,0.3125889003276825,-0.06513947248458862,0.34781932830810547,0.8762514591217041,0.30577266216278076,-0.2210778146982193,-0.12454371899366379,-0.9100920557975769,-0.49322840571403503,0.7181164026260376,0.05701305344700813,0.21253593266010284,-0.7315503358840942,-0.045434433966875076,0.19116339087486267,-0.4040771722793579,0.5411794185638428,0.10560834407806396,-0.4317127466201782,-0.4140241742134094,-0.6641684174537659,0.5008248686790466,-0.3865107595920563,0.39460092782974243,-0.2765161097049713,-0.2627737820148468,-0.3538264334201813,-0.07853245735168457,-0.14125299453735352,0.38505619764328003,0.15755628049373627,-0.406017005443573,0.04555484279990196,-0.3035001754760742,0.34717223048210144,-0.3050439953804016,-0.38808533549308777,0.26260778307914734,-0.729866623878479,0.13192123174667358,0.00826906319707632,0.015297548845410347,0.46334701776504517,0.2666538953781128,-0.23130515217781067,-0.5067695379257202,0.08614401519298553,-0.0029002593364566565,0.18119749426841736,0.18961015343666077,-0.12634599208831787,0.27357858419418335,-0.04330245777964592,-0.459316223859787,-0.157211035490036,-0.025811778381466866,0.236271470785141,-0.6006852984428406,0.3619177043437958,-0.5114741921424866,0.09319138526916504,0.3864457309246063,-0.0255416389554739,0.033765409141778946,0.23667743802070618,-0.2526072859764099,0.0006357419188134372,0.28006666898727417,0.48706531524658203,0.1434965282678604,-0.5585665702819824,-0.17122779786586761,0.22745321691036224,0.4008226990699768,-0.11756619065999985,0.15370506048202515,0.034993719309568405,0.14930790662765503,0.7117738723754883,-0.30294111371040344,2.0189459323883057,-0.42595839500427246,0.6104440689086914,-0.19975566864013672,0.03961888700723648,0.18821687996387482,-0.14431947469711304,0.18851839005947113,-0.6404515504837036,-0.07033513486385345,-0.057811252772808075,-0.20373176038265228,-0.8080421090126038,0.19093281030654907,-0.639116644859314,-0.31591537594795227,0.37226879596710205,0.42765775322914124,0.27337491512298584,0.4238716661930084,0.4555235505104065,-0.5705620050430298,-0.7186359167098999,0.14890488982200623,-0.4298304319381714,-0.3261103630065918,-0.23694251477718353,0.23389899730682373,-0.001609153812751174,0.3163100779056549,-0.20584842562675476,-0.6881484985351562,0.23085950314998627,0.46551862359046936,-0.242882639169693,0.17692211270332336,-0.6511085629463196,-0.24519281089305878,-0.0007209003088064492,0.5648451447486877,-0.1290738880634308,-0.48845791816711426,-0.2590544521808624,-0.36973243951797485,-0.5963693857192993,-0.15350432693958282,-0.3950954079627991,-0.3251241445541382,-0.6337018609046936,0.11165807396173477,0.40678197145462036,-0.43289539217948914,-0.3772581219673157,0.2742496728897095,0.6081014275550842,-0.2723451554775238,0.2889777421951294,0.19854846596717834,-0.3317859172821045,-0.1611635535955429,0.2710299491882324,0.600959062576294,-0.29959070682525635,-0.23940889537334442,0.03057560697197914,-0.22329755127429962,-0.5387405157089233,0.40769100189208984,-0.835466742515564,-0.5147595405578613,0.6735711693763733,-0.2867950201034546,0.25574684143066406,-0.1872439980506897,0.09215792268514633,0.016811572015285492,0.03838379681110382,-0.22185178101062775,-0.5880140066146851,-0.1280813068151474,0.465305358171463,-0.2197427898645401,0.49643129110336304,0.0934351459145546,-0.020443474873900414,-0.4647606313228607,-0.04107048362493515,0.4235337972640991,-0.010905684903264046,0.12305686622858047,0.298408567905426,0.28867217898368835,1.951491117477417,1.9261084162280895e-05,1.104135513305664,-0.25294384360313416,0.056762874126434326,-0.5263929963111877,0.22294844686985016,0.35812047123908997,-0.15942063927650452,0.4772226810455322,-0.0258274395018816,-0.1505291610956192,0.33094725012779236,0.00541171757504344,0.6113002896308899,0.3228128254413605,0.3195772171020508,0.12791596353054047,0.41247960925102234,0.21878650784492493,-0.1598261296749115,0.5661444664001465,0.6493648290634155,0.10836391896009445,-0.3740347623825073,-0.18612314760684967,0.19927337765693665,-0.05928392335772514,0.5249261260032654,0.15660154819488525,0.12215102463960648,-0.5389139652252197,0.20660582184791565,0.26974862813949585,0.6815354228019714,-0.32430702447891235,-0.3027915060520172,0.52583247423172,0.3124130070209503,-0.16581156849861145,-0.23241518437862396,-0.45642396807670593,-0.2947249114513397,0.14121724665164948,0.2765769958496094,0.11592856794595718,-0.34667858481407166,-0.025402791798114777,-1.061123251914978,0.6088377237319946,0.3484480082988739,-0.21002084016799927,0.12269966304302216,0.535415530204773,-0.5288679599761963,0.33312317728996277,0.21652129292488098,-0.12360421568155289,0.1012963354587555,-0.2557525336742401,0.23359641432762146,-0.09964672476053238,0.7377352118492126,-0.5847603678703308,0.193622887134552,-0.13610070943832397,0.21324793994426727,0.12641946971416473,-0.4086591601371765,0.3283618986606598,-0.23511625826358795,-0.6434227228164673,0.3319564163684845,0.7434538006782532,0.6256142854690552,0.07557763904333115,-0.5013753175735474,-0.569790780544281,0.36880844831466675,-0.7124069333076477,0.007690855767577887,-0.2664696276187897,-0.010935445316135883,-0.3652128279209137,0.05942241847515106,0.09551748633384705,-0.09406640380620956,0.24566297233104706,-0.3288968503475189,0.5685005187988281,-0.8811085820198059,0.038049839437007904,-0.48463520407676697,0.20764657855033875,0.458499014377594,-0.11955469846725464,-0.3028988540172577,-0.908507227897644,-0.23281139135360718,-0.32243454456329346,-0.6702898144721985,0.6987854838371277,-0.41539475321769714,0.056796520948410034,0.16386017203330994,0.5594576001167297,-1.435229778289795,-0.3806264102458954,-0.26042768359184265,0.32349738478660583,-0.17100252211093903,0.3330017626285553,0.15325947105884552,-0.05659125745296478,-0.461517333984375,0.47579216957092285,-0.3171752691268921,-1.3177317380905151,0.13752390444278717,-0.026215147227048874,1.120640754699707,0.3202109932899475,-0.6274091601371765,-0.4441697597503662,0.2693052291870117,-0.09831079840660095,0.5105209946632385,0.4176208972930908,0.38498684763908386,0.40710851550102234,-0.021826915442943573,-0.394511342048645,0.5032864212989807,-0.6395226120948792,0.06639236211776733,-0.1365460306406021,-0.13115781545639038,-0.4161880612373352,0.25432610511779785,-0.2164650410413742,0.0018261780496686697,0.1683904230594635,0.22901126742362976,0.6920161843299866,0.007570188958197832,0.40191856026649475,-0.24714568257331848,-0.29622602462768555,0.4209401309490204,0.01843235269188881,-0.2115645855665207,0.04677953943610191,-0.032566312700510025,-0.05806386470794678,0.5042049884796143,-0.16737765073776245,0.28721171617507935,-0.3340376019477844,-0.10440628230571747,-0.6370776891708374,0.22372765839099884,0.19721251726150513,-0.5621208548545837,0.20033228397369385,0.10203777253627777,-0.05730856955051422,-0.37520551681518555,-0.1929437220096588,-0.27237269282341003,0.39420753717422485,-0.5456698536872864,0.7130975127220154,-0.21775512397289276,-0.0019233138300478458,0.14852046966552734,0.11307211965322495,-0.035215023905038834,0.24527473747730255,-0.40203019976615906,-0.5536983609199524,-0.442412793636322,-1.101335883140564,-0.04954243078827858,-0.16843779385089874,-0.37853872776031494,-0.15320292115211487,0.08849978446960449,0.0638323575258255,0.12500393390655518,-0.294931560754776,0.07704257220029831,-0.696758508682251,-0.010892625898122787,-0.005358003545552492,-0.6432323455810547,0.216934934258461,-0.1646495908498764,0.3553173840045929,-0.08764832466840744,-0.9896183013916016,-0.12142079323530197,0.1505311131477356,0.4174536466598511,0.11315195262432098,-0.15129458904266357,0.9513666033744812,0.504997730255127,0.35430222749710083,0.13653168082237244,-0.0012015777174383402,0.006440069526433945,0.39574745297431946,-0.04398626089096069,-0.24399025738239288,0.3519649803638458,0.23812754452228546,0.08137334138154984,-0.12579156458377838,-0.3307342827320099,0.3071196675300598,-0.21990588307380676,-0.0774838849902153,0.06453276425600052,0.05923690274357796,-0.21180640161037445,-0.24284937977790833,-0.24003911018371582,0.7936163544654846,0.3699028193950653,0.04637070745229721,0.0924244225025177,-0.019714249297976494,-0.6471735835075378,0.1018470823764801,-0.0389830656349659,0.4403822720050812,-0.329502671957016,0.06835206598043442,-0.13493631780147552,-0.9997585415840149,0.2238815724849701,-1.1165798902511597,-0.19542773067951202,0.15528027713298798,0.02466551959514618,0.12067259848117828,-0.41502177715301514,0.16761089861392975,-0.13637767732143402,0.18978190422058105,0.06889474391937256,-0.09701427817344666,-0.10614843666553497,-0.1155368760228157,-0.027086997404694557,-0.06177699193358421,1.040170669555664,-0.2522314488887787,-1.2090898752212524,0.27607306838035583,0.29466137290000916,0.29335376620292664,-0.016487743705511093,-0.2548600435256958,-0.10648176074028015,0.5846608877182007,-0.2622649371623993,0.4401110112667084,0.23608791828155518,0.18959887325763702,0.6216586828231812,0.23752634227275848,0.2607840299606323,0.3923254609107971,-0.44641202688217163,-0.6670709252357483,-0.48763930797576904,-0.42049044370651245,-0.07468496263027191,0.07069873809814453,0.6037235856056213,-0.25919461250305176,0.24635657668113708,0.8455485701560974,-0.35445937514305115,-0.21966256201267242,0.25168827176094055,0.7328657507896423,-0.003386611817404628,-0.6197729110717773,-0.057791586965322495,-0.05463583394885063,-0.41494524478912354,0.6763990521430969,0.31787919998168945,-0.0625656321644783,-0.04420657828450203,0.22822417318820953,-0.0616091825067997,0.08552394807338715,1.3871188163757324,0.3295060098171234,-0.1980571448802948,-0.06460649520158768,-0.580908477306366,0.5545845627784729,0.23631413280963898,-0.1348670870065689,-0.36611855030059814,-0.027530549094080925,-0.6245937347412109,0.14840714633464813,0.24629849195480347,-0.6144421100616455,-0.03577214479446411,0.075717993080616,-0.539550244808197,-0.030677301809191704,0.4032718539237976,0.06501734256744385,0.2583636939525604,0.38220667839050293,0.6537428498268127,0.04306252300739288,0.42612311244010925,0.2921123802661896,0.10649338364601135,0.0887971818447113,-0.33658134937286377,-0.1227506771683693,-0.22486262023448944,0.09775587171316147,-0.5906406044960022,-0.16654063761234283,0.013860099017620087,-0.4073864221572876,0.38498058915138245,0.26885154843330383,-0.25063541531562805,0.018534773960709572,-0.4153194725513458,-0.44055360555648804,-0.16968056559562683,0.43813684582710266,-0.06517830491065979,0.432466596364975,-0.33311349153518677,-0.09483899176120758,0.3864099383354187,0.1476050317287445,-0.10990002006292343,-0.009632203727960587,-0.17263321578502655,-0.11839572340250015,-0.5217944383621216,0.12191913276910782,-0.1408967822790146,0.27631551027297974,-0.1884276270866394,0.12167180329561234,0.23433060944080353,-0.02662847377359867,-0.3548419177532196,0.0673447847366333],"xaxis":"x","y":[-0.00020846941333729774,-0.002437826944515109,0.005132079124450684,0.04389399290084839,0.005025553051382303,-0.03243511915206909,-0.021834103390574455,0.01724853366613388,-0.006388693582266569,-0.011249268427491188,0.025823866948485374,-0.02708706073462963,0.037737008184194565,0.03039371408522129,0.004002529662102461,-0.025539573282003403,-0.000606840243563056,-0.007340901996940374,0.03154383972287178,-0.009596077725291252,0.04526764899492264,-0.04283330589532852,0.06745856255292892,0.0045895203948020935,0.015958111733198166,-0.02778596244752407,0.004471430554986,0.016450200229883194,0.014138010330498219,0.019626688212156296,0.02219952456653118,0.022022685036063194,0.03619932010769844,0.021389411762356758,-0.023650331422686577,-0.004596169572323561,0.03643707558512688,0.003970797639340162,-0.01296458300203085,-0.001978909829631448,-0.029259804636240005,-0.028648026287555695,0.007506685797125101,0.007808549329638481,0.06487397849559784,0.06742557883262634,-0.0486704483628273,-0.037374097853899,-0.012397252023220062,0.02940414659678936,-0.041335463523864746,0.06087096408009529,0.05900577828288078,-0.0008127879118546844,0.013366798870265484,0.005408725701272488,-0.00605031568557024,-0.0276802871376276,0.019212743267416954,-0.0039076185785233974,0.05002956464886665,0.008429644629359245,-0.06583058834075928,0.024473149329423904,-0.042584411799907684,0.04693964868783951,0.03437311202287674,-0.04195144772529602,-0.03910108655691147,0.014437898993492126,0.023316828534007072,-0.006138830911368132,0.021229606121778488,0.013301653787493706,-0.021936532109975815,-0.005901457276195288,-0.02247626893222332,0.023228321224451065,0.03641610965132713,0.06153613701462746,-0.03175295144319534,0.020939575508236885,-0.02175925485789776,-0.07067353278398514,0.07165612280368805,-0.055065255612134933,-0.0035797262098640203,-0.03183867409825325,0.011937959119677544,0.04564294219017029,0.05516229569911957,0.030050843954086304,0.06170311197638512,0.0074189770966768265,0.02426287718117237,-0.035869717597961426,-0.015094427391886711,0.034958820790052414,0.025378147140145302,-0.03474177420139313,-0.004011634737253189,-0.013779466971755028,-0.023600120097398758,-0.10481510311365128,0.03863230720162392,-0.03210970759391785,-0.006382815074175596,0.001692509395070374,0.021525545045733452,-0.0010742265731096268,0.012654151767492294,0.054350197315216064,-0.01485631987452507,-0.02550474926829338,-0.03261958062648773,0.047274183481931686,-0.01964254304766655,-0.03304722532629967,-0.0009672247688286006,-0.08954223990440369,-0.006839773152023554,-3.659708090708591e-05,-0.029335280880331993,0.0015573098789900541,-0.00825768057256937,-0.0006663902313448489,0.05277421325445175,-0.013865054585039616,0.013654309324920177,-0.062342409044504166,-0.0407368503510952,0.030509768053889275,0.031193144619464874,0.01659686118364334,-0.010710558854043484,0.10055265575647354,0.008255952969193459,0.02033049426972866,0.003364743199199438,0.011676582507789135,0.05652753636240959,0.026827944442629814,0.02146810106933117,-0.03887007385492325,0.02052784338593483,-0.02618575096130371,-0.08800417929887772,-0.019986461848020554,0.03777174651622772,-0.020520752295851707,0.07311460375785828,0.060852374881505966,0.02684270404279232,-0.008155863732099533,6.212061998667195e-05,-0.00429531279951334,0.05379125103354454,0.06584476679563522,-0.07421326637268066,-0.041903071105480194,-0.0019045694498345256,0.052127838134765625,0.0033580921590328217,-0.006082842592149973,0.05382801964879036,0.02257932350039482,-0.032786671072244644,-0.022941458970308304,-0.004282334353774786,0.0002777528716251254,0.06215449795126915,-0.0025132782757282257,-0.04385719075798988,-0.02565077878534794,-0.06514294445514679,0.004864913411438465,-0.03233613073825836,0.02181592769920826,-0.03178168088197708,0.009095804765820503,0.06849923729896545,0.013469554483890533,0.11079252511262894,-0.0423307903110981,0.007466564886271954,0.015130661427974701,0.006836793385446072,0.050010353326797485,0.04423195496201515,-0.030062386766076088,-0.008607297204434872,-0.011222973465919495,-0.024677634239196777,-0.05436442047357559,-0.004296626430004835,-0.01565164513885975,-0.0012103107292205095,0.005306597333401442,-0.0032725974451750517,-0.04171490669250488,0.046270620077848434,-0.07505803555250168,0.03520729020237923,0.0339922197163105,0.027739403769373894,0.022250913083553314,-0.027413683012127876,-0.023216193541884422,0.0717860609292984,-0.0064218128100037575,-0.03091299720108509,-0.0038906133268028498,-0.026101989671587944,-0.013357022777199745,0.004339882172644138,-0.07026635855436325,0.02828177809715271,-0.0394832007586956,-0.033768054097890854,-0.027228808030486107,-0.025416230782866478,0.122760109603405,-0.1252889335155487,-0.03826838731765747,0.0193508081138134,0.04251031205058098,-0.03220857307314873,-0.011169495061039925,0.08272533118724823,-0.014122674241662025,-0.06345172971487045,-0.04230028763413429,0.021028388291597366,0.02766086906194687,0.03326951339840889,-0.010904688388109207,-0.03175816684961319,0.0017416509799659252,-0.00020027928985655308,-0.0012537118745967746,0.0269487202167511,-0.006684823893010616,-0.0021924858447164297,0.03389032185077667,0.0467885360121727,0.04973338544368744,0.03422663360834122,0.03199697285890579,0.016702810302376747,0.009427090175449848,-0.00849371962249279,-0.003976225852966309,0.0003174222947563976,-0.014923312701284885,0.00792301818728447,0.030841389670968056,-0.021315565332770348,0.043646231293678284,0.057550814002752304,-0.07895347476005554,0.02510949783027172,0.014873117208480835,-0.04000551253557205,0.027161147445440292,0.010492188856005669,0.01951310597360134,0.008702052757143974,-0.05599529296159744,0.0427311509847641,-0.10718175768852234,0.0001074199826689437,0.00819644145667553,0.023602774366736412,-0.08942034840583801,-0.0061900862492620945,0.05652781203389168,0.007558307144790888,0.07987518608570099,-0.001169343013316393,0.029830539599061012,0.07876162230968475,-0.04728800058364868,0.017062557861208916,0.032248739153146744,0.004209349397569895,0.02299502119421959,-0.11133036017417908,0.019689282402396202,0.045563243329524994,-0.055572185665369034,-0.032672058790922165,-0.0027164865750819445,-0.06573545187711716,0.007401700131595135,0.030823400244116783,-0.041859861463308334,0.0903133749961853,-0.04166104272007942,-0.02188325859606266,0.017904670909047127,-0.0329139418900013,-0.024480318650603294,0.02441379427909851,-0.020839938893914223,0.04484405368566513,0.010918905958533287,-0.09074428677558899,0.010399692691862583,-0.015048141591250896,-0.05442080646753311,-0.07761308550834656,0.027891134843230247,0.013636707328259945,0.04735395312309265,0.00714261457324028,-0.062234483659267426,0.08108669519424438,-0.07623741775751114,0.08565489202737808,0.022020872682332993,-0.03103022277355194,0.005002453923225403,-0.03517767786979675,-0.011947725899517536,-0.026970138773322105,0.0020193022210150957,-0.11142964661121368,-0.021859370172023773,0.0386485829949379,-0.10318822413682938,0.047692909836769104,0.033008016645908356,0.07735925912857056,0.0323723703622818,-0.016599440947175026,-0.041524823755025864,-0.012068333104252815,-0.01665886677801609,0.012168233282864094,0.050166770815849304,-0.03148210793733597,0.010924894362688065,-0.024796785786747932,-0.004300926811993122,-0.07157883048057556,0.005926637910306454,0.0497765988111496,0.022436922416090965,0.0181876327842474,-0.01347003411501646,0.03798079490661621,-0.033944323658943176,0.05602047219872475,0.05614471063017845,0.01924874074757099,-0.07808483392000198,-0.016603592783212662,0.010556637309491634,-0.0377228669822216,-0.027784671634435654,0.04165850952267647,0.013038392178714275,0.006000102963298559,0.057669028639793396,0.02560347132384777,-0.031331222504377365,-0.031542662531137466,-0.027317125350236893,-0.02966357208788395,-0.05186306685209274,0.01211341843008995,0.045863911509513855,0.03279270604252815,0.06927710026502609,0.035534076392650604,-0.00571417948231101,-0.020129164680838585,-0.006545009557157755,0.02701021544635296,0.010487448424100876,0.011291969567537308,0.07103001326322556,0.012705839239060879,-0.02180493250489235,-0.015928003937005997,-0.09131446480751038,-0.004869495984166861,0.03699628263711929,-0.028448129072785378,0.01762663759291172,0.05124959722161293,0.011493753641843796,-0.015739867463707924,-0.02418801188468933,0.013605508022010326,0.03019019216299057,-0.019827701151371002,-0.0523080937564373,-0.016246331855654716,0.03153962269425392,0.009450037032365799,0.029752502217888832,0.02391880378127098,0.033568065613508224,-0.010621101595461369,-0.001825805869884789,0.06129341199994087,-0.044048380106687546,-0.06678462028503418,0.03054407797753811,-0.09158165007829666,0.09246929734945297,-0.05875560641288757,0.02417968027293682,-0.006328111048787832,0.11706653982400894,0.004422682803124189,-0.05261868238449097,-0.03547890856862068,0.014126196503639221,0.057939060032367706,0.0060532838106155396,0.07040153443813324,0.024943822994828224,0.05528935790061951,-0.061141520738601685,0.041342802345752716,-0.03845830261707306,0.04619111865758896,0.007054728921502829,0.0006210471619851887,0.0010353209218010306,-0.01573105901479721,0.056040115654468536,0.055352333933115005,-0.012241736985743046,-0.0031274312641471624,-0.02857210487127304,-0.038529105484485626,-0.005020886659622192,0.043661922216415405,-0.03336476534605026,0.01591820828616619,-0.026610173285007477,-0.0039100381545722485,-0.03579198569059372,0.04142816364765167,-0.012800521217286587,-0.057099658995866776,-0.05815955996513367,-0.016168592497706413,0.029245536774396896,0.04393221065402031,-0.06733818352222443,-0.05488734319806099,0.013093524612486362,0.03481153026223183,0.004614152945578098,0.013275985606014729,0.07556253671646118,-0.05908571556210518,0.08502618223428726,0.045770205557346344,0.05331428349018097,-0.028023026883602142,-0.004869573749601841,-0.009611010551452637,0.009154011495411396,0.021884510293602943,-0.07836796343326569,0.002389570465311408,-0.0034447757061570883,-0.024144478142261505,-0.006159024313092232,0.004213033709675074,-0.06340819597244263,0.03654705733060837,0.02150685340166092,-0.03712783381342888,0.010517190210521221,-0.02469111606478691,-0.0016966242110356688,0.05409249663352966,0.015693793073296547,0.017209066078066826,0.04387745261192322,0.08126803487539291,0.03145858645439148,0.042982373386621475,-0.012139450758695602,0.022006738930940628,0.0023564426228404045,-0.0016172713367268443,-0.03155761957168579,-0.03651396557688713,0.09223949164152145,-0.057589612901210785,0.001897555310279131,-0.028034117072820663,0.039824049919843674,0.0353076308965683,-0.026571067050099373,0.009199319407343864,-0.02385145239531994,0.09752305597066879,-0.077543705701828,0.07637692987918854,0.03421667218208313,0.04792913794517517,-0.00938116479665041,-0.035070858895778656,-0.007278243079781532,0.0008625983609817922,0.01876482367515564,-0.025403648614883423,-0.08150729537010193,-0.021010402590036392,0.09008553624153137,-0.021533401682972908,0.012326747179031372,-0.052069827914237976,-0.014865787699818611,0.011805692687630653,0.017141617834568024,-0.007587617728859186,-0.05014503002166748,0.002787154633551836,0.04337165504693985,-0.038960449397563934,0.05509105324745178,0.03210591524839401,-0.01324428804218769,-0.09806076437234879,-0.06735722720623016,0.007973483763635159,-0.003226165659725666,-0.04214846342802048,-0.0514107383787632,-0.03371710330247879,0.05170133337378502,0.058582913130521774,-0.09024445712566376,-0.0021342248655855656,-0.07678163051605225,-0.021574947983026505,-0.014547771774232388,0.021568981930613518,-0.01588047482073307,-0.06098764389753342,0.015910515561699867,-0.0272111464291811,-0.0037237636279314756,0.004616554360836744,-0.06602541357278824,0.06421038508415222,-0.0018156885635107756,0.014582287520170212,-0.12663081288337708,0.028868574649095535,0.049492448568344116,0.05837370455265045,-0.00581080699339509,0.04948940500617027,-0.03500483185052872,0.013624269515275955,0.0506562739610672,-0.061978910118341446,-0.035244785249233246,0.12339469790458679,0.02068992517888546,0.10444989055395126,-0.039974793791770935,0.05983741581439972,0.0046197278425097466,-0.014941668137907982,0.04109878093004227,-0.0593685619533062,-0.024368254467844963,0.024874264374375343,0.024484895169734955,-0.0077703907154500484,-0.011529398150742054,0.03576009348034859,1.1048003216274083e-05,-0.007138602435588837,0.02572920173406601,0.035655152052640915,0.03725481405854225,0.01731407642364502,0.007788106799125671,-0.0035831723362207413,-0.04842148348689079,-0.0181137528270483,0.05725283920764923,0.048212386667728424,-0.0030442909337580204,0.01597798988223076,-0.00987126026302576,-0.09102535992860794,-0.050566378980875015,-0.010597352869808674,0.10242589563131332,0.013004299253225327,-0.02605186216533184,-0.00297928717918694,0.05097606033086777,-0.0451570488512516,0.04412241280078888,0.05005921423435211,-0.04400891065597534,0.026952320709824562,-0.0465860515832901,-0.07605613023042679,-0.08430631458759308,-0.0020345000084489584,-0.016902271658182144,0.0047913603484630585,0.03393545001745224,0.00957897212356329,0.0040004681795835495,0.056961238384246826,0.0598163940012455,0.03004603087902069,-0.003607101971283555,0.05200528725981712,-0.06786121428012848,0.000949226669035852,0.06024627387523651,-0.09657955169677734,0.05977959930896759,-0.02656904235482216,-0.03329090401530266,0.011131292209029198,0.006294907070696354,-0.018612472340464592,0.045546937733888626,-0.011992238461971283,0.06958137452602386,-0.033678729087114334,0.042116839438676834,0.0014273134293034673,-0.06911369413137436,0.029004395008087158,0.03845172002911568,0.05989694222807884,0.11798525601625443,-0.061903055757284164,-0.06476551294326782,0.0014293949352577329,0.04305405542254448,0.027797430753707886,0.05112693831324577,-0.026673879474401474,0.028697406873106956,0.04501882940530777,0.08872822672128677,0.036279212683439255,0.02700170874595642,0.018076086416840553,0.005392947234213352,-0.026544591411948204,0.035281646996736526,0.03699711710214615,-0.039152149111032486,0.0016842707991600037,0.030981138348579407,0.08483633399009705,-0.06712708622217178,-0.035803984850645065,-0.012794173322618008,-0.005070654209703207,-0.08242454379796982,0.06464237719774246,-0.049589503556489944,0.028662722557783127,-0.0206484105437994,-0.051961906254291534,-0.06629138439893723,0.0033145975321531296,-0.04944893345236778,0.0670175552368164,-0.01050348300486803,-0.00948885828256607,0.09786069393157959,0.0567004419863224,0.07237072288990021,0.013646503910422325,-0.036716654896736145,0.013021081686019897,0.002509942278265953,-0.028591183945536613,0.04379807040095329,0.033901918679475784,0.013898368924856186,-0.00624716654419899,-0.05170636624097824,0.02191181667149067,0.07399962842464447,0.03524370491504669,-0.09546837955713272,-0.05805453285574913,-0.013565354980528355,0.07592908293008804,0.006627316121011972,-0.01060525979846716,0.055438753217458725,-0.02396656572818756,0.04632999002933502,0.014145542867481709,0.018627531826496124,0.001098146429285407,0.04717889428138733,0.011584602296352386,-0.12184736132621765,-0.03675026819109917,-0.040869489312171936,-0.01899861916899681,-0.04758496582508087,0.05235976353287697,-0.06041897088289261,0.05191824957728386,0.028246091678738594,0.008402576670050621,-0.10349356383085251,-0.0196122657507658,-0.03437233716249466,0.0014735377626493573,-0.004649218171834946,-0.012599456124007702,-0.12194474041461945,-0.06751927733421326,-0.03444533422589302,0.056502826511859894,0.02052277885377407,-0.05641315132379532,-0.05661367252469063,0.09657426923513412,-0.05156201124191284,-0.04349668696522713,0.04858849197626114,-0.03838339075446129,0.05655112862586975,-0.007492720615118742,-0.08943474292755127,0.07018423080444336,-0.05949322506785393,-0.03991977125406265,0.027284342795610428,0.09209983795881271,0.04978605732321739,-0.01987331360578537,-0.02065214142203331,-0.016359250992536545,-0.05315207690000534,0.05411004275083542,0.010295446962118149,0.021832410246133804,0.04238712415099144,-0.1065685898065567,-0.045711737126111984,-0.015379173681139946,0.06339515000581741,-0.017887791618704796,-0.02786005288362503,-0.013887567445635796,0.019121114164590836,-0.03921772539615631,0.008474289439618587,0.04331550374627113,0.029688697308301926,0.07313806563615799,-0.022227760404348373,0.00465940497815609,0.013548769988119602,0.05987062305212021,0.06795264035463333,-0.01834399625658989,-0.1124928891658783,-0.06789682805538177,0.017844432964920998,0.03138793259859085,0.008801193907856941,0.0403631255030632,-0.04825950786471367,0.07596874237060547,0.007563794031739235,0.020732669159770012,-0.0031493022106587887,-0.036075275391340256,-0.048389676958322525,-0.0064369067549705505,0.0547177717089653,0.01666921004652977,-0.05499719828367233,0.10587044060230255,0.05778203532099724,-0.016049182042479515,-0.006777764763683081,0.029605837538838387,-0.021758899092674255,0.10522901266813278,0.08878772705793381,0.031147371977567673,0.03787599503993988,-0.08212275803089142,0.00878741592168808,0.024859169498085976,-0.11754819005727768,-0.0053237853571772575,-0.029714420437812805,-0.009834163822233677,0.03249134495854378,0.07036424428224564,-0.03919491544365883,-0.05759216845035553,-0.04611777886748314,0.008307309821248055,0.018170321360230446,-0.04199734702706337,0.06676033139228821,0.02603749744594097,-0.0822596475481987,-0.018030958250164986,-0.08288592845201492,-0.015716232359409332,0.017339035868644714,0.0997655987739563,0.0008574411040171981,-0.006883278489112854,-0.03319776430726051,0.026754900813102722,-0.017154326662421227,-0.07460855692625046,0.031817562878131866,-0.0650804340839386,0.00806308165192604,0.04423217475414276,0.013676725327968597,0.06706799566745758,-0.029514798894524574,0.04644958674907684,-0.018276259303092957,-0.005496754311025143,0.04028491675853729,0.13665154576301575,0.030019911006093025,0.06428418308496475,0.05775580182671547,-0.002326743211597204,0.022416364401578903,0.049534257501363754,-0.08359789103269577,0.021017875522375107,0.10370436310768127,-0.019244203343987465,-0.012089553289115429,-0.01004614494740963,-0.012075630016624928,0.03953392058610916,-0.10984686762094498,-0.01948682963848114,0.019560646265745163,0.0572534017264843,-0.032665152102708817,-0.03418693691492081,0.05533347278833389,0.045126620680093765,-0.06738916784524918,0.08271117508411407,-0.031061023473739624,0.023155972361564636,0.09290830045938492,-0.006167251616716385,-0.07365603744983673,0.015394936315715313,0.03587428107857704,-0.01694973185658455,0.08964557945728302,0.038352299481630325,0.023005923256278038,0.032237160950899124,0.06345616281032562,-0.008679322898387909,-0.08166693150997162,0.03656836971640587,-0.03129664808511734,-0.05769870802760124,0.08916506171226501,0.07749336212873459,-0.0022755563259124756,0.054362911731004715,-0.08830728381872177,-0.02552422136068344,-0.027470283210277557,-0.0562705434858799,0.03913499787449837,-0.025809340178966522,-0.03307902812957764,0.008577975444495678,-0.06771508604288101,-0.016863537952303886,-0.13943232595920563,-0.04126066341996193,-0.0803823173046112,0.036189936101436615,0.04444983974099159,-0.04583051800727844,0.06667596846818924,-0.06085062026977539,0.0160200297832489,-0.0026856805197894573,0.006468707229942083,0.009436882100999355,-0.03835759684443474,-0.056161798536777496,-0.001462378422729671,-0.04281487688422203,-0.024496624246239662,-0.027345219627022743,0.08762593567371368,0.0008108801557682455,-0.030503202229738235,-0.006941074971109629,0.1073571965098381,-0.016209911555051804,0.06450024992227554,-0.026651639491319656,-0.022197358310222626,0.06104029715061188,-0.0004591376055032015,-0.033355776220560074,-0.07464107871055603,0.015531955286860466,-0.038507647812366486,0.026869701221585274,-0.049275316298007965,-0.046599891036748886,0.02289574220776558,-0.013924228958785534,-0.015686864033341408,-0.007698289584368467,0.05062398687005043,-0.040160320699214935,-0.05386574938893318,-0.11603645980358124,-0.06669753789901733,-0.026043932884931564,0.06273889541625977,0.05512595549225807,-0.058893464505672455,0.011990263126790524,-0.03737001121044159,-0.01919017732143402,0.024401463568210602,0.027317525818943977,0.062364012002944946,0.05774455517530441,0.05975867807865143,-0.03179057687520981,-0.0467253178358078,-0.011276237666606903,0.016372211277484894,-0.039177559316158295,0.048332661390304565,0.07810036092996597,-0.05687250941991806,-0.024654360488057137,0.0034845671616494656,0.0704084113240242,0.051116980612277985,0.00794588215649128,-0.0052092960104346275,0.0096563920378685,-0.04145439341664314,-0.010270693339407444,0.008874132297933102,0.03683992102742195,0.028352228924632072,-0.07811100780963898,-0.02126290649175644,-0.008587769232690334,-0.0731387510895729,0.04243316873908043,0.03991204872727394,0.030864771455526352,0.06774848699569702,0.0011343623045831919,-0.0372549332678318,0.026908550411462784,-0.08309272676706314,1.1626981176959816e-05,0.02160617709159851,0.012921476736664772,0.01681060530245304,-0.006987650878727436,-0.050691504031419754,0.04488814249634743,0.0784604474902153,-0.01222151704132557,-0.019866706803441048,0.042013019323349,-0.020505420863628387,0.06367117911577225,-0.011179104447364807,0.03423139080405235,-0.03432207927107811,0.0336601696908474,0.023112697526812553,-0.004579923115670681,0.05294228717684746,0.0028938790783286095,0.11930016428232193,0.06883595883846283,0.03789292648434639,-0.031409524381160736,-0.037982381880283356,-0.08058344572782516,0.04520599916577339,-0.03564603999257088,0.02233179286122322,0.09258510917425156,-0.030918482691049576,-0.0012204619124531746,-0.018494218587875366,0.14549879729747772,0.022776402533054352,-0.014958394691348076,0.07776074856519699,-0.0487305223941803,-0.01146868895739317,0.018444184213876724,0.020326683297753334,0.015101478435099125,-0.09486169368028641,-0.02243323251605034,0.08631979674100876,0.022361833602190018,0.062483593821525574,-0.012592227198183537,-0.04457937926054001,0.022016210481524467,-0.06269112229347229,-0.03294374793767929,-0.09966778010129929,0.028075620532035828,0.002061976119875908,-0.05294787138700485,-0.05678236111998558,-0.10248316824436188,0.006181477103382349,-0.048434048891067505,0.05923324078321457,0.09883055835962296,0.05166691541671753,0.04476642608642578,-0.010022293776273727,-0.020543042570352554,-0.07232897728681564,0.014863510616123676,0.019114086404442787,-0.004851889796555042,-0.0721956267952919,0.015449012629687786,0.026775695383548737,0.009490693919360638,-0.005367602687329054,-0.00019714153313543648,0.014234348200261593,-0.048813022673130035,0.055334482342004776,0.01785082183778286,-0.035845085978507996,-0.003777110017836094,-0.08390893787145615,0.006594935432076454,-0.048800330609083176,-0.022685278207063675,-0.11395987868309021,0.05772985890507698,0.026509929448366165,-0.03827507793903351,0.019896596670150757,0.007231175899505615,-0.06881126761436462,0.01603531278669834,-0.057354290038347244,-0.035904642194509506,-0.042851854115724564,-0.053093072026968,-0.00914781168103218,-0.05384315922856331,0.08033403009176254,-0.024750757962465286,0.02142411097884178,0.007352905813604593,0.06063348799943924,-0.0018450012430548668,-0.015240820124745369,-0.013011634349822998,-0.0932370200753212,0.03294709697365761,-0.05435248836874962,0.010064669884741306,0.03008275479078293,-0.06799684464931488,0.01118081621825695,-0.010511551052331924,0.020987529307603836,0.018115578219294548,-0.02874777838587761,0.06542360782623291,0.057194605469703674,0.023515740409493446,-0.0580146349966526,-0.023670947179198265,-0.02809096686542034,0.09229771047830582,-0.053449805825948715,-0.09124875068664551,-0.0012734729098156095,0.023086996749043465,1.9035347577300854e-05,-0.050690241158008575,-0.01634734869003296,-0.054565105587244034,-0.0179202388972044,0.011247669346630573,0.06349652260541916,-0.022158341482281685,-0.04675819352269173,-0.014002411626279354,-0.08412935584783554,0.05757282301783562,-0.020498326048254967,0.03668857365846634,0.021434236317873,-0.06936068832874298,0.08243793249130249,-0.05375377833843231,0.015128014609217644,-0.0645824670791626,-0.0013130708830431104,-0.021305855363607407,-0.07436998933553696,-0.07661359012126923,-0.021370859816670418,0.07875450700521469,-0.03365872800350189,0.0146724758669734,-0.0689726397395134,-0.056878138333559036,0.024895504117012024,0.024731501936912537,-0.061564527451992035,0.013011147268116474,0.06827298551797867,0.0020905572455376387,0.026921944692730904,-0.04986003786325455,-0.02767987735569477,-0.027987012639641762,-0.004002929665148258,-0.021418394520878792,-0.09107010811567307,0.04075435549020767,0.005599336698651314,-0.03089129738509655,-0.09984132647514343,-0.04731609299778938,-0.0219984482973814,-0.010212208144366741,-0.010186257772147655,0.011901254765689373,0.017460834234952927,0.09874261170625687,1.2000401511613745e-05,0.0020189040806144476,0.02522922493517399,0.00125973136164248,-0.04798693582415581,0.0010234600631520152,-0.0067580826580524445,0.0033823393750935793,0.03362400457262993,-0.08928629010915756,-0.008283954113721848,-0.03811613842844963,-0.026643993332982063,-0.030749410390853882,0.027287859469652176,0.023642750456929207,0.0038796360604465008,0.055540405213832855,0.06286381930112839,-0.01774020306766033,0.051734015345573425,-0.020586542785167694,-0.028261153027415276,0.1582610160112381,0.01336747594177723,-0.038442742079496384,-0.04161117970943451,0.0004794653214048594,0.03609083592891693,0.0477672703564167,-0.006876674480736256,-0.010367924347519875,0.05378174036741257,0.10002603381872177,-0.0012530825333669782,0.0018071811646223068,-0.011871401220560074,0.0002645525673869997,-0.024211907759308815,0.01574971154332161,-0.1001780778169632,-0.059562746435403824,-0.0303689893335104,0.03018012084066868,-0.05553091689944267,0.021827660501003265,-0.04668952152132988,0.039057131856679916,0.0970066487789154,-0.00334223173558712,-0.09065000712871552,-0.04177406057715416,-0.032880350947380066,-0.05554598197340965,-0.015091018751263618,0.02060128003358841,0.005667736753821373,-0.09660986810922623,-0.03531487658619881,-0.0618869811296463,0.029538288712501526,-0.010356135666370392,0.07105377316474915,-0.001470130286179483,0.06501643359661102,-0.004430912900716066,-0.01033023651689291,-0.024587295949459076,-0.03376200422644615,0.11936061829328537,-0.014294465072453022,-0.027458062395453453,0.0692979171872139,-0.03938046842813492,-0.03193510323762894,-0.00019328566850163043,-0.006142064929008484,-0.005877216812223196,0.011833577416837215,-0.06444914638996124,0.037680480629205704,0.013857515528798103,0.043915074318647385,-0.019561737775802612,-0.05382023751735687,0.02785230241715908,0.02495497092604637,0.004898982122540474,-0.05490961670875549,0.03275443613529205,0.023119080811738968,0.04403900355100632,0.03136463463306427,0.11602727323770523,-0.040565136820077896,0.01620781607925892,-0.019663270562887192,-0.023887937888503075,-0.014881469309329987,0.005714242346584797,0.045318134129047394,-0.05739612504839897,0.1040123924612999,0.036914896219968796,-0.05502843111753464,-0.02275184914469719,-0.07164642959833145,-0.12315217405557632,-0.03977600857615471,-0.02104252576828003,0.050261110067367554,0.04190652072429657,0.01589190401136875,0.004949019756168127,-0.07453009486198425,-0.02989937551319599,-0.036669038236141205,0.08829792588949203,-0.02001042664051056,-0.08403906226158142,0.012975428253412247,0.04326576367020607,-0.04253615811467171,0.017738182097673416,0.006512740161269903,0.05077482387423515,0.0021307882852852345,0.11799611896276474,-0.060068998485803604,-0.023999221622943878,-0.01428302749991417,0.006814718246459961,-0.07932839542627335,-0.045014962553977966,0.04199188947677612,0.08105875551700592,0.050327982753515244,0.0014975343365222216,-0.04622504115104675,-0.05728619173169136,-0.07657716423273087,-0.019055182114243507,0.01322865765541792,0.1054450199007988,-0.09648356586694717,-0.06763320416212082,-0.010853498242795467,-0.0007077623158693314,0.018528064712882042,0.048491593450307846,-0.07081940025091171,-0.010267623700201511,-0.029120033606886864,0.1100861057639122,0.04858127236366272,0.09279575198888779,0.01803252287209034,-0.027649223804473877,-0.003142743604257703,0.028753697872161865,-0.04239331930875778,-0.00344845955260098,0.039445746690034866,-0.010088798590004444,-0.034144144505262375,0.0196500476449728,-0.04838454723358154,-0.018688572570681572,-0.09815070033073425,-0.04402431845664978,-0.06183771789073944,-0.0605945847928524,0.00931202620267868,-0.003232113551348448,-0.03147135302424431,-0.02173640765249729,-0.06669769436120987,0.004654502496123314,0.08508019894361496,-0.021482544019818306,0.0011558716651052237,-0.0659918263554573,0.04599123075604439,-0.04067233204841614,0.02283680997788906,-0.04137200862169266,0.006964207161217928,0.0675588995218277,0.008386357687413692,-0.018664099276065826,0.014019369147717953,0.0016488437540829182,-0.01356069277971983,-0.029761815443634987,0.007186941336840391,-0.018296435475349426,0.11962325870990753,0.0777163952589035,-0.10674884170293808,-0.010601513087749481,-0.05397878959774971,-0.008207201957702637,0.04431811347603798,0.057605721056461334,0.09327578544616699,0.006751133594661951,0.04300929978489876,0.10294829308986664,0.03834925964474678,-0.028171157464385033,0.009930265136063099,-0.03628374636173248,0.05012748762965202,-0.07955841720104218,-0.04269952327013016,-0.10373932123184204,0.016068264842033386,-0.053825974464416504,0.02821224555373192,-0.06370766460895538,0.008522612042725086,0.06576945632696152,0.10356296598911285,-0.059885647147893906,-0.02305898815393448,0.08819606155157089,-0.005509034730494022,0.058365583419799805,-0.011747682467103004,-0.0027912685181945562,-0.017959898337721825,0.04149805009365082,0.05576236918568611,0.03141283243894577,0.05183451250195503,0.05944206938147545,-0.006468936800956726,-0.04088045284152031,-0.06067092716693878,-0.07294540107250214,0.033282581716775894,-0.04199711233377457,0.014690653420984745,0.04540739208459854,0.042027801275253296,-0.05466475337743759,0.02644672803580761,0.06827829778194427,0.07883349061012268,-0.08306767046451569,0.05659431219100952,0.03384275361895561,-0.04972514882683754,-0.0538833811879158,0.027060456573963165,-0.03086833842098713,0.001631931052543223,0.06453549861907959,0.014696717262268066,0.06657776236534119,0.011464743874967098,0.008244985714554787,0.009935088455677032,0.06980272382497787,0.007364990189671516,0.023648520931601524,0.09735068678855896,0.034723103046417236,-0.05248071998357773,-0.02740861102938652,-0.01185793150216341,-0.035028375685214996,0.004787870217114687,0.012950312346220016,-0.01341638807207346,-0.050563663244247437,-0.05245138332247734,-0.008206679485738277,0.062326326966285706,0.0024533167015761137,0.03518804907798767,-0.1040087640285492,0.017815429717302322,-0.002571864752098918,-0.0460803396999836,0.09667012095451355,-0.0696813240647316,0.019580164924263954,-0.03740131855010986,-0.08511470258235931,-0.0036503931041806936,-0.08664777129888535,-0.03554391860961914,0.05256178602576256,0.055590108036994934,-0.026982983574271202,-0.03158506751060486,-0.040259744971990585,0.008635043166577816,0.04608609899878502,0.02741539292037487,0.06273731589317322,0.08395754545927048,-0.04219257831573486,-0.006153685972094536,-0.013261349871754646,-0.036610011011362076,0.019204072654247284,0.025704961270093918,-0.02822762541472912,0.016041386872529984,0.07608445733785629,-0.02893282100558281,-0.03699333593249321,-0.044185612350702286,-0.1012999564409256,0.11668413877487183,-0.06228035315871239,-0.030463218688964844,-0.016929898411035538,0.0012493786634877324,0.03658697009086609,-0.025342853739857674,0.05373018980026245,-0.025830451399087906,-0.02805451489984989,-0.002477760426700115,0.044681087136268616,0.002823165152221918,-0.037195295095443726,-0.00022387092758435756,-0.0009506146889179945,-0.06286782771348953,-0.0427226759493351,0.025470435619354248,0.03869641572237015,0.005406197626143694,0.06963349133729935,-0.0327322855591774,-0.021262306720018387,-0.05033900961279869,-0.002785904798656702,0.006447966210544109,-0.10949062556028366,0.015477088280022144,-0.012033488601446152,0.04295118898153305,-0.0076263584196567535,-0.054019395262002945,-0.02193405292928219,0.015229932963848114,0.05740208178758621,-0.061307597905397415,0.03225525841116905,0.11623204499483109,-0.03149380534887314,-0.04100315272808075,0.004221729468554258,-0.06192043796181679,-0.02257554605603218,0.02404562570154667,0.05329231545329094,0.10110744088888168,0.030265459790825844,0.008197981864213943,-0.09852822870016098,0.03292188420891762,-0.023305991664528847,0.06829741597175598,0.033459149301052094,-0.0004566983843687922,-0.021629564464092255,0.02315988391637802,0.05412376672029495,-0.004163971170783043,-0.0540219247341156,-0.0383925624191761,0.030644018203020096,-0.09922921657562256,-0.07122144103050232,-0.006848189048469067,0.06306970864534378,-0.0339336171746254,-0.06014695018529892,-0.03178096190094948,-0.022566579282283783,-0.005660105496644974,0.017513088881969452,-0.024422353133559227,-0.034402646124362946,0.02432396076619625,-0.02258255146443844,0.02118597738444805,-0.03699425980448723,-0.021684646606445312,-0.044744815677404404,0.028163760900497437,-0.08999476581811905,0.1389172375202179,-0.05121895298361778,0.03451179713010788,0.013063494116067886,-0.0040983771905303,-0.05688106641173363,0.0025593070313334465,0.03820822387933731,0.007958897389471531,0.03949924558401108,0.021668657660484314,-0.01983756385743618,-0.005107662174850702,0.05713001638650894,-0.056349385529756546,-0.07846380770206451,-0.05379351228475571,0.07298596948385239,0.05901513621211052,0.015126966871321201,-0.021032599732279778,0.005492819473147392,-0.022179946303367615,0.01999376341700554,-0.11599690467119217,0.03657323867082596,-0.01903633400797844,0.1142207533121109,0.1063801571726799,-0.031029807403683662,0.008217044174671173,-0.00694684311747551,0.05247382074594498,0.057539038360118866,-0.005409030709415674,0.014518681913614273,0.054362718015909195,0.016147563233971596,-0.054066531360149384,0.032186318188905716,0.012095886282622814,-0.013018617406487465,-0.05893845856189728,-0.04360579699277878,-0.01769564114511013,-0.08669278025627136,-0.008004121482372284,0.052159592509269714,0.002279105130583048,-0.05425892770290375,-0.08547715842723846,-0.06346303969621658,-0.09482082724571228,0.08781478554010391,-0.01968926005065441,0.020196858793497086,-0.08044757694005966,-0.026101727038621902,0.03243251144886017,0.030552148818969727,0.06823184341192245,-0.027353554964065552,0.008763467893004417,-0.020878121256828308,-0.06558182835578918,-0.12397973984479904,-0.02983006276190281,-0.008352573029696941,0.009279021993279457,0.01475480291992426,-0.07387111335992813,0.015412818640470505,0.051448892802000046,-0.03318177908658981,-0.01529289223253727,0.023527035489678383,-0.05233054980635643,-0.04184865579009056,0.01057897973805666,-0.08167927712202072,-0.004737782292068005,0.04872533306479454,0.021129269152879715,0.062626414000988,0.03496889770030975,0.05461374670267105,0.05473044887185097,-0.03758092597126961,0.04906227067112923,-0.04199374467134476,-0.02092895656824112,-0.018137482926249504,-0.02597993053495884,0.03365207090973854,0.027144616469740868,-0.07067937403917313,-0.015804436057806015,-0.03391942381858826,-0.10259965807199478,-0.02783198654651642,-0.03838903084397316,0.012202232144773006,0.007014712784439325,-0.003800211939960718,-0.105845607817173,-0.013158712536096573,-0.02386116236448288,0.016421331092715263,-5.601736847893335e-05,-0.004934342112392187,-0.01104481890797615,-0.0831146091222763,-0.03238846734166145,0.021774010732769966,0.03536950796842575,-0.044993869960308075,0.03437880799174309,0.021299973130226135,0.09123475849628448,-0.03266824781894684,-0.01566162519156933,-0.045226167887449265,-0.024234503507614136,-0.0003877710842061788,-0.02589794062077999,-0.0012531944084912539,0.004735851194709539,-0.0033899350091814995,0.04142560437321663,0.017130542546510696,0.0016124009853228927,-0.026020515710115433,0.08959128707647324,0.029420996084809303,-0.04840068146586418,0.047505952417850494,-0.00042995737749151886,-0.0166687723249197,-0.03525572270154953,-0.01052155438810587,0.02021305076777935,-0.008723793551325798,0.0254786629229784,-0.006369445007294416,-0.07413703203201294,-0.03720008209347725,-0.029039796441793442,-0.06056583672761917,-0.01276811957359314,-0.0776904970407486,0.009771575219929218,-0.021066980436444283,-0.016446130350232124,0.08995536714792252,-0.0441528744995594,-0.028363166376948357,-0.02263678051531315,-0.021410994231700897,-0.0606212317943573,-0.03324604779481888,0.016857990995049477,-0.03680525720119476,-0.09292864054441452,-0.011705543845891953,0.0010326317278668284,0.01980826072394848,0.03071129508316517,-0.07113776355981827,-0.048932455480098724,-0.03439515456557274,0.030453374609351158,0.05031958594918251,0.012748315930366516,0.030590342357754707,0.0008953158976510167,0.047938279807567596,-0.0486595518887043,-0.07247444242238998,-0.005823737941682339,0.018676960840821266,0.023257024586200714,0.04902566224336624,-0.06556607782840729,0.006821179296821356,-0.008722033351659775,0.008730179630219936,-0.05505678430199623,0.021105393767356873,0.024368425831198692,0.024113666266202927,0.10016464442014694,0.06476007401943207,-0.0597805380821228,0.04543990269303322,0.04829920828342438,0.07007138431072235,0.0034192767925560474,0.036864444613456726,-0.0733204111456871,0.04064919799566269,0.03901592642068863,0.046776168048381805,0.02629554457962513,0.05524981766939163,-0.08388545364141464,0.0024842654820531607,-0.032120268791913986,-0.009708101861178875,0.038159068673849106,0.032196640968322754,-0.03275236859917641,-0.03286976367235184,0.08642803877592087,-0.07133656740188599,-0.01631227880716324,-0.04664669930934906,0.02933507226407528,-0.05867643281817436,0.06519765406847,0.021322447806596756,-0.044793590903282166,-0.06408719718456268,-0.04062676802277565,-0.0028058469761162996,-0.028592776507139206,0.005072282161563635,-0.009405572898685932,0.028777586296200752,0.05414443835616112,0.0300894882529974,0.03205560892820358,0.02290095016360283,0.010005989111959934,0.014535283669829369,-0.042085450142621994,-0.001464726054109633,0.013615732081234455,-0.11384189128875732,-0.00042912911158055067,0.08822790533304214,0.04280075803399086,-0.01226085051894188,0.026778416708111763,0.06271817535161972,-0.06438057869672775,-0.1239776536822319,0.045660391449928284,-0.03915092721581459,0.00794632826000452,0.022321349009871483,-0.015362784266471863,0.05997699126601219,-0.03121812641620636,-0.0006633733864873648,-0.04524560645222664,0.012438323348760605,0.06505558639764786,-0.02489045076072216,0.05295230820775032,-0.00769166462123394,-0.008832722902297974,0.031238717958331108,-0.008500429801642895,0.009982902556657791,0.051779814064502716,0.029861627146601677,-0.013033019378781319,0.009921200573444366,-0.03626679256558418,-0.036831118166446686,0.03474488481879234,-0.009977473877370358,-0.023460041731595993,0.09108279645442963,-0.005205919966101646,0.005788924638181925,0.06162775307893753,-0.0481146015226841,0.01560547761619091,0.01379909086972475,0.04920094460248947,-0.05623611435294151,-0.04384352266788483,0.05252086743712425,0.0022265356965363026,-0.042598482221364975,0.027391720563173294,0.04578041285276413,-0.03665218874812126,0.002009084215387702,-0.04297209903597832,-0.008007500320672989,0.0049215457402169704,-0.013268934562802315,0.07584219425916672,-0.059234075248241425,0.008327149786055088,-0.0922081470489502,0.006343155633658171,0.004155779257416725,0.01312166266143322,0.038432054221630096,0.07916969060897827,0.015575542114675045,0.03577173873782158,0.042593345046043396,-0.025287924334406853,0.04714491590857506,-0.01158407237380743,-0.03669248893857002,0.01137405913323164,-0.03295503556728363,-0.03317665681242943,0.06509853154420853,0.07457707077264786,0.018907638266682625,0.015631265938282013,0.01024606917053461,0.0009987996891140938,-0.08519618213176727,0.028892241418361664,-0.07281159609556198,-0.0003842141595669091,0.039774902164936066,0.015161339193582535,0.054802797734737396,0.022265704348683357,0.001860613701865077,0.07117106765508652,0.09585531055927277,-0.011276071891188622,0.02751881256699562,0.005319100338965654,-0.0006132054841145873,-0.03934885933995247,-0.027732644230127335,-0.0011327051324769855,-0.011431753635406494,-0.05574849620461464,-0.060108330100774765,-0.006574437953531742,0.020600298419594765,0.023172322660684586,0.04145871102809906,0.0025174510665237904,-0.008627906441688538,0.0654897689819336,0.058482568711042404,0.049655940383672714,-0.0045619807206094265,0.07189827412366867,-0.01894783042371273,-0.06835969537496567,-0.042958080768585205,-0.0531807616353035,0.05445312336087227,-0.11697392165660858,-0.1104491651058197,-0.010981154628098011,-0.07523003220558167,0.0007447918178513646,8.249147504102439e-05,0.06576711684465408,-0.02350740134716034,0.037259504199028015,-0.003554146969690919,0.044223230332136154,0.03585909307003021,0.002789992606267333,-0.0662342980504036,0.011705189011991024,0.024845177307724953,-0.04629207402467728,0.03883520886301994,-0.04617921635508537,-0.0824350118637085,0.05054232105612755,-0.02822905220091343,-0.09810731559991837,0.008560804650187492,0.03157161548733711,0.0010064705274999142,0.03927933797240257,-0.0875353291630745,0.034024372696876526,0.06510000675916672,-0.06704115867614746,0.004561623092740774,-0.06686442345380783,-0.030281683430075645,0.05936837196350098,0.004761362448334694,-0.013935865834355354,0.025520356371998787,-0.08170535415410995,-0.038062915205955505,0.05828667804598808,0.10200252383947372,0.010392384603619576,-0.027096925303339958,0.014793802984058857,-0.016970276832580566,0.10762068629264832,-0.02264973148703575,0.002544223330914974,0.008625561371445656,0.06299641728401184,0.014048479497432709,0.022868897765874863,-0.004897008184343576,0.0890488252043724,0.01589413732290268,0.010817458853125572,-0.12598943710327148,0.015487843193113804,-0.03958861902356148,0.03329867124557495,0.17098231613636017,-0.08821547031402588,0.046161361038684845,-0.07933276891708374,0.03449876233935356,-0.07283443957567215,0.030551712960004807,0.00929484236985445,-0.02283621020615101,0.022194871678948402],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"x0"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"x1"}},"coloraxis":{"colorbar":{"title":{"text":"color"}},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"legend":{"tracegroupgap":0,"itemsizing":"constant"},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('f4af6e93-2687-41d8-81da-3b8622f7dcee');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>
<ol type="1">
<li><p>The clusterings simply divide the words based on their x0 weights, so the variations mainly exist there. Some of the significant outliers of the x0 axis are <strong>Trumps</strong> (Trump’s), rep, gop, j, im, more, mr, that, nov. Some of them are more like stop words (I’m, that before standardization) that appear a lot but don’t really mean anything. Trump is an outlier because obviously he’s one of his kind in terms of spreading false information, making false claims, and creating chaos on the social network (Trump’s twitter).</p></li>
<li><p>Meanwhile, <strong>trump</strong> appears somewhere in the middle orange cluster as well as some other politician last name (like <strong>clinton</strong>). The standardization does not collapse trumps and trump into the same thing. So politician last names mostly appear in the same group.</p></li>
<li><p>In terms of the x1 axis, one word that gets weighted heavily is <strong>knowledge</strong>. It’s quite easy to think of sentences like ‘currently, scientists have xx knowledge on…’, ‘officials claimed no knowledge of…’ to be in a supposedly ‘informative’ piece of news.</p></li>
<li><p>The rightest cluster has words like <strong>friday, tuesday, thurdsay</strong> that belong to the same category. To the right, there are adverbs like <strong>allegedly, apparently, recently</strong> that seems to be common in all news articles.</p></li>
<li><p>The middle clusters are mostly noun, proper nouns, and verb that are not weighted heavily.</p></li>
</ol>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>